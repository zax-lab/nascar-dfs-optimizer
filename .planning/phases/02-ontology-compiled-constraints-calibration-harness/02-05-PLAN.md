---
phase: 02-ontology-compiled-constraints-calibration-harness
plan: 05
type: execute
wave: 3
depends_on: [02-01, 02-02, 02-03, 02-04]
files_modified:
  - apps/backend/app/kernel.py
  - apps/backend/app/tests/test_kernel_instrumentation.py
  - apps/backend/app/calibration/diagnostics.py
  - packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py
autonomous: true

must_haves:
  truths:
    - "Kernel validation instrumentation tracks rejection rates and veto reasons across scenarios"
    - "Compiled constraints integrate with SkeletonNarrative scenario generation"
    - "Calibration metrics available in scenario diagnostics (CRPS, log score, coverage)"
    - "End-to-end pipeline validates scenarios with kernel and assesses calibration"
    - "Rejection rate logged and available in optimization diagnostics"
  artifacts:
    - path: "apps/backend/app/kernel.py"
      provides: "Kernel validation with instrumentation (rejection tracking, veto reasons)"
      exports: ["KernelLogic.validate_dominator_conservation", "get_rejection_stats"]
      min_lines: 50 (modifications)
    - path: "packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py"
      provides: "SkeletonNarrative integration with compiled ConstraintSpec"
      exports: ["SkeletonNarrative.__init__", "generate_scenarios_with_constraints"]
      min_lines: 80 (modifications)
    - path: "apps/backend/app/calibration/diagnostics.py"
      provides: "End-to-end calibration assessment with scenario results"
      exports: ["assess_scenario_calibration", "generate_calibration_report"]
      min_lines: 60 (modifications)
    - path: "apps/backend/app/tests/test_kernel_instrumentation.py"
      provides: "Integration tests for kernel validation and calibration"
      exports: ["test_rejection_tracking", "test_end_to_end_calibration"]
      min_lines: 100
  key_links:
    - from: "apps/backend/app/kernel.py"
      to: "packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py"
      via: "Kernel validates conservation constraints for generated scenarios"
      pattern: "kernel\\.validate_dominator_conservation.*scenario"
    - from: "packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py"
      to: "apps/backend/app/constraints/models.py"
      via: "Scenario generation uses compiled ConstraintSpec instead of live Neo4j queries"
      pattern: "SkeletonNarrative.*constraint_spec"
    - from: "apps/backend/app/calibration/diagnostics.py"
      to: "apps/backend/app/kernel.py"
      via: "Calibration assessment includes kernel rejection rates"
      pattern: "rejection_stats.*kernel"
---

<objective>
Integrate compiled constraints, kernel validation, and calibration into end-to-end pipeline with instrumentation tracking rejection rates and veto reasons, enabling scenario-driven optimization with full diagnostic visibility.

Purpose: Phase 2 delivers multiple components (constraint compilation, telemetry, calibration, API) that must integrate into cohesive pipeline. Kernel validation instrumentation provides visibility into conservation enforcement, compiled constraints eliminate live Neo4j queries, and calibration metrics quantify uncertainty by track type. End-to-end integration ensures all components work together for production use.

Output: Integrated pipeline with `SkeletonNarrative` using `ConstraintSpec`, kernel validation with rejection tracking, and calibration diagnostics in scenario outputs.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-RESEARCH.md

@apps/backend/app/kernel.py
@apps/backend/app/constraints/compiler.py
@packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py
@apps/backend/app/calibration/diagnostics.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add kernel validation instrumentation</name>
  <files>apps/backend/app/kernel.py</files>
  <action>
    Add instrumentation to `apps/backend/app/kernel.py` for tracking validation statistics:

    1. Add rejection tracking:
       - Create module-level dict: `rejection_stats = {"total_validated": 0, "total_rejected": 0, "veto_reasons": {}}`
       - Create function `get_rejection_stats() -> Dict[str, Any]`:
         * Returns copy of rejection_stats dict
         * Computes rejection_rate = total_rejected / total_validated
       - Create function `reset_rejection_stats()`:
         * Resets rejection_stats to zeros

    2. Modify `validate_dominator_conservation()` method:
       - After line 299 (result creation), add instrumentation:
         * Increment total_validated
         * If not result.is_valid:
           - Increment total_rejected
           - For each veto_reason in result.veto_reasons:
             * Increment count in rejection_stats["veto_reasons"][veto_reason]
         * Log validation result with counts

    3. Add logging for key events:
       - Log when validation starts (scenario_id)
       - Log when validation fails (veto reasons)
       - Log rejection rate every 100 validations
       - Use structured logging with: scenario_id, is_valid, veto_reasons, rejection_rate

    4. Add `get_rejection_summary()` class method:
       - Returns dict with:
         * total_validated
         * total_rejected
         * rejection_rate
         * top_veto_reasons (top 5 by count)
       - Use for diagnostics and monitoring

    5. Update `batch_validate_scenarios()` method:
       - Track batch-level statistics
       * Log batch start/end with scenario count
       * Return summary along with results
       * Compute batch rejection rate

    DO NOT: Break existing validate_dominator_conservation API, skip logging, or track PII
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.kernel import KernelLogic, get_rejection_stats, reset_rejection_stats
    import numpy as np

    # Reset stats
    reset_rejection_stats()

    # Create kernel
    kernel = KernelLogic(field_size=40)

    # Test valid scenario
    valid_scenario = {
        'laps_led': [100, 50] + [0] * 38,
        'fastest_laps': [10, 8] + [0] * 38,
        'start_positions': list(range(1, 41)),
        'finish_positions': list(range(1, 41)),
        'race_length': 200,
        'green_flag_laps': 180
    }
    result = kernel.validate_dominator_conservation(valid_scenario)

    # Test invalid scenario (violates conservation)
    invalid_scenario = {
        'laps_led': [150, 70] + [0] * 38,  # Total > 200
        'fastest_laps': [15, 12] + [0] * 38,
        'start_positions': list(range(1, 41)),
        'finish_positions': list(range(1, 41)),
        'race_length': 200,
        'green_flag_laps': 180
    }
    result = kernel.validate_dominator_conservation(invalid_scenario)

    # Check stats
    stats = get_rejection_stats()
    assert stats['total_validated'] == 2, 'Total validated incorrect'
    assert stats['total_rejected'] == 1, 'Total rejected incorrect'
    assert stats['rejection_rate'] == 0.5, 'Rejection rate incorrect'
    assert 'laps_led' in str(stats.get('veto_reasons', {})), 'Veto reason not tracked'
    print('PASS: Rejection tracking functional')
    print(f'Rejection rate: {stats[\"rejection_rate\"]:.2%}')
    print(f'Top veto reasons: {list(stats.get(\"veto_reasons\", {}).keys())[:3]}')
    "
    ```
  </verify>
  <done>
    Kernel validation instrumentation tracks rejection rates and veto reasons for monitoring and diagnostics.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate compiled constraints with SkeletonNarrative</name>
  <files>packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py</files>
  <action>
    Modify `packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py` to use compiled ConstraintSpec:

    1. Update `SkeletonNarrative.__init__()`:
       - Add parameter: `constraint_spec: Optional['ConstraintSpec'] = None`
       - Store constraint_spec if provided
       - If constraint_spec provided:
         * Extract driver priors from spec instead of live Neo4j queries
         * Extract track difficulty from spec instead of ontology cache
       - Log: "Using compiled constraint spec" or "Using live ontology queries"

    2. Modify `_extract_driver_ids()` method:
       - If constraint_spec provided, extract driver_ids from spec.drivers.keys()
       - Otherwise, use existing CBN-based extraction

    3. Modify `sample_race_flow_regime()` method:
       - If constraint_spec provided and track_id in spec.tracks:
         * Use TrackConstraints from spec for regime sampling
         * Use caution_rate from spec
       - Otherwise, use existing track_difficulty cache

    4. Create `generate_scenarios_with_constraints()` function:
       - Parameters: constraint_spec, n_scenarios, kernel, random_seed
       - Creates SkeletonNarrative with constraint_spec
       * Calls existing generate_scenarios()
       * Returns scenarios + rejection_stats from kernel

    5. Update `generate_scenarios()` standalone function:
       - Add optional parameter: `constraint_spec: Optional['ConstraintSpec'] = None`
       * Pass through to SkeletonNarrative if provided

    6. Add type hints:
       - Import ConstraintSpec from apps.backend.app.constraints.models
       - Use Optional[ConstraintSpec] in type hints
       - Handle import errors gracefully (try/except ImportError)

    DO NOT: Break existing generate_scenarios API, require constraint_spec (must be optional), or skip live query fallback
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/packages/axiomatic-sim && python -c "
    from axiomatic_sim.scenario_generator import generate_scenarios_with_constraints, generate_scenarios
    from app.constraints.models import ConstraintSpec, DriverConstraints, TrackConstraints
    from app.kernel import KernelLogic, reset_rejection_stats

    # Reset kernel stats
    reset_rejection_stats()

    # Create constraint spec
    spec = ConstraintSpec(
        slate_id='test',
        compiled_at='2024-01-01T00:00:00',
        drivers={
            f'driver_{i}': DriverConstraints(
                f'driver_{i}', skill=0.5, aggression=0.5, shadow_risk=0.5,
                min_laps_led=0, max_laps_led=100, veto_rules=[]
            )
            for i in range(1, 41)
        },
        tracks={
            'daytona': TrackConstraints(
                'daytona', difficulty=0.7, aggression_factor=0.6,
                caution_rate=0.05, pit_window_laps=[35, 70, 105, 140, 175]
            )
        },
        version='1.0',
        hash='test_hash'
    )

    # Generate with constraints
    kernel = KernelLogic(field_size=40)
    scenarios = generate_scenarios_with_constraints(
        constraint_spec=spec,
        n_scenarios=10,
        kernel=kernel,
        random_seed=42
    )

    assert len(scenarios) == 10, 'Wrong number of scenarios generated'
    print(f'PASS: Generated {len(scenarios)} scenarios with compiled constraints')

    # Test backward compatibility (without constraint spec)
    scenarios_legacy = generate_scenarios(
        track_id='daytona',
        n_scenarios=5,
        kernel=kernel,
        driver_ids=[f'driver_{i}' for i in range(1, 41)]
    )
    assert len(scenarios_legacy) == 5, 'Legacy API broken'
    print('PASS: Backward compatibility maintained')
    "
    ```
  </verify>
  <done>
    SkeletonNarrative accepts optional ConstraintSpec, uses compiled constraints when provided, maintains backward compatibility.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate calibration diagnostics with scenario results</name>
  <files>apps/backend/app/calibration/diagnostics.py</files>
  <action>
    Modify `apps/backend/app/calibration/diagnostics.py` to assess calibration on scenario results:

    1. Create `assess_scenario_calibration()` function:
       - Parameters: scenarios (List[ScenarioComponents]), predictions (jnp.ndarray), track_archetype (str)
       - Extract observed outcomes from scenarios:
         * Build observed_finish_positions array from scenario.driver_outcomes
         * Shape: (n_scenarios, n_drivers)
       - Call compute_all_metrics() with predictions vs observed
       - Return metrics dict with CRPS, log_score, coverage

    2. Create `generate_calibration_report()` enhancement:
       - Add parameter: kernel_rejection_stats (Dict[str, Any])
       - Include rejection metrics in report:
         * Rejection rate from kernel
         * Top veto reasons
         * Total validated vs rejected
       - Add section: "Kernel Validation Performance"
       - Log warnings if rejection rate > 50% (too many invalid scenarios)

    3. Create `end_to_end_calibration()` function:
       - Parameters: constraint_spec, track_id, n_scenarios, kernel
       - Runs complete pipeline:
         * Generate scenarios with compiled constraints
         * Track kernel rejection stats
         * Extract observed outcomes from scenarios
         * Compute calibration metrics (if predictions provided)
         * Generate calibration report with kernel stats
       - Returns dict with:
         * scenarios (valid only)
         * calibration_metrics
         * kernel_rejection_stats
         * report_path

    4. Update `posterior_predictive_check()` function:
       - Add scenario metadata to InferenceData
       * Include regime information
       * Include conservation validation results
       * Tag by track archetype for grouped analysis

    5. Add logging:
       - Log calibration assessment start/end
       - Log warnings for miscalibration (coverage deviation > 10%)
       - Log kernel rejection statistics

    DO NOT: Break existing calibration functions, require predictions (optional for scenario-only assessment), or skip kernel stats
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.calibration.diagnostics import assess_scenario_calibration, end_to_end_calibration
    from app.constraints.models import ConstraintSpec, DriverConstraints, TrackConstraints
    from app.kernel import KernelLogic, reset_rejection_stats
    import jax.numpy as jnp
    import jax.random as random

    # Reset kernel stats
    reset_rejection_stats()

    # Create constraint spec
    spec = ConstraintSpec(
        slate_id='test',
        compiled_at='2024-01-01T00:00:00',
        drivers={
            f'driver_{i}': DriverConstraints(
                f'driver_{i}', skill=0.5, aggression=0.5, shadow_risk=0.5,
                min_laps_led=0, max_laps_led=100, veto_rules=[]
            )
            for i in range(1, 41)
        },
        tracks={'daytona': TrackConstraints('daytona', 0.7, 0.6, 0.05, [35, 70])},
        version='1.0',
        hash='test_hash'
    )

    # Run end-to-end calibration
    kernel = KernelLogic(field_size=40)
    result = end_to_end_calibration(
        constraint_spec=spec,
        track_id='daytona',
        n_scenarios=10,
        kernel=kernel
    )

    assert 'scenarios' in result, 'Scenarios missing'
    assert 'kernel_rejection_stats' in result, 'Kernel stats missing'
    assert 'total_validated' in result['kernel_rejection_stats'], 'Total validated missing'
    print(f'PASS: End-to-end calibration completed')
    print(f'Scenarios generated: {len(result[\"scenarios\"])}')
    print(f'Kernel rejection rate: {result[\"kernel_rejection_stats\"].get(\"rejection_rate\", \"N/A\")}')
    "
    ```
  </verify>
  <done>
    Calibration diagnostics assess scenario outcomes, integrate kernel rejection stats, and generate comprehensive reports.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Kernel instrumentation check**: `get_rejection_stats()` returns total_validated, total_rejected, rejection_rate, veto_reasons

2. **Constraint integration check**: `generate_scenarios_with_constraints()` uses compiled ConstraintSpec and returns scenarios

3. **Backward compatibility check**: `generate_scenarios()` still works without constraint_spec parameter

4. **Calibration integration check**: `end_to_end_calibration()` runs full pipeline and returns scenarios + metrics + kernel stats

5. **End-to-end check**: Full pipeline from ConstraintSpec -> Scenarios -> Kernel Validation -> Calibration Assessment

Test commands:
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
python -m pytest tests/test_kernel_instrumentation.py -v
```
</verification>

<success_criteria>
- Kernel validation tracks rejection rates and veto reasons with get_rejection_stats()
- SkeletonNarrative uses compiled ConstraintSpec when provided, falls back to live queries if None
- Calibration assessment includes kernel rejection stats in reports
- End-to-end pipeline runs constraint compilation -> scenario generation -> validation -> calibration
- Backward compatibility maintained for existing generate_scenarios() API
</success_criteria>

<output>
After completion, create `.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-05-SUMMARY.md` with:
- Files modified (kernel.py instrumentation, scenario_generator.py integration, diagnostics.py enhancement)
- End-to-end pipeline demonstrated (constraint spec -> scenarios -> validation -> calibration)
- Kernel rejection statistics example (rate, top veto reasons)
- Calibration report sample (with kernel validation section)
- Phase 2 completion summary (all success criteria met)
- Next steps (Phase 3 preview)
</output>
