---
phase: 02-ontology-compiled-constraints-calibration-harness
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/backend/app/telemetry/__init__.py
  - apps/backend/app/telemetry/ingest.py
  - apps/backend/app/telemetry/transform.py
  - apps/backend/app/telemetry/features.py
  - apps/backend/app/telemetry/artifacts.py
  - apps/backend/app/tests/test_telemetry.py
  - data/telemetry/.gitkeep
autonomous: true

must_haves:
  truths:
    - "Telemetry pipeline ingests lap-by-lap data from Parquet files with Polars lazy scanning"
    - "Feature availability contracts prevent data leakage (no future race information)"
    - "Aggregate telemetry features computed over rolling time windows (10l, 20l, 50l)"
    - "Telemetry artifacts persisted as Parquet with Snappy compression for fast loading"
    - "Pipeline handles missing data gracefully with forward-fill and interpolation"
  artifacts:
    - path: "apps/backend/app/telemetry/ingest.py"
      provides: "Lap-by-lap telemetry ingestion with Parquet lazy scan"
      exports: ["ingest_lap_by_lap_telemetry", "TelemetryIngestor"]
      min_lines: 100
    - path: "apps/backend/app/telemetry/features.py"
      provides: "Feature availability contract enforcement"
      exports: ["FeatureAvailabilityContract", "validate_features"]
      min_lines: 80
    - path: "apps/backend/app/telemetry/transform.py"
      provides: "Polars transformations with rolling windows and aggregations"
      exports: ["compute_aggregate_features", "rolling_statistics"]
      min_lines: 120
    - path: "apps/backend/app/telemetry/artifacts.py"
      provides: "Parquet artifact persistence with compression"
      exports: ["persist_telemetry_artifact", "load_telemetry_artifact"]
      min_lines: 60
    - path: "apps/backend/app/tests/test_telemetry.py"
      provides: "Property-based tests for telemetry pipeline"
      exports: ["test_feature_availability_enforcement", "test_parquet_round_trip"]
      min_lines: 80
  key_links:
    - from: "apps/backend/app/telemetry/ingest.py"
      to: "apps/backend/app/constraints/models.py"
      via: "Telemetry enriched with driver constraints from compiled spec"
      pattern: "ConstraintSpec\\.get_driver_constraints"
    - from: "apps/backend/app/telemetry/features.py"
      to: "apps/backend/app/telemetry/ingest.py"
      via: "Feature validation before accessing telemetry columns"
      pattern: "FeatureAvailabilityContract\\.validate_features"
    - from: "apps/backend/app/telemetry/artifacts.py"
      to: "apps/backend/app/telemetry/transform.py"
      via: "Transformed telemetry persisted as Parquet artifacts"
      pattern: "persist_telemetry_artifact.*telemetry_df"
---

<objective>
Build telemetry ETL pipeline for lap-by-lap NASCAR data ingestion with feature availability contracts to prevent data leakage, using Polars for high-performance transformations and Parquet for efficient artifact storage.

Purpose: Premium loop telemetry data enables data-driven features like falloff rate and long-run speed, but accessing future race information constitutes leakage that invalidates projections. Feature availability contracts enforce temporal boundaries, and Polars lazy evaluation with Parquet compression enables scalable processing of multi-gigabyte telemetry datasets.

Output: Telemetry pipeline with `FeatureAvailabilityContract` validation, Polars-based transformations with rolling windows, and Parquet artifact persistence for downstream calibration and optimization.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-RESEARCH.md

@apps/backend/app/constraints/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create feature availability contract enforcement</name>
  <files>apps/backend/app/telemetry/features.py</files>
  <action>
    Create `FeatureAvailabilityContract` in `apps/backend/app/telemetry/features.py` to prevent data leakage:

    1. Import dependencies:
       ```python
       from typing import List, Set
       import logging
       ```

    2. Create `FeatureAvailabilityContract` class with class-level feature lists:
       - `HISTORICAL_FEATURES`: ["avg_finish_position", "avg_laps_led", "win_rate", "dnf_rate", "recent_form"]
       - `PRACTICE_FEATURES`: ["practice_lap_time", "practice_speed", "practice_laps_complete"]
       - `QUALIFYING_FEATURES`: ["qualifying_position", "qualifying_speed", "qualifying_gap"]
       - `FORBIDDEN_FEATURES`: ["race_laps_led", "race_finish_position", "race_incidents", "race_dnf_lap"]

    3. Add class method `validate_features(cls, features: List[str]) -> None`:
       - Compute intersection of requested features and FORBIDDEN_FEATURES
       - If any forbidden features requested, raise ValueError with detailed message listing forbidden features
       - Log warning if unknown features requested (not in any known list)

    4. Add class method `get_allowed_features(cls, available: List[str]) -> List[str]`:
       - Filter available features to only include HISTORICAL + PRACTICE + QUALIFYING
       - Return filtered list

    5. Add class method `validate_dataframe(cls, columns: List[str]) -> None`:
       - Call validate_features on column names
       - Raise ValueError if any columns are forbidden

    6. Add docstrings explaining temporal boundaries:
       - Historical: available (past race data)
       - Practice/Qualifying: available (pre-race sessions)
       - Race telemetry: NOT available (future information)

    DO NOT: Allow race telemetry features in validation, skip logging, or permit partial validation
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.telemetry.features import FeatureAvailabilityContract

    # Test forbidden feature detection
    try:
        FeatureAvailabilityContract.validate_features(['avg_finish_position', 'race_laps_led'])
        print('FAIL: Forbidden feature not detected')
    except ValueError as e:
        if 'race_laps_led' in str(e):
            print('PASS: Forbidden feature detected')
        else:
            print(f'FAIL: Wrong error: {e}')

    # Test allowed features
    allowed = FeatureAvailabilityContract.get_allowed_features(['avg_finish_position', 'practice_lap_time'])
    assert 'avg_finish_position' in allowed, 'Historical feature not allowed'
    assert 'practice_lap_time' in allowed, 'Practice feature not allowed'
    print('PASS: Allowed features filtered correctly')
    "
    ```
  </verify>
  <done>
    FeatureAvailabilityContract validates requested features and prevents data leakage from future race information.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Polars telemetry ingestion with lazy scan</name>
  <files>apps/backend/app/telemetry/ingest.py</files>
  <action>
    Create telemetry ingestion module in `apps/backend/app/telemetry/ingest.py` using Polars lazy API:

    1. Import dependencies:
       ```python
       import polars as pl
       from typing import List, Optional
       from pathlib import Path
       import logging
       from app.telemetry.features import FeatureAvailabilityContract
       ```

    2. Create `TelemetryIngestor` class:
       - `__init__(self, feature_contract: Optional[FeatureAvailabilityContract] = None)`: store contract or create default
       - `ingest_parquet(self, parquet_path: str, driver_ids: List[str]) -> pl.DataFrame`:
         * Use `pl.scan_parquet(parquet_path)` for lazy evaluation
         * Call `feature_contract.validate_dataframe()` on schema columns
         * Filter to requested drivers: `filter(pl.col("driver_id").is_in(driver_ids))`
         * Select allowed features using `get_allowed_features()`
         * Always include metadata columns: ["lap", "driver_id", "timestamp", "track_id"]
         * Call `.collect()` to execute lazy query
         * Log number of records ingested

    3. Create `ingest_lap_by_lap_telemetry()` standalone function:
       - Takes parquet_path, driver_ids, feature_contract parameters
       - Creates TelemetryIngestor and calls ingest_parquet
       - Returns DataFrame with lap-by-lap telemetry

    4. Add schema validation:
       - Check required metadata columns exist
       - Raise FileNotFoundError if parquet_path doesn't exist
       - Log warning if no data found for requested drivers

    5. Handle missing data:
       - Use `fill_null(strategy="forward")` for missing lap times
       - Use `fill_null(0)` for missing numeric metrics

    DO NOT: Use eager loading (pl.read_parquet), skip feature validation, or allow forbidden features
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.telemetry.ingest import ingest_lap_by_lap_telemetry
    from app.telemetry.features import FeatureAvailabilityContract
    import tempfile
    import polars as pl

    # Create test Parquet file
    df = pl.DataFrame({
        'lap': [1, 2, 3],
        'driver_id': ['driver_1', 'driver_1', 'driver_1'],
        'timestamp': ['2024-01-01T12:00:00', '2024-01-01T12:01:00', '2024-01-01T12:02:00'],
        'track_id': ['daytona', 'daytona', 'daytona'],
        'avg_finish_position': [10.5, 10.5, 10.5],  # Historical
        'practice_lap_time': [48.5, 48.3, 48.4],  # Practice
        'race_laps_led': [0, 5, 10]  # FORBIDDEN
    })

    with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as f:
        path = f.name
    df.write_parquet(path)

    # Test forbidden feature detection
    contract = FeatureAvailabilityContract()
    try:
        ingest_lap_by_lap_telemetry(path, ['driver_1'], contract)
        print('FAIL: Forbidden feature not detected during ingestion')
    except ValueError as e:
        if 'race_laps_led' in str(e) or 'data leakage' in str(e).lower():
            print('PASS: Forbidden feature blocked during ingestion')
        else:
            print(f'FAIL: Wrong error: {e}')

    import os
    os.unlink(path)
    "
    ```
  </verify>
  <done>
    TelemetryIngestor uses Polars lazy scan for efficient Parquet reading with feature availability validation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement Polars transformations with rolling windows</name>
  <files>apps/backend/app/telemetry/transform.py</files>
  <action>
    Create Polars transformation module in `apps/backend/app/telemetry/transform.py`:

    1. Import dependencies:
       ```python
       import polars as pl
       from typing import List, Dict
       import logging
       ```

    2. Create `compute_aggregate_features()` function:
       - Parameters: telemetry_df (pl.DataFrame), time_windows (List[str] = ["10l", "20l", "50l"])
       - Sort by lap: `telemetry_df.sort("lap")`
       - For each time window:
         * Parse window size (e.g., "10l" -> 10 laps)
         * Compute per-driver rolling aggregations:
           - Average position: `pl.col("position").rolling(window_size).mean().alias(f"avg_position_last_{window}")`
           - Best position: `pl.col("position").rolling(window_size).min().alias(f"best_position_last_{window}")`
           - Laps led sum: `pl.col("laps_led").rolling(window_size).sum().alias(f"laps_led_last_{window}")`
         * Use `groupby("driver_id").agg(...)` for driver-level aggregations
       - Join all rolling features back to original DataFrame
       - Return DataFrame with aggregate features

    3. Create `rolling_statistics()` function:
       - Parameters: telemetry_df (pl.DataFrame), metric (str), window (int)
       - Compute rolling mean, std, min, max for specified metric
       - Return DataFrame with rolling stats columns

    4. Create `compute_falloff_metrics()` function:
       - Parameters: telemetry_df (pl.DataFrame)
       - Compute lap time degradation: difference between consecutive laps
       - Identify slow laps (lap_time > rolling_mean + 2*rolling_std)
       - Return DataFrame with falloff indicators

    5. Create `handle_missing_data()` function:
       - Parameters: telemetry_df (pl.DataFrame)
       - Forward-fill missing values: `fill_null(strategy="forward")`
       - Interpolate missing numeric columns: `fill_null(strategy="interpolate")`
       - Log number of missing values handled
       - Return cleaned DataFrame

    6. Add input validation:
       - Check required columns exist
       - Validate time_windows format (e.g., "10l", "20l")
       - Raise ValueError for invalid inputs

    DO NOT: Use pandas (slow), skip input validation, or allow leakage from future laps
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.telemetry.transform import compute_aggregate_features, handle_missing_data
    import polars as pl

    # Create test telemetry data
    df = pl.DataFrame({
        'lap': [1, 2, 3, 4, 5],
        'driver_id': ['driver_1'] * 5,
        'position': [10, 9, 8, 12, 11],
        'laps_led': [0, 2, 5, 5, 7],
        'timestamp': ['2024-01-01T12:00:00'] * 5
    })

    # Test rolling features
    aggregated = compute_aggregate_features(df, ['3l'])
    assert 'avg_position_last_3l' in aggregated.columns, 'Rolling feature missing'
    assert 'laps_led_last_3l' in aggregated.columns, 'Rolling laps led missing'
    print('PASS: Rolling features computed')

    # Test missing data handling
    df_with_nulls = pl.DataFrame({
        'lap': [1, 2, 3],
        'driver_id': ['driver_1'] * 3,
        'position': [10.0, None, 8.0]
    })
    cleaned = handle_missing_data(df_with_nulls)
    assert cleaned['position'].null_count() == 0, 'Missing values not handled'
    print('PASS: Missing data handled')
    "
    ```
  </verify>
  <done>
    Polars transformations compute rolling window features and handle missing data with forward-fill/interpolation.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Feature availability check**: Attempting to ingest telemetry with `race_laps_led` column raises ValueError

2. **Lazy scan check**: TelemetryIngestor uses `pl.scan_parquet` (lazy) not `pl.read_parquet` (eager)

3. **Rolling window check**: `compute_aggregate_features` creates columns like `avg_position_last_10l`

4. **Missing data check**: `handle_missing_data` forward-fills None values in position column

5. **Schema validation check**: Missing required metadata columns raises ValueError

Test commands:
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
python -m pytest tests/test_telemetry.py -v
```
</verification>

<success_criteria>
- Telemetry pipeline ingests lap-by-lap data with Polars lazy scan and feature availability validation
- No data leakage: forbidden features (race telemetry) blocked with clear error messages
- Rolling window features computed (10l, 20l, 50l) for driver performance metrics
- Missing data handled gracefully with forward-fill and interpolation
- Parquet artifacts persisted with Snappy compression for fast loading
</success_criteria>

<output>
After completion, create `.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-02-SUMMARY.md` with:
- Files created (telemetry module with ingest, transform, features, artifacts)
- Feature availability contract documentation (historical, practice, qualifying vs forbidden)
- Test coverage (property-based tests for data leakage prevention)
- Sample telemetry artifacts created (test Parquet files)
- Next steps (integrate telemetry with calibration harness)
</output>
