---
phase: 02-ontology-compiled-constraints-calibration-harness
plan: 03
type: execute
wave: 2
depends_on: [02-01, 02-02]
files_modified:
  - apps/backend/app/calibration/__init__.py
  - apps/backend/app/calibration/metrics.py
  - apps/backend/app/calibration/models.py
  - apps/backend/app/calibration/diagnostics.py
  - apps/backend/app/tests/test_calibration.py
  - packages/axiomatic-sim/pyproject.toml
autonomous: true

user_setup:
  - service: none
    why: "No external services required - calibration uses local NumPyro MCMC and ArviZ diagnostics"

must_haves:
  truths:
    - "Calibration metrics computed (CRPS, log score, coverage) for track-archetype predictions"
    - "NumPyro MCMC calibration model samples posterior distributions for track-type uncertainty"
    - "ArviZ diagnostics perform posterior predictive checks and calibration assessment"
    - "Joint-event validation checks co-hit frequencies by track type (superspeedway, intermediate, short track)"
    - "Calibration results persisted as NetCDF files for reproducibility"
  artifacts:
    - path: "apps/backend/app/calibration/metrics.py"
      provides: "CRPS, log score, and coverage calculations"
      exports: ["compute_crps", "compute_log_score", "compute_coverage"]
      min_lines: 100
    - path: "apps/backend/app/calibration/models.py"
      provides: "NumPyro calibration models with NUTS sampling"
      exports: ["track_archetype_calibration_model", "run_mcmc_calibration"]
      min_lines: 150
    - path: "apps/backend/app/calibration/diagnostics.py"
      provides: "ArviZ posterior predictive checks and calibration plots"
      exports: ["posterior_predictive_check", "plot_calibration_curve", "compute_joint_event_validation"]
      min_lines: 120
    - path: "apps/backend/app/tests/test_calibration.py"
      provides: "Property-based tests for calibration metrics"
      exports: ["test_crps_properties", "test_mcmc_convergence"]
      min_lines: 80
  key_links:
    - from: "apps/backend/app/calibration/models.py"
      to: "apps/backend/app/telemetry/transform.py"
      via: "Calibration trained on aggregated telemetry features"
      pattern: "compute_aggregate_features.*telemetry_df"
    - from: "apps/backend/app/calibration/metrics.py"
      to: "packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py"
      via: "Metrics evaluated on CBN-sampled scenario outcomes"
      pattern: "SkeletonNarrative\\.generate_scenarios"
    - from: "apps/backend/app/calibration/diagnostics.py"
      to: "apps/backend/app/constraints/models.py"
      via: "Calibration assessed by track archetype from compiled constraints"
      pattern: "TrackConstraints\\.track_id.*archetype"
---

<objective>
Create probabilistic calibration harness for track-archetype uncertainty quantification using NumPyro for MCMC sampling and ArviZ for CRPS/log score/coverage diagnostics, enabling assessment of how well CBN-sampled scenarios match observed outcomes by track type.

Purpose: Before trusting simulation outputs for optimization, we need to calibrate uncertainty by comparing predicted distributions to observed outcomes. Different track types (superspeedway, intermediate, short track) have different prediction uncertainties, and calibration metrics quantify how well our models capture this uncertainty. NumPyro provides JAX-accelerated MCMC for efficient Bayesian calibration.

Output: Calibration harness with NumPyro NUTS sampler, ArviZ diagnostics (CRPS, log score, coverage), and joint-event validation for track-archetype-specific assessment.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-RESEARCH.md

@apps/backend/app/telemetry/transform.py
@apps/backend/app/constraints/models.py
@packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement calibration metrics (CRPS, log score, coverage)</name>
  <files>apps/backend/app/calibration/metrics.py</files>
  <action>
    Create calibration metrics module in `apps/backend/app/calibration/metrics.py`:

    1. Import dependencies:
       ```python
       import jax.numpy as jnp
       import numpy as np
       from typing import Dict, List, Tuple
       import logging
       ```

    2. Create `compute_crps(predictions: jnp.ndarray, observations: jnp.ndarray) -> float` function:
       - CRPS (Continuous Ranked Probability Score) measures probabilistic prediction accuracy
       - For each observation, compute: CRPS = ∫(F(x) - 1(x ≥ y))² dx where F is CDF, y is observation
       - Approximate using: mean(2 * predictions - observations) when predictions are ensemble
       - Handle shape: (n_samples, n_predictions) predictions, (n_predictions,) observations
       - Return scalar CRPS value (lower is better)
       - Add docstring with formula and interpretation

    3. Create `compute_log_score(predictions: jnp.ndarray, observations: jnp.ndarray) -> float` function:
       - Log score measures predictive likelihood
       - Convert predictions to probability distribution using kernel density estimation
       - Compute log probability of observations under predictive distribution
       - Return mean log score (higher is better)
       - Handle edge cases: zero probability (add small epsilon), NaN/infinite values

    4. Create `compute_coverage(predictions: jnp.ndarray, observations: jnp.ndarray, levels: List[float] = [0.5, 0.8, 0.95]) -> Dict[float, float]` function:
       - Coverage checks if observations fall within prediction intervals
       - For each confidence level (e.g., 0.5, 0.8, 0.95):
         * Compute prediction intervals: percentiles at [(1-level)/2, 1-(1-level)/2]
         * Count observations within interval
         * Compute empirical coverage: count / total
       - Return dict mapping level -> empirical_coverage
       - Log warnings if coverage deviates >10% from nominal (miscalibration)

    5. Add `compute_all_metrics()` function:
       - Takes predictions, observations, optional levels
       - Calls compute_crps, compute_log_score, compute_coverage
       - Returns dict with all metrics

    6. Add input validation:
       - Check shapes match (except sample dimension for predictions)
       - Validate levels in (0, 1)
       - Handle NaN/infinite values

    DO NOT: Use slow Python loops, skip validation, or return NaN without error handling
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.calibration.metrics import compute_crps, compute_log_score, compute_coverage
    import numpy as np

    # Create test data: predictions from 100 samples, 10 observations
    np.random.seed(42)
    predictions = np.random.randn(100, 10) + 5  # Mean 5, std 1
    observations = np.random.randn(10) + 5  # Mean 5, std 1

    # Test CRPS
    crps = compute_crps(predictions, observations)
    assert crps > 0, 'CRPS should be positive'
    assert crps < 10, 'CRPS should be reasonable for close predictions'
    print(f'PASS: CRPS = {crps:.3f}')

    # Test coverage
    coverage = compute_coverage(predictions, observations, [0.5, 0.95])
    assert 0.5 in coverage, '0.5 level missing'
    assert 0.0 <= coverage[0.5] <= 1.0, 'Coverage out of range'
    print(f'PASS: Coverage at 0.5 = {coverage[0.5]:.3f}')

    # Test log score
    log_score = compute_log_score(predictions, observations)
    assert not np.isnan(log_score), 'Log score is NaN'
    print(f'PASS: Log score = {log_score:.3f}')
    "
    ```
  </verify>
  <done>
    Calibration metrics (CRPS, log score, coverage) compute correctly for probabilistic predictions vs observations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement NumPyro calibration models with MCMC</name>
  <files>apps/backend/app/calibration/models.py</files>
  <action>
    Create NumPyro calibration models in `apps/backend/app/calibration/models.py`:

    1. Import dependencies:
       ```python
       import jax
       import jax.numpy as jnp
       import jax.random as random
       import numpyro
       import numpyro.distributions as dist
       from numpyro.infer import MCMC, NUTS, Predictive
       from typing import Dict, List, Optional
       import logging
       ```

    2. Create `track_archetype_calibration_model()` function:
       - Parameters: observed_finish_positions (jnp.ndarray), predicted_finish_probs (jnp.ndarray), track_archetype (str)
       - Model structure:
         * Hyperpriors for calibration parameters:
           - `slope_mu = numpyro.sample("slope_mu", dist.Normal(0.0, 1.0))`
           - `slope_sigma = numpyro.sample("slope_sigma", dist.HalfNormal(0.5))`
           - `slope = numpyro.sample("slope", dist.Normal(slope_mu, slope_sigma))` (per-archetype slope)
           - `intercept = numpyro.sample("intercept", dist.Normal(0.0, 1.0))`
           - `noise_sigma = numpyro.sample("noise_sigma", dist.HalfNormal(1.0))`
         * Calibration transformation: `calibrated_probs = jax.scipy.special.sigmoid(slope * logit(predicted_probs) + intercept)`
         * Likelihood: `numpyro.sample("observed", dist.Categorical(probs=calibrated_probs), obs=observed_finish_positions)`
       - Add docstring explaining calibration parameters

    3. Create `run_mcmc_calibration()` function:
       - Parameters: observed_data, predicted_probs, track_archetype, n_warmup=500, n_samples=1000, random_seed=42
       - Set up NUTS kernel: `kernel = NUTS(track_archetype_calibration_model)`
       - Create MCMC sampler: `mcmc = MCMC(kernel, num_warmup=n_warmup, num_samples=n_samples)`
       - Run MCMC: `mcmc.run(random.PRNGKey(random_seed), observed_finish_positions=..., predicted_finish_probs=..., track_archetype=...)`
       - Extract samples: `mcmc.get_samples()`
       - Return samples dict and mcmc object (for diagnostics)

    4. Create `predict_with_calibrated_model()` function:
       - Parameters: mcmc_samples, predicted_probs, track_archetype
       - Use `Predictive` to generate posterior predictive samples
       - Return calibrated predictions

    5. Add `compute_calibration_summary()` function:
       - Takes MCMC samples
       - Computes posterior means, HDI intervals for calibration parameters
       - Returns dict with calibration summaries per track archetype

    6. Add error handling:
       - Check input shapes match
       - Validate track_archetype is one of: superspeedway, intermediate, short_track, road_course
       - Handle MCMC convergence warnings

    DO NOT: Use PyMC instead of NumPyro, skip warmup, or use insufficient MCMC samples (<500)
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.calibration.models import run_mcmc_calibration, compute_calibration_summary
    import jax.numpy as jnp
    import jax.random as random

    # Create synthetic test data
    key = random.PRNGKey(42)
    n_races, n_drivers = 50, 40

    # Observed finish positions (1-40)
    observed = random.randint(key, (n_races, n_drivers), 1, 41)

    # Predicted probabilities (naive uniform for testing)
    predicted = jnp.ones((n_races, n_drivers, 40)) / 40.0

    # Test MCMC calibration
    print('Running MCMC calibration (this may take 30-60 seconds)...')
    samples, mcmc = run_mcmc_calibration(
        observed, predicted, 'intermediate',
        n_warmup=100, n_samples=200, random_seed=42
    )

    # Check samples exist
    assert 'slope' in samples, 'Slope parameter missing'
    assert 'intercept' in samples, 'Intercept parameter missing'
    print(f'PASS: MCMC completed, {len(samples[\"slope\"])} samples collected')

    # Test summary
    summary = compute_calibration_summary(samples, 'intermediate')
    assert 'slope_mean' in summary, 'Slope mean missing from summary'
    print(f'PASS: Calibration summary computed, slope_mean={summary[\"slope_mean\"]:.3f}')
    "
    ```
  </verify>
  <done>
    NumPyro MCMC calibration model samples posterior distributions for track-archetype-specific calibration parameters.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement ArviZ diagnostics and joint-event validation</name>
  <files>apps/backend/app/calibration/diagnostics.py</files>
  <action>
    Create ArviZ diagnostics module in `apps/backend/app/calibration/diagnostics.py`:

    1. Import dependencies:
       ```python
       import arviz as az
       import jax.numpy as jnp
       import numpy as np
       from typing import Dict, List, Tuple
       import logging
       from app.calibration.metrics import compute_crps, compute_log_score, compute_coverage
       ```

    2. Create `posterior_predictive_check()` function:
       - Parameters: mcmc_samples (Dict), predicted_probs (jnp.ndarray), observed (jnp.ndarray), track_archetype (str)
       - Convert NumPyro samples to ArviZ InferenceData:
         * Use `az.from_numpyro()` if available, or manual conversion
         * Create posterior group with calibration parameters
         * Create posterior_predictive group with predicted observations
       - Run ArviZ posterior predictive checks: `az.compare()` on observed vs predicted
       - Return ArviZ InferenceData object

    3. Create `plot_calibration_curve()` function:
       - Parameters: predictions, observations, track_archetype, output_path (Optional[str])
       - Bin predicted probabilities into quantiles (e.g., 10 bins)
       - For each bin, compute empirical frequency of observed events
       - Plot observed frequency vs predicted probability (ideal: 45-degree line)
       - Use matplotlib for plotting
       - Save to output_path if provided
       - Return figure object

    4. Create `compute_joint_event_validation()` function:
       - Parameters: predictions (Dict[str, jnp.ndarray]), observations (jnp.ndarray), track_types (List[str])
       - For each track type:
         * Extract predictions and observations for that track
         * Define joint events: e.g., "top 5 finish AND no DNF", "led laps AND top 10"
         * Compute predicted probability of joint event from ensemble
         * Compute empirical frequency from observations
         * Compute calibration error: |predicted - observed|
       - Return dict mapping (track_type, joint_event) -> calibration_error
       - Log warnings for calibration error > 0.1 (miscalibrated)

    5. Create `generate_calibration_report()` function:
       - Parameters: mcmc_samples, predictions, observations, track_archetypes, output_path
       - Run all diagnostics:
         * Compute metrics (CRPS, log score, coverage) via metrics module
         * Posterior predictive check via ArviZ
         * Joint event validation
       - Compile into markdown report with:
         * Per-archetype metrics tables
         * Calibration curve plots (embedded as base64 or saved separately)
         * Joint event validation results
         * MCMC convergence diagnostics (R-hat, ESS)
       - Write report to output_path

    6. Add `assess_mcmc_convergence()` function:
       - Takes ArviZ InferenceData object
       - Computes R-hat statistics (should be < 1.05 for convergence)
       - Computes effective sample size (ESS)
       - Returns convergence summary dict

    DO NOT: Skip convergence checks, generate reports without plots, or omit joint-event validation
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.calibration.diagnostics import compute_joint_event_validation, assess_mcmc_convergence
    from app.calibration.metrics import compute_all_metrics
    import jax.numpy as jnp
    import jax.random as random
    import tempfile
    import os

    # Create test data for multiple track types
    key = random.PRNGKey(42)
    predictions = {
        'superspeedway': random.uniform(key, (100, 40)),
        'intermediate': random.uniform(key, (100, 40))
    }
    observations = random.randint(key, (40,), 1, 41)

    # Test joint event validation
    joint_validation = compute_joint_event_validation(predictions, observations, ['superspeedway', 'intermediate'])
    assert len(joint_validation) > 0, 'No joint events validated'
    print(f'PASS: Joint event validation computed for {len(joint_validation)} events')

    # Test metrics
    metrics = compute_all_metrics(predictions['superspeedway'][:10], observations[:10])
    assert 'crps' in metrics, 'CRPS missing'
    assert 'log_score' in metrics, 'Log score missing'
    assert 'coverage' in metrics, 'Coverage missing'
    print(f'PASS: All metrics computed (CRPS={metrics[\"crps\"]:.3f})')

    print('PASS: Diagnostics module functional')
    "
    ```
  </verify>
  <done>
    ArviZ diagnostics perform posterior predictive checks, calibration curves, and joint-event validation for track-archetype assessment.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Metrics check**: CRPS, log score, and coverage computed without errors for synthetic predictions

2. **MCMC check**: NumPyro MCMC completes sampling (500 warmup + 1000 samples) and returns posterior samples

3. **Convergence check**: R-hat statistics < 1.05 for all calibration parameters

4. **Diagnostics check**: Posterior predictive check generates InferenceData object with posterior and posterior_predictive groups

5. **Joint-event check**: Joint-event validation computes calibration errors for top-5/no-DNF type events

Test commands:
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
python -m pytest tests/test_calibration.py -v
```
</verification>

<success_criteria>
- Calibration metrics (CRPS, log score, coverage) computed for track-archetype predictions
- NumPyro MCMC calibration model samples posterior distributions with NUTS
- ArviZ diagnostics perform posterior predictive checks and assess convergence (R-hat < 1.05)
- Joint-event validation checks co-hit frequencies by track type (superspeedway, intermediate, short track)
- Calibration harness integrated with telemetry and scenario generation for end-to-end assessment
</success_criteria>

<output>
After completion, create `.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-03-SUMMARY.md` with:
- Files created (calibration module with metrics, models, diagnostics)
- NumPyro MCMC configuration (warmup=500, samples=1000, NUTS kernel)
- Sample calibration report (markdown with CRPS/log score/coverage tables)
- Joint-event validation results (calibration errors by track type and event)
- Next steps (integrate calibration with headless /optimize API)
</output>
