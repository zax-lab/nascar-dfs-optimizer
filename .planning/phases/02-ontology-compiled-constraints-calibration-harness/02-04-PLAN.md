---
phase: 02-ontology-compiled-constraints-calibration-harness
plan: 04
type: execute
wave: 3
depends_on: [02-01, 02-03]
files_modified:
  - apps/backend/app/api/__init__.py
  - apps/backend/app/api/optimize.py
  - apps/backend/app/api/contracts.py
  - apps/backend/app/main.py
  - apps/backend/app/tests/test_api.py
autonomous: true

must_haves:
  truths:
    - "POST /optimize endpoint accepts scenario-driven configs and returns lineups + diagnostics"
    - "Request validation via Pydantic models ensures type-safe API contracts"
    - "Multiple optimization requests can run concurrently with independent status tracking"
    - "Response includes lineup projections, scenario metadata, and calibration metrics"
    - "API returns helpful error messages for invalid requests and failed optimizations"
  artifacts:
    - path: "apps/backend/app/api/optimize.py"
      provides: "FastAPI /optimize endpoint with scenario-driven configs"
      exports: ["optimize_lineup", "get_optimization_status"]
      min_lines: 150
    - path: "apps/backend/app/api/contracts.py"
      provides: "Pydantic request/response models for API contracts"
      exports: ["OptimizeRequest", "OptimizeResponse", "ScenarioConfig", "Diagnostics"]
      min_lines: 100
    - path: "apps/backend/app/main.py"
      provides: "FastAPI app with /optimize route registered"
      exports: ["app", "include_router"]
      min_lines: 50 (modifications only)
    - path: "apps/backend/app/tests/test_api.py"
      provides: "API contract tests with request/response validation"
      exports: ["test_optimize_endpoint", "test_scenario_config_validation"]
      min_lines: 100
  key_links:
    - from: "apps/backend/app/api/optimize.py"
      to: "apps/backend/app/constraints/compiler.py"
      via: "Optimization uses compiled ConstraintSpec from request"
      pattern: "ConstraintCompiler\\.compile_spec"
    - from: "apps/backend/app/api/optimize.py"
      to: "packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py"
      via: "Optimization uses SkeletonNarrative for scenario-driven outcomes"
      pattern: "SkeletonNarrative\\.generate_scenarios"
    - from: "apps/backend/app/api/optimize.py"
      to: "apps/backend/app/calibration/diagnostics.py"
      via: "Background optimization assesses calibration and returns metrics"
      pattern: "assess_scenario_calibration|compute_all_metrics"
    - from: "apps/backend/app/api/contracts.py"
      to: "apps/backend/app/constraints/versioning.py"
      via: "Request includes RunConfig for reproducibility"
      pattern: "OptimizeRequest\\.run_config"
---

<objective>
Implement headless `/optimize` API endpoint with scenario-driven optimization contracts, accepting Pydantic-validated requests with compiled constraints and returning lineups with scenario diagnostics and calibration metrics.

Purpose: External systems need programmatic access to DFS optimization without interactive UI. Headless API with Pydantic contracts ensures type-safe requests/responses, scenario-driven configs enable reproducible runs, and async background processing prevents timeouts for long-running optimizations (1000+ scenarios).

Output: FastAPI `/optimize` endpoint with Pydantic request/response models, async background optimization with status polling, and scenario diagnostics (regime, conservation metadata, calibration metrics).
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-RESEARCH.md

@apps/backend/app/constraints/compiler.py
@apps/backend/app/constraints/versioning.py
@packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py
@apps/backend/app/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Pydantic API contracts for request/response</name>
  <files>apps/backend/app/api/contracts.py</files>
  <action>
    Create Pydantic contract models in `apps/backend/app/api/contracts.py`:

    1. Import dependencies:
       ```python
       from pydantic import BaseModel, Field, validator, constr
       from typing import List, Dict, Any, Optional
       from enum import Enum
       ```

    2. Create `PitStrategy` enum:
       - Values: AGGRESSIVE, STANDARD, CONSERVATIVE

    3. Create `ScenarioConfig` BaseModel:
       - Fields:
         * n_scenarios: int = Field(1000, ge=10, le=10000, description="Number of scenarios to generate")
         * track_id: str = Field(..., min_length=1, description="Track identifier")
         * race_length: int = Field(200, ge=50, le=500, description="Total race laps")
         * field_size: int = Field(40, ge=6, le=50, description="Number of drivers")
         * calibration_enabled: bool = Field(True, description="Whether to use calibrated probabilities")
       - Add validators: n_scenarios divisible by 10 for efficient batching

    4. Create `DriverConstraintsRequest` BaseModel:
       - Fields:
         * driver_id: str
         * skill: float = Field(..., ge=0, le=1)
         * aggression: float = Field(..., ge=0, le=1)
         * shadow_risk: float = Field(..., ge=0, le=1)
         * min_laps_led: int = Field(0, ge=0)
         * max_laps_led: int = Field(100, ge=0)
       - Add validator: min_laps_led <= max_laps_led

    5. Create `OptimizeRequest` BaseModel:
       - Fields:
         * slate_id: str = Field(..., min_length=1, description="Unique slate identifier")
         * drivers: List[DriverConstraintsRequest] = Field(..., min_items=6, max_items=50)
         * scenario_config: ScenarioConfig
         * salary_cap: int = Field(50000, ge=10000, le=100000)
         * random_seed: Optional[int] = Field(None, ge=0, description="Random seed for reproducibility")
       - Add validator: all driver_ids unique

    6. Create `DriverSelection` BaseModel:
       - Fields: driver_id, name, salary, projected_points, position

    7. Create `ScenarioDiagnostics` BaseModel:
       - Fields:
         * n_scenarios_generated: int
         * n_valid: int
         * rejection_rate: float
         * avg_laps_led: float
         * avg_position_differential: float
         * calibration_metrics: Optional[Dict[str, float]] = None

    8. Create `OptimizeResponse` BaseModel:
       - Fields:
         * lineup: List[DriverSelection]
         * total_projected_points: float
         * total_salary: int
         * scenario_diagnostics: ScenarioDiagnostics
         * run_id: str
         * status: str
         * constraint_spec_hash: str

    9. Create `OptimizationStatus` BaseModel:
       - Fields:
         * run_id: str
         * status: str  # pending, running, completed, failed
         * progress: float  # 0.0 to 1.0
         * error: Optional[str] = None

    DO NOT: Use regular classes (not Pydantic), skip validation, or allow missing required fields
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from app.api.contracts import OptimizeRequest, ScenarioConfig, DriverConstraintsRequest
    from pydantic import ValidationError

    # Test valid request
    try:
        req = OptimizeRequest(
            slate_id='test_slate',
            drivers=[
                DriverConstraintsRequest(
                    driver_id='d1', skill=0.5, aggression=0.5, shadow_risk=0.5,
                    min_laps_led=0, max_laps_led=100
                )
            ] * 6,
            scenario_config=ScenarioConfig(track_id='daytona'),
            salary_cap=50000,
            random_seed=42
        )
        print('PASS: Valid request accepted')
    except ValidationError as e:
        print(f'FAIL: Valid request rejected: {e}')

    # Test invalid request (skill > 1)
    try:
        req = OptimizeRequest(
            slate_id='test_slate',
            drivers=[
                DriverConstraintsRequest(
                    driver_id='d1', skill=1.5, aggression=0.5, shadow_risk=0.5,
                    min_laps_led=0, max_laps_led=100
                )
            ] * 6,
            scenario_config=ScenarioConfig(track_id='daytona'),
            salary_cap=50000
        )
        print('FAIL: Invalid skill not rejected')
    except ValidationError as e:
        if 'skill' in str(e):
            print('PASS: Invalid skill rejected')
        else:
            print(f'FAIL: Wrong validation: {e}')
    "
    ```
  </verify>
  <done>
    Pydantic contract models validate requests with type checking and constraints, preventing invalid API calls.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement /optimize endpoint with async background processing</name>
  <files>apps/backend/app/api/optimize.py</files>
  <action>
    Create `/optimize` endpoint in `apps/backend/app/api/optimize.py`:

    1. Import dependencies:
       ```python
       from fastapi import APIRouter, HTTPException, BackgroundTasks, status
       from typing import Dict
       import logging
       import uuid
       from app.api.contracts import OptimizeRequest, OptimizeResponse, OptimizationStatus
       from app.constraints.compiler import ConstraintCompiler
       from app.constraints.versioning import create_run_config
       from packages.axiomatic_sim.src.axiomatic_sim.scenario_generator import generate_scenarios
       from app.optimizer import LineupOptimizer
       ```

    2. Create global state store:
       ```python
       optimization_jobs: Dict[str, OptimizationStatus] = {}
       optimization_results: Dict[str, OptimizeResponse] = {}
       ```

    3. Create `run_optimization_background()` function:
       - Parameters: run_id, request, background_tasks
       - Update status to "running"
       - Compile constraints from request.drivers
       - Create RunConfig with random_seed from request
       - Generate scenarios using generate_scenarios()
       - Run optimization using LineupOptimizer
       - Assess calibration: call `assess_scenario_calibration()` or `compute_all_metrics()` from calibration module
       - Build ScenarioDiagnostics.calibration_metrics with CRPS, log_score, coverage
       - Build OptimizeResponse with lineup + diagnostics (including calibration_metrics)
       - Store result in optimization_results
       - Update status to "completed"
       - Handle exceptions: set status to "failed" with error message

    4. Create `POST /optimize` endpoint:
       - Route: `/optimize`
       - Accepts: OptimizeRequest
       - Returns: OptimizationStatus with run_id
       - Generate unique run_id using uuid.uuid4()
       - Create OptimizationStatus (status="pending", progress=0.0)
       - Store in optimization_jobs
       - Add background task: `background_tasks.add_task(run_optimization_background, run_id, request, background_tasks)`
       - Return status immediately

    5. Create `GET /optimize/{run_id}/status` endpoint:
       - Route: `/optimize/{run_id}/status`
       - Returns: OptimizationStatus
       - Look up run_id in optimization_jobs
       - Return 404 if not found
       - Return current status with progress

    6. Create `GET /optimize/{run_id}/result` endpoint:
       - Route: `/optimize/{run_id}/result`
       - Returns: OptimizeResponse
       - Look up run_id in optimization_results
       - Return 404 if not found or not completed
       - Return 202 if still running
       - Return OptimizeResponse when complete

    7. Add error handling:
       - Catch ConstraintCompiler errors (invalid constraints)
       - Catch scenario generation errors (invalid config)
       - Catch optimization errors (no valid lineup)
       - Return HTTPException with appropriate status codes

    8. Add rate limiting:
       - Use slowapi Limiter (already configured in main.py)
       - Limit: 10 requests per minute per IP

    DO NOT: Use synchronous processing (blocks request), skip background tasks, or return results without status polling
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from fastapi.testclient import TestClient
    from app.main import app
    from app.api.contracts import OptimizeRequest, ScenarioConfig, DriverConstraintsRequest

    client = TestClient(app)

    # Test optimize endpoint
    request_data = {
        'slate_id': 'test_slate',
        'drivers': [
            {
                'driver_id': f'd{i}',
                'skill': 0.5,
                'aggression': 0.5,
                'shadow_risk': 0.5,
                'min_laps_led': 0,
                'max_laps_led': 100
            }
            for i in range(1, 7)
        ],
        'scenario_config': {
            'track_id': 'daytona',
            'n_scenarios': 100
        },
        'salary_cap': 50000,
        'random_seed': 42
    }

    # Submit optimization
    response = client.post('/optimize', json=request_data)
    assert response.status_code == 200, f'POST /optimize failed: {response.text}'
    run_id = response.json()['run_id']
    print(f'PASS: Optimization submitted, run_id={run_id}')

    # Check status
    status_response = client.get(f'/optimize/{run_id}/status')
    assert status_response.status_code == 200, f'GET status failed: {status_response.text}'
    status = status_response.json()
    assert 'status' in status, 'Status missing'
    print(f'PASS: Status check returned: {status[\"status\"]}')
    "
    ```
  </verify>
  <done>
    POST /optimize endpoint accepts scenario-driven configs, runs optimization in background, and returns run_id for status polling.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate /optimize route with FastAPI app</name>
  <files>apps/backend/app/main.py</files>
  <action>
    Integrate `/optimize` route with existing FastAPI app in `apps/backend/app/main.py`:

    1. Import optimize router:
       ```python
       from app.api.optimize import router as optimize_router
       ```

    2. Include router in app:
       - Add after existing routes: `app.include_router(optimize_router, prefix="/api/v1", tags=["optimization"])`
       - This prefixes all optimize routes with `/api/v1/optimize`

    3. Update existing POST /optimize endpoint (if exists):
       - Remove old `/optimize` endpoint (lines 131-255)
       - Replace with router include
       - Keep backward compatibility if needed (add deprecation warning)

    4. Update OpenAPI documentation:
       - Add tags description for "optimization" tag
       - Add summary and descriptions for API docs
       - Example:
         ```python
         app.tags_metadata = [
             {
                 "name": "optimization",
                 "description": "Operations for DFS lineup optimization with scenario-driven configs"
             }
         ]
         ```

    5. Add CORS middleware (if not present):
       - Allow CORS for frontend integration
       - Configure allowed origins, methods, headers
       - Example:
         ```python
         from fastapi.middleware.cors import CORSMiddleware
         app.add_middleware(
             CORSMiddleware,
             allow_origins=["*"],  # Configure appropriately for production
             allow_methods=["*"],
             allow_headers=["*"],
         )
         ```

    6. Verify existing endpoints still work:
       - GET /health should remain functional
       - Rate limiting should apply to new routes

    DO NOT: Break existing /optimize endpoint without migration, skip CORS configuration, or remove rate limiting
  </action>
  <verify>
    ```bash
    cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend && python -c "
    from fastapi.testclient import TestClient
     from app.main import app

    client = TestClient(app)

    # Test existing health endpoint
    response = client.get('/health')
    assert response.status_code == 200, 'Health check broken'
    assert response.json()['status'] == 'ok', 'Health check returned wrong status'
    print('PASS: Health check functional')

    # Test new optimize route with prefix
    response = client.get('/api/v1/docs')
    assert response.status_code == 200, 'API docs not accessible'
    print('PASS: API docs accessible at /api/v1/docs')

    # Test OpenAPI schema includes optimize endpoints
    response = client.get('/openapi.json')
    schema = response.json()
    assert '/api/v1/optimize' in schema['paths'], 'Optimize endpoint not in schema'
    assert '/api/v1/optimize/{run_id}/status' in schema['paths'], 'Status endpoint not in schema'
    print('PASS: Optimize endpoints in OpenAPI schema')
    "
    ```
  </verify>
  <done>
    FastAPI app includes /optimize router with `/api/v1` prefix, existing endpoints remain functional, OpenAPI docs updated.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Contract validation check**: Invalid request (skill=1.5) returns ValidationError with clear message

2. **Async processing check**: POST /optimize returns run_id immediately (<100ms), optimization runs in background

3. **Status polling check**: GET /optimize/{run_id}/status returns current status (pending/running/completed/failed)

4. **Result retrieval check**: GET /optimize/{run_id}/result returns OptimizeResponse when complete, 202 if running, 404 if not found

5. **Integration check**: Existing /health endpoint still works, new routes prefixed with /api/v1

Test commands:
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
python -m pytest tests/test_api.py -v
```
</verification>

<success_criteria>
- POST /optimize accepts scenario-driven configs (slate_id, drivers, scenario_config, salary_cap, random_seed)
- Pydantic models validate requests with type checking and constraints
- Optimization runs asynchronously with status polling endpoint
- Response includes lineup + scenario diagnostics (n_scenarios, rejection_rate, calibration_metrics)
- Rate limiting (10 req/min) prevents abuse
- OpenAPI documentation auto-generated for API contracts
</success_criteria>

<output>
After completion, create `.planning/phases/02-ontology-compiled-constraints-calibration-harness/02-04-SUMMARY.md` with:
- Files created (api module with optimize.py, contracts.py)
- FastAPI routes registered (/api/v1/optimize, /api/v1/optimize/{run_id}/status, /api/v1/optimize/{run_id}/result)
- Request/response contracts documented (Pydantic models with validation)
- Example API calls (curl commands for testing)
- Next steps (integrate with frontend dashboard)
</output>
