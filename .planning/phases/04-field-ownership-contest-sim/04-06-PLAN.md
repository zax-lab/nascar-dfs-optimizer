---
phase: 04-field-ownership-contest-sim
plan: 06
type: execute
wave: 3
depends_on: [04-01, 04-02, 04-03, 04-04, 04-05]
files_modified:
  - apps/backend/app/api/contracts.py
  - apps/backend/app/main.py
  - apps/backend/app/contest/tests/test_contest_api_integration.py
autonomous: true

must_haves:
  truths:
    - "API exposes /ownership endpoint for ownership estimation"
    - "API exposes /contest-sim endpoint for contest simulation"
    - "API exposes /optimize-with-leverage endpoint for leverage-aware optimization"
    - "API request/response models validate all input/output data"
    - "End-to-end integration tests validate full pipeline"
  artifacts:
    - path: "apps/backend/app/api/contracts.py"
      provides: "API Pydantic models for ownership and contest endpoints"
      exports: ["OwnershipRequest", "ContestSimRequest", "LeverageOptimizeRequest"]
      min_lines: 150
    - path: "apps/backend/app/main.py"
      provides: "FastAPI endpoints for ownership, contest sim, leverage optimization"
      exports: ["/ownership", "/contest-sim", "/optimize-with-leverage"]
      min_lines: 100
    - path: "apps/backend/app/contest/tests/test_contest_api_integration.py"
      provides: "Integration tests for contest simulation API"
      exports: ["test_ownership_endpoint", "test_contest_sim_endpoint", "test_leverage_optimize_endpoint"]
      min_lines: 300
  key_links:
    - from: "apps/backend/app/main.py"
      to: "apps/backend/app/ownership/ensemble.py"
      via: "HybridOwnershipEstimator for ownership endpoint"
      pattern: "from.*ownership.ensemble import HybridOwnershipEstimator"
    - from: "apps/backend/app/main.py"
      to: "apps/backend/app/contest/contest_sim.py"
      via: "ContestSimulator for contest-sim endpoint"
      pattern: "from.*contest.contest_sim import ContestSimulator"
    - from: "apps/backend/app/main.py"
      to: "apps/backend/app/optimizer/leverage_aware.py"
      via: "LeverageAwareOptimizer for optimize-with-leverage endpoint"
      pattern: "from.*optimizer.leverage_aware import LeverageAwareOptimizer"
---

<objective>
Integrate ownership estimation, contest simulation, and leverage-aware optimization into the FastAPI backend with proper request/response validation and end-to-end integration tests.

Purpose: Expose Phase 4 functionality through REST API endpoints with validated contracts
Output: Three new API endpoints (/ownership, /contest-sim, /optimize-with-leverage) with integration tests
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-field-ownership-contest-sim/04-CONTEXT.md
@.planning/phases/04-field-ownership-contest-sim/04-RESEARCH.md

# Reference existing API structure
@apps/backend/app/main.py
@apps/backend/app/api/contracts.py

# Reference all Phase 4 components
@apps/backend/app/ownership/ensemble.py
@apps/backend/app/contest/contest_sim.py
@apps/backend/app/optimizer/leverage_aware.py

# Reference existing API patterns from Phase 2-3
@apps/backend/app/api/
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create API Pydantic models</name>
  <files>apps/backend/app/api/contracts.py</files>
  <action>
Extend api/contracts.py with Phase 4 request/response models:

**Add to apps/backend/app/api/contracts.py:**

Import existing ownership and contest models:
- from apps.backend.app.ownership.models import OwnershipRequest, OwnershipResponse
- from apps/backend.app.contest.models import ContestSize, HistoricalContestData

Create ContestSimRequest model:
- my_lineup_scores: List[float]
- scenario_driver_scores: List[List[float]]  # n_scenarios x n_drivers
- field_size: int = 1000
- n_contest_sims: int = 100
- contest_buyin: float = 20.0
- contest_size_tier: str = 'large'  # 'small', 'medium', 'large'

Create ContestSimResponse model:
- roi: float
- roi_std: float
- roi_lower_5: float
- roi_upper_95: float
- cash_pct: float
- cash_se: float
- win_prob: float
- win_se: float
- best_rank: int
- worst_rank: int
- best_payout: float
- n_simulations: int

Create LeverageOptimizeRequest model:
- constraint_spec: ConstraintSpec (reuse from existing)
- ownership_estimates: List[float]  # Ownership % for each driver
- n_lineups: int = 20
- leverage_penalty: float = 0.5
- max_ownership_per_driver: float = 0.3
- min_low_ownership_drivers: int = 2
- max_total_ownership: float = 3.0
- use_regime_allocation: bool = False
- n_scenarios: int = 1000

Create LeverageOptimizeResponse model:
- lineups: List[LineupWithLeverage]
- portfolio_metrics: PortfolioMetrics
- ownership_metrics: OwnershipMetrics

Create LineupWithLeverage model:
- drivers: List[DriverInLineup] (reuse from existing)
- total_projected_points: float
- total_salary: int
- avg_ownership: float
- max_ownership: float
- total_ownership: float
- leverage_score: float
- regime: Optional[str] = None  # 'dominator', 'chaos', 'fuel_mileage'

Create OwnershipMetrics model:
- avg_lineup_ownership: float
- min_lineup_ownership: float
- max_lineup_ownership: float
- low_ownership_driver_count: int  # Drivers with <10% ownership
- diversification_score: float  # 1 - sum(ownership^2) / sum(ownership)^2

Add validation:
- contest_size_tier must be one of: small, medium, large
- leverage_penalty in [0, 1]
- max_ownership_per_driver in [0, 1]
- ownership_estimates must sum to ~100 (allow small tolerance)

Reference existing API contract patterns in apps/backend/app/api/contracts.py.
  </action>
  <verify>
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
python -c "
from apps.backend.app.api.contracts import (
    ContestSimRequest,
    ContestSimResponse,
    LeverageOptimizeRequest,
    LeverageOptimizeResponse,
    LineupWithLeverage,
    OwnershipMetrics
)

# Create sample contest sim request
contest_request = ContestSimRequest(
    my_lineup_scores=[150.0, 155.0, 160.0],
    scenario_driver_scores=[[50]*6]*100,  # 100 scenarios, 6 drivers
    field_size=1000,
    n_contest_sims=100,
    contest_buyin=20.0
)

# Create sample contest sim response
contest_response = ContestSimResponse(
    roi=0.25,
    roi_std=0.15,
    roi_lower_5=-0.05,
    roi_upper_95=0.55,
    cash_pct=0.22,
    cash_se=0.01,
    win_prob=0.01,
    win_se=0.003,
    best_rank=5,
    worst_rank=800,
    best_payout=50000,
    n_simulations=10000
)

print('Contest sim API models test passed')
print(f'Contest ROI: {contest_response.roi:.2%}')

# Create sample leverage optimize request
leverage_request = LeverageOptimizeRequest(
    constraint_spec=None,  # Would be real ConstraintSpec
    ownership_estimates=[20, 15, 12, 10, 8, 7],
    n_lineups=10,
    leverage_penalty=0.5
)

print('Leverage optimize API models test passed')
print(f'Leverage penalty: {leverage_request.leverage_penalty}')
"
```
  </verify>
  <done>
API Pydantic models created with:
- ContestSimRequest/Response for contest simulation endpoint
- LeverageOptimizeRequest/Response for leverage-aware optimization
- LineupWithLeverage extends existing lineup model with ownership metrics
- OwnershipMetrics for portfolio-level ownership statistics
- All models validate input ranges
  </done>
</task>

<task type="auto">
  <name>Task 2: Add FastAPI endpoints</name>
  <files>apps/backend/app/main.py</files>
  <action>
Add three new endpoints to FastAPI app:

**Add to apps/backend/app/main.py:**

Import Phase 4 components:
- from apps.backend.app.ownership.ensemble import HybridOwnershipEstimator
- from apps.backend.app.contest.contest_sim import ContestSimulator
- from apps.backend.app.contest.field_sim import FieldLineupSampler
- from apps/backend.app.contest.payout_curve import PayoutCurveFitter
- from apps.backend.app.optimizer.leverage_aware import LeverageAwareOptimizer
- from apps.backend.app.api.contracts import (
    ContestSimRequest, ContestSimResponse,
    LeverageOptimizeRequest, LeverageOptimizeResponse
  )

Add endpoint POST /ownership:
- Accept: OwnershipRequest
- Process:
  - Create feature matrix from request.driver_data and request.race_data
  - Initialize HybridOwnershipEstimator with specified parameters
  - If no training data provided, use pre-trained model (cache in global variable)
  - Generate ownership predictions
  - If include_uncertainty: call predict_with_uncertainty()
- Return: OwnershipResponse with predictions and optional uncertainty

Add endpoint POST /contest-sim:
- Accept: ContestSimRequest
- Process:
  - Create FieldLineupSampler with ownership estimates
  - Load or fit PayoutCurveFitter for contest_size_tier
  - Create ContestSimulator with field sampler and payout curve
  - Convert scenario_driver_scores to numpy array
  - Call simulate_portfolio() with my_lineup_scores and scenarios
  - Compute metrics (ROI, cash%, win probability)
- Return: ContestSimResponse with all metrics

Add endpoint POST /optimize-with-leverage:
- Accept: LeverageOptimizeRequest
- Process:
  - Convert ownership_estimates to numpy array
  - Create base NASCAROptimizer with constraint_spec
  - Create LeverageAwareOptimizer with ownership and leverage parameters
  - If use_regime_allocation: call generate_regime_aware_portfolio()
  - Else: call optimize_lineup_with_leverage()
  - Calculate portfolio-level ownership metrics
- Return: LeverageOptimizeResponse with lineups and metrics

Add error handling:
- Wrap all endpoints in try-except
- Return 400 for validation errors
- Return 500 for server errors with error message
- Log errors for debugging

Add caching:
- Cache trained ownership models globally (key: track_archetype)
- Cache fitted payout curves globally (key: contest_size_tier)
- Avoid re-training on every request

Reference existing endpoint patterns in apps/backend/app/main.py (/optimize, /health).
  </action>
  <verify>
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
python -c "
from fastapi.testclient import TestClient
from apps.backend.app.main import app

client = TestClient(app)

# Test health check (should work)
response = client.get('/health')
print(f'Health check: {response.status_code}')

# Test ownership endpoint structure (will fail without data)
try:
    response = client.post('/ownership', json={
        'driver_data': [{'driver_id': 1, 'salary': 10000, 'projected_points': 50, 'skill': 0.9}],
        'race_data': {'race_id': 1, 'track_archetype': 'superspeedway', 'race_date': '2024-02-01T00:00:00'},
        'ensemble_method': 'voting'
    })
    print(f'Ownership endpoint: {response.status_code}')
except Exception as e:
    print(f'Ownership endpoint test (expected to fail): {e}')

print('API endpoints structure test passed')
"
```
  </verify>
  <done>
FastAPI endpoints added with:
- POST /ownership for ownership estimation
- POST /contest-sim for contest simulation
- POST /optimize-with-leverage for leverage-aware optimization
- Error handling and validation
- Model caching for performance
  </done>
</task>

<task type="auto">
  <name>Task 3: Create integration tests</name>
  <files>apps/backend/app/contest/tests/test_contest_api_integration.py</files>
  <action>
Create comprehensive integration tests for Phase 4 API:

**File: apps/backend/app/contest/tests/test_contest_api_integration.py**

Create test class TestOwnershipEndpoint:
- test_ownership_voting_ensemble():
  - Create sample driver_data and race_data
  - POST to /ownership with ensemble_method='voting'
  - Assert 200 status
  - Assert predictions sum to ~100
  - Assert all ownership in [0, 100]

- test_ownership_with_uncertainty():
  - POST to /ownership with include_uncertainty=True
  - Assert 200 status
  - Assert uncertainty_lower <= ownership <= uncertainty_upper
  - Assert std > 0 for uncertainty bounds

- test_ownership_invalid_track_archetype():
  - POST to /ownership with invalid track_archetype
  - Assert 422 validation error

Create test class TestContestSimEndpoint:
- test_contest_sim_basic():
  - Create sample my_lineup_scores and scenario_driver_scores
  - POST to /contest-sim
  - Assert 200 status
  - Assert ROI in reasonable range [-1, 10]
  - Assert cash_pct in [0, 1]
  - Assert win_prob in [0, 1]

- test_contest_sim_large_field():
  - POST to /contest-sim with field_size=10000
  - Assert 200 status
  - Assert n_simulations = n_scenarios * n_contest_sims
  - Assert best_rank >= 1

- test_contest_sim_invalid_tier():
  - POST to /contest-sim with invalid contest_size_tier
  - Assert 422 validation error

Create test class TestLeverageOptimizeEndpoint:
- test_leverage_optimize_basic():
  - POST to /optimize-with-leverage
  - Assert 200 status
  - Assert len(lineups) <= n_lineups
  - Assert all lineups have leverage_score
  - Assert avg_ownership in [0, 100]

- test_leverage_optimize_with_regimes():
  - POST to /optimize-with-leverage with use_regime_allocation=True
  - Assert 200 status
  - Assert lineups have regime field
  - Assert all regimes in ['dominator', 'chaos', 'fuel_mileage']

- test_leverage_optimize_ownership_constraints():
  - POST to /optimize-with-leverage with max_ownership_per_driver=0.2
  - Assert 200 status
  - Assert all lineups have max_ownership <= 20

Create test class TestEndToEndPipeline:
- test_full_pipeline():
  - Start with ownership estimation
  - Use ownership for leverage optimization
  - Simulate contest with optimized lineups
  - Assert all steps complete successfully
  - Assert results consistent across steps

Use pytest fixtures for common test data:
- sample_driver_data: List of driver dicts
- sample_ownership: Array of ownership percentages
- sample_scenarios: Array of scenario driver scores

Reference existing API integration tests in apps/backend/app/tests/.
  </action>
  <verify>
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
# Run integration tests
python -m pytest apps/backend/app/contest/tests/test_contest_api_integration.py -v --tb=short

# Run end-to-end pipeline test
python -c "
from fastapi.testclient import TestClient
from apps.backend.app.main import app
import numpy as np

client = TestClient(app)

# Step 1: Get ownership estimates
ownership_response = client.post('/ownership', json={
    'driver_data': [
        {'driver_id': i, 'salary': 8000 + i*200, 'projected_points': 45 - i*2, 'skill': 0.9 - i*0.05}
        for i in range(1, 13)
    ],
    'race_data': {'race_id': 1, 'track_archetype': 'superspeedway', 'race_date': '2024-02-01T00:00:00'},
    'ensemble_method': 'voting'
})
assert ownership_response.status_code == 200
ownership = [p['ownership_percent'] for p in ownership_response.json()['ownership_predictions']]
print(f'Step 1: Ownership estimates retrieved (sum={sum(ownership):.1f}%)')

# Step 2: Run contest sim with ownership
contest_response = client.post('/contest-sim', json={
    'my_lineup_scores': [150.0],
    'scenario_driver_scores': [[50]*12]*10,
    'field_size': 100,
    'contest_size_tier': 'large'
})
assert contest_response.status_code == 200
roi = contest_response.json()['roi']
print(f'Step 2: Contest sim complete (ROI={roi:.2%})')

# Step 3: Run leverage optimization
leverage_response = client.post('/optimize-with-leverage', json={
    'constraint_spec': {'salary_cap': 50000, 'n_drivers': 6},
    'ownership_estimates': ownership,
    'n_lineups': 3,
    'leverage_penalty': 0.5
})
assert leverage_response.status_code == 200
lineups = leverage_response.json()['lineups']
print(f'Step 3: Leverage optimization complete ({len(lineups)} lineups generated)')

print('E2E pipeline test passed')
"
```
  </verify>
  <done>
Integration tests created with:
- TestOwnershipEndpoint: 3 tests for ownership API
- TestContestSimEndpoint: 3 tests for contest sim API
- TestLeverageOptimizeEndpoint: 3 tests for leverage optimization API
- TestEndToEndPipeline: 1 test for full pipeline
- Pytest fixtures for common data
- Comprehensive validation of API contracts
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify:

1. **API contracts**: All Pydantic models validate input/output correctly
2. **Ownership endpoint**: Returns ownership predictions with optional uncertainty
3. **Contest sim endpoint**: Returns ROI, cash%, win probability from simulation
4. **Leverage optimize endpoint**: Returns leverage-aware lineups with ownership metrics
5. **Integration tests**: All tests pass (or fail with clear error messages)
6. **Error handling**: Validation errors return 422, server errors return 500

Run verification:
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
python -c "
from fastapi.testclient import TestClient
from apps.backend.app.main import app

client = TestClient(app)

# Test endpoints exist
response = client.get('/health')
assert response.status_code == 200

# Check OpenAPI schema includes new endpoints
schema = app.openapi()
paths = schema['paths'].keys()
assert '/ownership' in paths
assert '/contest-sim' in paths
assert '/optimize-with-leverage' in paths

print('API integration verification passed')
print(f'Endpoints: {list(paths)}')
"
```
</verification>

<success_criteria>
1. All three API endpoints implemented and accessible
2. Pydantic models validate all input/output
3. Error handling returns appropriate status codes
4. Integration tests cover all endpoints
5. End-to-end pipeline test validates full workflow
6. All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-field-ownership-contest-sim/04-06-SUMMARY.md`
</output>
