---
phase: 04-field-ownership-contest-sim
plan: 04
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - apps/backend/app/contest/field_sim.py
  - apps/backend/app/contest/contest_sim.py
  - apps/backend/app/contest/metrics.py
autonomous: true

must_haves:
  truths:
    - "Field sampler generates lineups from ownership using Dirichlet-multinomial distribution"
    - "Field sampler respects DraftKings constraints (6 drivers, salary cap 50K)"
    - "Contest simulator runs Monte Carlo simulations with vectorized NumPy operations"
    - "Contest simulator calculates rank, payout, cash%, and top-1% probability"
    - "Contest metrics compute ROI, cash%, win probability from simulation results"
  artifacts:
    - path: "apps/backend/app/contest/field_sim.py"
      provides: "FieldLineupSampler class"
      exports: ["FieldLineupSampler"]
      min_lines: 200
    - path: "apps/backend/app/contest/contest_sim.py"
      provides: "ContestSimulator class"
      exports: ["ContestSimulator", "ContestResult"]
      min_lines: 300
    - path: "apps/backend/app/contest/metrics.py"
      provides: "Contest metrics calculations"
      exports: ["compute_roi", "compute_cash_pct", "compute_win_prob"]
      min_lines: 150
  key_links:
    - from: "apps/backend/app/contest/field_sim.py"
      to: "apps/backend/app/ownership/ensemble.py"
      via: "HybridOwnershipEstimator for ownership estimates"
      pattern: "from.*ownership.ensemble import HybridOwnershipEstimator"
    - from: "apps/backend/app/contest/contest_sim.py"
      to: "apps/backend/app/contest/payout_curve.py"
      via: "PayoutCurveFitter for payout interpolation"
      pattern: "from.*payout_curve import PayoutCurveFitter"
    - from: "apps/backend/app/contest/contest_sim.py"
      to: "packages/axiomatic-sim/axiomatic_sim/scenario_generator.py"
      via: "Load Phase 1 scenario driver scores for simulation"
      pattern: "load_scenario_outcomes|scenario_driver_scores"
    - from: "apps/backend/app/contest/metrics.py"
      to: "apps/backend/app/tail_metrics.py"
      via: "CVaR and tail metrics patterns"
      pattern: "from.*tail_metrics import.*"
---

<objective>
Build field lineup simulation and Monte Carlo contest simulation infrastructure. This generates realistic field lineups from ownership estimates and simulates contest outcomes to compute ROI, cash%, and win probability.

Purpose: Simulate contest outcomes by modeling field behavior through ownership-aware lineup generation and payout-aware contest scoring
Output: FieldLineupSampler for ownership-based field generation, ContestSimulator for Monte Carlo simulation, and contest metrics utilities
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-field-ownership-contest-sim/04-CONTEXT.md
@.planning/phases/04-field-ownership-contest-sim/04-RESEARCH.md

# Reference ownership estimation from Plans 01-02
@apps/backend/app/ownership/ensemble.py

# Reference payout curve from Plan 03
@apps/backend/app/contest/payout_curve.py

# Reference existing optimizer for DK constraints
@apps/backend/app/optimizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create field lineup sampler</name>
  <files>apps/backend/app/contest/field_sim.py</files>
  <action>
Create FieldLineupSampler for ownership-based field generation:

**File: apps/backend/app/contest/field_sim.py**

Create FieldLineupSampler class:
- __init__(ownership: np.ndarray, driver_pool: List[Dict], salary_cap: int = 50000, n_drivers: int = 6):
  - ownership: Array of ownership percentages (sums to 100)
  - driver_pool: List of driver dicts with driver_id, salary, projected_points
  - salary_cap: DraftKings salary cap (default 50K)
  - n_drivers: Number of drivers per lineup (default 6)
- sample_lineups(n_lineups: int) -> List[List[int]]:
  - Sample driver compositions using Dirichlet-multinomial distribution
  - Each lineup has n_drivers slots, each assigned to a driver based on ownership
  - Use numpy.random.multinomial for efficient sampling
  - Return list of driver_id lists
- sample_lineups_with_constraints(n_lineups: int) -> List[List[int]]:
  - Sample initial compositions using Dirichlet-multinomial
  - Filter to lineups that satisfy salary cap constraint
  - If insufficient valid lineups, resample (up to max_attempts)
  - Return list of valid driver_id lists
- compute_lineup_scores(driver_scores: np.ndarray, lineups: List[List[int]]) -> np.ndarray:
  - driver_scores: Array of driver scores for this scenario
  - lineups: List of driver_id lists
  - Sum scores for each lineup
  - Return array of lineup scores

Add utility functions:
- dirichlet_multinomial_sample(ownership, n_samples, n_draws):
  - Sample from Dirichlet-multinomial distribution
  - Returns array of shape (n_samples, n_drivers)
  - Use numpy for vectorized sampling

DO NOT:
- Don't add "chalk lineup" templates yet (simple ownership sampling is sufficient)
- Don't add correlation structure beyond ownership (keep it simple)

Reference RESEARCH.md section "Field Lineup Sampling" for implementation details.
  </action>
  <verify>
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
python -c "
from apps.backend.app.contest.field_sim import FieldLineupSampler
import numpy as np

# Create sample data
ownership = np.array([20, 15, 12, 10, 8, 7, 6, 5, 5, 4, 4, 4])  # 12 drivers
driver_pool = [
    {'driver_id': i+1, 'salary': 8000 + i*200, 'projected_points': 45 - i*2}
    for i in range(12)
]

# Test sampling
sampler = FieldLineupSampler(ownership, driver_pool, salary_cap=50000, n_drivers=6)
lineups = sampler.sample_lineups_with_constraints(10)
print(f'Sampled {len(lineups)} valid lineups')
print(f'Sample lineup: {lineups[0]}')

# Test score calculation
driver_scores = np.random.gamma(20, 2, size=len(ownership))
lineup_scores = sampler.compute_lineup_scores(driver_scores, lineups)
print(f'Lineup scores: {lineup_scores[:3]}')

print('Field sampler test passed')
"
```
  </verify>
  <done>
FieldLineupSampler created with:
- sample_lineups() generates lineups from ownership using Dirichlet-multinomial
- sample_lineups_with_constraints() filters to salary-cap compliant lineups
- compute_lineup_scores() sums driver scores for each lineup
- Vectorized NumPy operations for efficiency
  </done>
</task>

<task type="auto">
  <name>Task 2: Create contest simulator</name>
  <files>apps/backend/app/contest/contest_sim.py</files>
  <action>
Create ContestSimulator for Monte Carlo contest simulation:

**File: apps/backend/app/contest/contest_sim.py**

Create ContestResult dataclass:
- my_rank: int
- my_payout: float
- my_score: float
- winning_score: float
- field_size: int
- cashed: bool
- top_1_pct: bool

Create ContestSimulator class:
- __init__(
    field_sampler: FieldLineupSampler,
    payout_curve: Optional[PayoutCurveFitter] = None,
    field_size: int = 1000,
    n_scenarios: int = 1000,
    n_contest_sims: int = 100,
    default_payout_structure: str = 'standard_gpp'
  ):
  - field_sampler: FieldLineupSampler instance
  - payout_curve: Optional fitted PayoutCurveFitter (if None, use default structure)
  - field_size: Number of lineups in contest
  - n_scenarios: Number of race scenarios to simulate
  - n_contest_sims: Number of contest simulations per scenario
  - default_payout_structure: Which default payout curve to use if no curve provided
    - 'standard_gpp': Top 25% cash, power-law payout decay
    - 'cash': Top 50% cash, linear payout decay
    - 'double_up': Top 45% double payout (2x buyin)

- simulate_contest(my_lineup_score: float, scenario_driver_scores: np.ndarray) -> ContestResult:
  - Sample field lineup scores using field_sampler
  - Combine my score with field scores
  - Sort all scores to determine rank
  - If payout_curve provided: Apply payout_curve.predict() to get payout for rank
  - Else: Apply default_payout_structure based on rank
  - Determine if cashed (typically top 25%)
  - Determine if top 1% (top 10 positions in 1000-entry contest)
  - Return ContestResult

- simulate_portfolio(my_lineup_scores: np.ndarray, scenario_driver_scores: np.ndarray) -> Dict[str, np.ndarray]:
  - my_lineup_scores: Scores for my lineups (n_lineups,)
  - scenario_driver_scores: Driver scores for each scenario (n_scenarios, n_drivers)
  - Run n_contest_sims per scenario
  - Pre-allocate results arrays (n_lineups, n_scenarios * n_contest_sims)
  - Compute ranks, payouts, cashed, top_1_pct for each simulation
  - Return dict with arrays of results

- compute_contest_metrics(results: Dict[str, np.ndarray], buyin: float = 20.0) -> Dict[str, float]:
  - Compute ROI: (mean(payouts) - buyin) / buyin
  - Compute cash_pct: mean(cashed)
  - Compute win_prob: mean(top_1_pct)
  - Compute std_roi for risk assessment
  - Return dict with metrics

DO NOT:
- Don't add regime-aware portfolio allocation yet (that's in optimization plan)
- Don't add adaptive sampling yet (use fixed n_contest_sims)

Reference RESEARCH.md section "Contest Simulation Engine" for complete implementation.
  </action>
  <verify>
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
python -c "
from apps.backend.app.contest.contest_sim import ContestSimulator, ContestResult
from apps.backend.app.contest.field_sim import FieldLineupSampler
from apps/backend.app.contest.payout_curve import PayoutCurveFitter
import numpy as np

# Create test data
ownership = np.array([20, 15, 12, 10, 8, 7, 6, 5, 5, 4, 4, 4])
driver_pool = [
    {'driver_id': i+1, 'salary': 8000 + i*200, 'projected_points': 45 - i*2}
    for i in range(12)
]
field_sampler = FieldLineupSampler(ownership, driver_pool)

# Fit payout curve
ranks = np.array([1, 2, 3, 5, 10, 20, 50, 100])
payouts = np.array([100000, 50000, 30000, 15000, 5000, 2000, 500, 200])
payout_curve = PayoutCurveFitter()
payout_curve.fit(ranks, payouts)

# Test contest simulation
simulator = ContestSimulator(field_sampler, payout_curve, field_size=100, n_scenarios=2, n_contest_sims=5)
my_score = 150.0
driver_scores = np.random.gamma(20, 2, size=(2, 12))  # 2 scenarios, 12 drivers
result = simulator.simulate_contest(my_score, driver_scores[0])

print('Contest simulator test passed')
print(f'My rank: {result.my_rank}')
print(f'My payout: ${result.my_payout:.2f}')
print(f'Cashed: {result.cashed}')
print(f'Top 1%: {result.top_1_pct}')
"
```
  </verify>
  <done>
ContestSimulator created with:
- simulate_contest() runs single contest simulation with field sampling
- simulate_portfolio() runs Monte Carlo simulations across scenarios
- compute_contest_metrics() calculates ROI, cash%, win probability
- Vectorized NumPy operations for efficiency
- ContestResult dataclass for structured output
  </done>
</task>

<task type="auto">
  <name>Task 3: Create contest metrics utilities</name>
  <files>apps/backend/app/contest/metrics.py</files>
  <action>
Create contest metrics calculation utilities:

**File: apps/backend/app/contest/metrics.py**

Create compute_roi(payouts: np.ndarray, buyin: float) -> Dict[str, float]:
- Calculate expected ROI: (mean(payouts) - buyin) / buyin
- Calculate std ROI: std(payouts) / buyin
- Calculate 5th and 95th percentile ROI
- Return dict with roi, roi_std, roi_lower_5, roi_upper_95

Create compute_cash_pct(cashed: np.ndarray) -> Dict[str, float]:
- Calculate cash percentage: mean(cashed)
- Calculate standard error: sqrt(pct * (1-pct) / n)
- Return dict with cash_pct, cash_se, n_simulations

Create compute_win_prob(top_1_pct: np.ndarray) -> Dict[str, float]:
- Calculate win probability (top 1%): mean(top_1_pct)
- Calculate standard error
- Return dict with win_prob, win_se, n_simulations

Create compute_portfolio_metrics(
  ranks: np.ndarray,
  payouts: np.ndarray,
  cashed: np.ndarray,
  top_1_pct: np.ndarray,
  buyin: float
) -> Dict[str, Any]:
- Compute all metrics using above functions
- Add portfolio-level statistics:
  - best_rank: min(ranks)
  - worst_rank: max(ranks)
  - avg_rank: mean(ranks)
  - best_payout: max(payouts)
  - total_entries: ranks.shape[0] * ranks.shape[1]
- Return dict with all metrics

Create print_contest_report(metrics: Dict[str, Any]):
- Pretty-print contest metrics
- Show ROI, cash%, win probability with confidence intervals
- Show portfolio statistics (best/worst rank, best payout)
- Use formatted output with dollar signs and percentages

Reference RESEARCH.md section "Contest Metrics" for metric definitions.
  </action>
  <verify>
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
python -c "
from apps.backend.app.contest.metrics import (
    compute_roi,
    compute_cash_pct,
    compute_win_prob,
    compute_portfolio_metrics,
    print_contest_report
)
import numpy as np

# Create sample simulation results
n_lineups, n_sims = 5, 1000
ranks = np.random.randint(1, 1000, size=(n_lineups, n_sims))
payouts = np.where(ranks <= 250, np.random.uniform(0, 100000, size=(n_lineups, n_sims)), 0)
cashed = ranks <= 250
top_1_pct = ranks <= 10

# Test individual metrics
roi = compute_roi(payouts.flatten(), buyin=20.0)
print(f'ROI: {roi[\"roi\"]:.2%} +/- {roi[\"roi_std\"]:.2%}')

cash = compute_cash_pct(cashed.flatten())
print(f'Cash%: {cash[\"cash_pct\"]:.2%}')

win = compute_win_prob(top_1_pct.flatten())
print(f'Win%: {win[\"win_prob\"]:.2%}')

# Test portfolio metrics
metrics = compute_portfolio_metrics(ranks, payouts, cashed, top_1_pct, buyin=20.0)
print(f'Best rank: {metrics[\"best_rank\"]}')
print(f'Worst rank: {metrics[\"worst_rank\"]}')
print(f'Best payout: ${metrics[\"best_payout\"]:.2f}')

print('\\nContest metrics test passed')
"
```
  </verify>
  <done>
Contest metrics utilities created with:
- compute_roi() calculates expected ROI with confidence intervals
- compute_cash_pct() calculates cash percentage with standard error
- compute_win_prob() calculates win probability with confidence intervals
- compute_portfolio_metrics() aggregates all metrics
- print_contest_report() pretty-prints results
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify:

1. **Field sampling**: Dirichlet-multinomial sampling generates ownership-based lineups
2. **Constraint compliance**: Sampled lineups respect salary cap constraint
3. **Contest simulation**: Monte Carlo simulation computes ranks and payouts correctly
4. **Metrics calculation**: ROI, cash%, win probability computed with confidence intervals
5. **Integration**: Contest simulator uses field sampler and payout curve

Run verification:
```bash
cd /Users/zax/Desktop/nascar-model\ copy\ 2
python -c "
from apps.backend.app.contest.field_sim import FieldLineupSampler
from apps.backend.app.contest.contest_sim import ContestSimulator
from apps.backend.app.contest.metrics import compute_portfolio_metrics
import numpy as np

# Quick integration test
ownership = np.array([20, 15, 12, 10, 8, 7, 6, 5, 5, 4, 4, 4])
driver_pool = [{'driver_id': i+1, 'salary': 8000 + i*200, 'projected_points': 45 - i*2} for i in range(12)]
sampler = FieldLineupSampler(ownership, driver_pool)
lineups = sampler.sample_lineups_with_constraints(5)
print(f'Generated {len(lineups)} field lineups')
print('Contest simulation verification passed')
"
```
</verification>

<success_criteria>
1. FieldLineupSampler generates ownership-based lineups with constraints
2. ContestSimulator runs Monte Carlo simulations with field sampling
3. Contest metrics compute ROI, cash%, win probability correctly
4. Vectorized NumPy operations for efficiency
5. All components integrate correctly
6. Unit tests pass for all components
</success_criteria>

<output>
After completion, create `.planning/phases/04-field-ownership-contest-sim/04-04-SUMMARY.md`
</output>
