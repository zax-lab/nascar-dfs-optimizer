---
phase: 07-background-jobs-gpu-offload
plan: 02
type: execute
wave: 2
depends_on: [07-01]
files_modified: [
  apps/native_mac/jobs/gpu_client.py,
  apps/native_mac/jobs/job_manager.py,
  apps/native_mac/gui/views/settings_tab.py,
  apps/native_mac/persistence/models.py,
  apps/native_mac/persistence/database.py
]
autonomous: true

must_haves:
  truths:
    - User toggles GPU offload in Settings to route jobs to Windows worker
    - GPU-offloaded jobs complete in 5-10s vs 30-60s locally
    - User views job history with timestamped runs and can re-run previous configs
    - Failed GPU jobs automatically fall back to local CPU execution
    - GPU worker connection status shown in Settings tab
    - Job details dialog shows full config and results with export option
  artifacts:
    - path: apps/native_mac/jobs/gpu_client.py
      provides: GPUWorkerClient for HTTP/FastAPI communication
      min_lines: 120
    - path: apps/native_mac/gui/dialogs/job_details_dialog.py
      provides: Job details dialog with config/results viewer
      min_lines: 100
  key_links:
    - from: apps/native_mac/jobs/job_manager.py
      to: apps/native_mac/jobs/gpu_client.py
      via: conditional routing based on gpu_enabled flag
      pattern: if config.get('gpu_offload'): gpu_client.submit_job()
    - from: apps/native_mac/gui/views/settings_tab.py
      to: apps/native_mac/jobs/gpu_client.py
      via: connection test button
      pattern: gpu_client.test_connection()
---

<objective>
Add GPU offload capability to route optimization jobs to Windows GPU worker, job history viewer with re-run, and automatic fallback to local CPU.

Purpose: Reduce optimization time from 30-60s to 5-10s by leveraging CUDA GPU acceleration on a remote Windows machine while maintaining local CPU fallback.

Output: GPUWorkerClient for HTTP communication, enhanced JobManager with GPU routing, job history persistence, re-run functionality, and Settings integration.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-background-jobs-gpu-offload/07-01-PLAN.md
@.planning/phases/06-foundation-gui-local-optimization/06-10-SUMMARY.md

**Prerequisites from 07-01:**
- JobManager with SQLite persistence and thread pool
- JobsTab with job listing and basic status
- Job model with config_json and result_json fields

**Requirements to satisfy:**
- OPT-07: GPU offload toggle (local Mac CPU vs Windows GPU worker)
- OPT-08: Job history with audit trail (timestamped runs with inputs/outputs)
- DATA-05: API data fetching (use same HTTP client pattern for GPU worker)

**Technical decisions from research:**
- HTTP/FastAPI pattern for GPU offload (not ZeroMQ - start simple)
- Windows worker runs FastAPI server accepting optimization requests
- JAX[cpu] for local, JAX[cuda] on Windows worker
- Configuration: GPU worker URL, API key, timeout settings

**Key patterns to follow:**
- JobManager already handles job lifecycle - extend for GPU routing
- SettingsTab pattern from Phase 6 - add GPU configuration section
- Signal/slot for async results from GPU worker
- JSON serialization for job config/results (already established)

**GPU Worker contract (assumed):**
```python
POST /optimize
{
  "config": { ... },  # Same as local config
  "job_id": "uuid"
}
Response:
{
  "status": "completed",
  "result": { ... },  # Lineup results
  "metrics": { "duration": 7.5, "iterations": 1000 }
}
```

**Fallback strategy:**
1. Submit to GPU worker with timeout
2. On timeout or error, log failure, update job status to "retrying"
3. Automatically re-submit to local CPU queue
4. User sees fallback in job details
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPUWorkerClient for HTTP offload</name>
  <files>apps/native_mac/jobs/gpu_client.py, apps/native_mac/jobs/__init__.py</files>
  <action>
Create GPUWorkerClient for communicating with Windows GPU worker:

1. Create apps/native_mac/jobs/gpu_client.py:
   - Import urllib.request or httpx (prefer httpx for async, but use stdlib to avoid new deps)
   - Class GPUWorkerClient:
   - __init__(base_url: str, api_key: str = None, timeout: float = 30.0)
   - test_connection() -> bool - GET /health or /ping
   - submit_job(job_id: str, config: dict) -> bool - POST /optimize
   - get_job_status(job_id: str) -> dict - GET /jobs/{id}/status
   - cancel_job(job_id: str) -> bool - POST /jobs/{id}/cancel
   - _make_request(method, endpoint, data=None) - internal HTTP helper

2. Connection pooling:
   - Use urllib.request with keep-alive if possible
   - Or simple requests per call (optimization is 5-10s, connection overhead minimal)
   - Handle ConnectionError, TimeoutError gracefully

3. Response handling:
   - Parse JSON responses
   - Return dict with status, result, error fields
   - Convert HTTP errors to exceptions or error dicts

4. Async considerations:
   - GPU worker is long-polling or callback-based
   - For MVP: blocking HTTP request (5-10s is acceptable)
   - For future: poll /status endpoint every 1s
   - JobManager will run gpu_client.submit_job() in thread pool anyway

5. Update apps/native_mac/jobs/__init__.py:
   - Export GPUWorkerClient

6. Error handling:
   - Connection refused -> return False, set error="Worker unreachable"
   - Timeout -> return False, set error="Worker timeout"
   - 4xx/5xx -> return False, set error="Worker error: {status}"

7. Do NOT require external HTTP library - use urllib.request from stdlib
8. Do NOT implement ZeroMQ - HTTP is sufficient for Phase 7
  </action>
  <verify>
python -c "
from apps.native_mac.jobs.gpu_client import GPUWorkerClient
client = GPUWorkerClient('http://localhost:8000', timeout=5.0)
assert client.base_url == 'http://localhost:8000'
assert client.timeout == 5.0
print('GPUWorkerClient created successfully')
"
  </verify>
  <done>GPUWorkerClient class exists with test_connection, submit_job, get_status methods using urllib</done>
</task>

<task type="auto">
  <name>Task 2: Integrate GPU offload into JobManager and Settings</name>
  <files>apps/native_mac/jobs/job_manager.py, apps/native_mac/gui/views/settings_tab.py, apps/native_mac/persistence/session_manager.py</files>
  <action>
Extend JobManager for GPU routing and add Settings configuration:

1. Modify apps/native_mac/jobs/job_manager.py:
   - In __init__(): add gpu_client parameter, store reference
   - Add submit_job() enhancement:
     * Check config.get('gpu_offload', False)
     * If True and gpu_client available:
       - Test connection first
       - If connected: route to GPU via _execute_job_gpu()
       - If not connected: log warning, route to local CPU
     * Add job metadata: execution_mode='gpu' or 'local'
   - Add _execute_job_gpu(job_id, config):
     * Update job status to RUNNING (GPU)
     * Call gpu_client.submit_job()
     * Poll for completion (if async) or wait (if sync)
     * On success: update job with result, emit completed
     * On failure: log error, mark for fallback, emit failed
   - Add fallback_job_to_local(job_id) - re-queue on local CPU
   - Property is_gpu_available -> bool

2. Modify apps/native_mac/gui/views/settings_tab.py:
   - Add "GPU Offload" section to Settings tab
   - Fields:
     * Checkbox: "Enable GPU offload" (default unchecked)
     * LineEdit: "GPU Worker URL" (default http://192.168.1.xxx:8000)
     * LineEdit: "API Key" (password field, optional)
     * SpinBox: "Timeout (seconds)" (default 30, range 10-120)
     * Button: "Test Connection" - shows success/failure
     * Label: "Status: Connected / Not connected / Unknown"
   - Settings persistence:
     * Save to session_manager under keys: gpu/enabled, gpu/url, gpu/api_key, gpu/timeout
     * Load on Settings tab show
   - Test connection button handler:
     * Create GPUWorkerClient with current settings
     * Call test_connection()
     * Show QMessageBox with result

3. Modify apps/native_mac/main.py:
   - Read GPU settings on startup
   - If enabled: create GPUWorkerClient, pass to JobManager
   - Else: pass None for gpu_client

4. Update apps/native_mac/persistence/session_manager.py:
   - Default settings for GPU:
     * gpu/enabled = False
     * gpu/url = ""
     * gpu/api_key = ""
     * gpu/timeout = 30

5. Key behaviors:
   - GPU toggle is per-job (user can choose per optimization)
   - Settings has global default for GPU toggle state
   - Connection test validates URL before enabling
   - Timeout prevents hanging on unreachable worker

6. Do NOT require GPU worker to be available - local CPU always works
7. Do NOT store API key in database - session settings only
  </action>
  <verify>
python -c "
from apps.native_mac.jobs.job_manager import JobManager
from apps.native_mac.jobs.gpu_client import GPUWorkerClient
client = GPUWorkerClient('http://localhost:8000')
jm = JobManager(max_workers=2, gpu_client=client)
assert jm.gpu_client is not None
assert jm.is_gpu_available == False  # Not actually running
jm.shutdown()
print('JobManager with GPU client integration successful')
"
  </verify>
  <done>JobManager routes jobs to GPU when enabled, Settings tab has GPU configuration with test connection</done>
</task>

<task type="auto">
  <name>Task 3: Enhance Jobs tab with history viewer and re-run</name>
  <files>apps/native_mac/gui/views/jobs_tab.py, apps/native_mac/gui/dialogs/job_details_dialog.py, apps/native_mac/gui/dialogs/__init__.py, apps/native_mac/persistence/database.py</files>
  <action>
Create job history viewer with details dialog and re-run functionality:

1. Create apps/native_mac/gui/dialogs/__init__.py:
   - Export JobDetailsDialog

2. Create apps/native_mac/gui/dialogs/job_details_dialog.py:
   - Class JobDetailsDialog(QDialog):
   - __init__(job_data: dict): show job details in dialog
   - Layout:
     * Header: Job name, ID, status with colored badge
     * Timing: Created, Started, Completed, Duration
     * Execution mode: Local CPU or GPU offload
     * Config tab: QTextEdit with formatted JSON (read-only, scrollable)
     * Results tab: QTextEdit with formatted JSON or lineup table
     * If results exist: show lineup count, best lineup score
     * If error: show error message in red
   - Buttons:
     * "Re-run Job" - closes dialog, submits new job with same config
     * "Export Results" - save results to JSON file
     * "Close" - dismiss dialog
   - Style:
     * Fixed size (800x600) or resizable
     * Monospace font for JSON
     * Syntax highlighting for JSON (optional, nice-to-have)

3. Modify apps/native_mac/gui/views/jobs_tab.py:
   - Enhance context menu:
     * "View Details" - opens JobDetailsDialog
     * "Re-run Job" - submits new job with same config
     * "Cancel Job" - if queued or running
     * "Delete Job" - remove from history
     * "Export Job" - save to JSON file
   - Add toolbar buttons:
     * "View Details" (requires selection)
     * "Re-run" (requires selection)
     * Separator
     * "Filter: All / Running / Completed / Failed"
     * "Sort: Date / Name / Duration"
   - Add search/filter:
     * QLineEdit for searching job names
     * Filter applies to job name and race name
   - Add stats label at bottom:
     * "Showing X jobs | Total: Y completed, Z failed"
   - Auto-refresh enhancement:
     * Only refresh rows that changed (optimize performance)
     * Keep selection across refresh if possible

4. Modify apps/native_mac/persistence/database.py:
   - Add query methods for history:
     * get_jobs_by_status(status, limit=100) - filter by status
     * get_jobs_by_date_range(start, end) - for date filtering
     * search_jobs(query) - text search on name/config
     * get_job_stats() -> dict with counts by status
   - Add delete_job(job_id) - soft or hard delete

5. Re-run implementation:
   - In JobsTab._rerun_job():
     * Get selected job_id
     * Load job from database
     * Parse config_json
     * Call job_manager.submit_job(config)
     * Show confirmation: "Job re-submitted as #{new_job_id}"
     * New job gets "(re-run)" suffix in name

6. Do NOT show all jobs by default - limit to last 50, load more on scroll
7. Do NOT allow editing historical jobs - history is immutable
  </action>
  <verify>
python -c "
from apps.native_mac.gui.dialogs.job_details_dialog import JobDetailsDialog
from PySide6.QtWidgets import QApplication
app = QApplication([])
mock_job = {
    'id': 'test-123',
    'name': 'Test Job',
    'status': 'completed',
    'config_json': '{}',
    'result_json': '{}',
    'created_at': '2026-01-29',
    'started_at': '2026-01-29',
    'completed_at': '2026-01-29'
}
dialog = JobDetailsDialog(mock_job)
assert dialog is not None
print('JobDetailsDialog created successfully')
"
  </verify>
  <done>Jobs tab has enhanced history viewer with filtering, JobDetailsDialog shows full details with re-run and export</done>
</task>

</tasks>

<verification>
1. Enable GPU in Settings, test connection - shows success/failure
2. Submit job with GPU toggle ON - routes to GPU worker
3. GPU job completes in ~5-10s vs local ~30-60s
4. Disconnect GPU worker, submit GPU job - falls back to local after timeout
5. View job history - shows all past jobs with status
6. Double-click completed job - opens details dialog
7. Click "Re-run" on historical job - creates new job with same config
8. Export job results - saves JSON file
</verification>

<success_criteria>
- GPUWorkerClient communicates with Windows GPU worker via HTTP
- Settings tab has GPU configuration section with connection test
- JobManager routes jobs to GPU when enabled, falls back to local on failure
- Jobs tab shows job history with filtering and sorting
- JobDetailsDialog displays full job config and results
- Re-run functionality creates new job from historical config
- Job execution mode (GPU/local) visible in job details
- GPU jobs complete significantly faster than local CPU jobs
</success_criteria>

<output>
After completion, create `.planning/phases/07-background-jobs-gpu-offload/07-02-SUMMARY.md`
</output>
