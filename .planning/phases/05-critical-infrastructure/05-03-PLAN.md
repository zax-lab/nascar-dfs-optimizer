---
phase: 05-critical-infrastructure
plan: 03
type: execute
wave: 3
depends_on: [05-01, 05-02]
files_modified:
  - apps/backend/pyproject.toml
  - apps/backend/app/logging_config.py
  - apps/backend/app/middleware.py
  - apps/backend/app/main.py
autonomous: true

must_haves:
  truths:
    - "All application logs are structured JSON with timestamp, level, message, context"
    - "Each HTTP request has a correlation ID (ULID format) that propagates through logs"
    - "Correlation ID is returned in X-Correlation-ID response header"
    - "Errors are logged with full stack traces and request context"
    - "Graceful shutdown waits up to 90 seconds for in-flight jobs to complete"
    - "Jobs still running after timeout are logged with warning"
  artifacts:
    - path: "apps/backend/app/logging_config.py"
      provides: "Structured JSON logging configuration"
      exports: ["configure_logging", "get_logger"]
      min_lines: 50
    - path: "apps/backend/app/middleware.py"
      provides: "Correlation ID middleware"
      exports: ["add_correlation_id"]
      min_lines: 40
    - path: "apps/backend/app/main.py"
      provides: "Graceful shutdown with job drain"
      contains: "graceful_shutdown, shutdown_event, lifespan"
  key_links:
    - from: "apps/backend/app/middleware.py"
      to: "structlog.contextvars"
      via: "bind_contextvars(correlation_id=correlation_id)"
      pattern: "structlog\\.contextvars\\.bind_contextvars"
    - from: "apps/backend/app/main.py"
      to: "app.state.job_manager.get_running_job_count()"
      via: "graceful_shutdown function polling running jobs"
      pattern: "await.*job_manager\\.get_running_job_count"
    - from: "apps/backend/app/main.py"
      to: "signal_handler"
      via: "lifespan shutdown section"
      pattern: "shutdown_event\\.set\\(\\)"
---

<objective>
Implement structured JSON logging with correlation IDs and graceful shutdown that waits for in-flight jobs to complete.

Purpose: Enable distributed tracing with correlation IDs and ensure clean container restarts without losing in-flight optimization work. Structured logging integrates with observability platforms (Phase 5.3).

Output: Structured logging config, correlation ID middleware, graceful shutdown with 90-second timeout
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/05-critical-infrastructure/05-CONTEXT.md
@.planning/phases/05-critical-infrastructure/05-RESEARCH.md
@apps/backend/app/main.py
@apps/backend/app/logging_config.py
@apps/backend/app/middleware.py
</context>

<tasks>

<task type="auto">
  <name>Add structured logging dependencies</name>
  <files>apps/backend/pyproject.toml</files>
  <action>
  Add to dependencies in pyproject.toml:
  1. "structlog>=24.1.0"
  2. "ulid-py>=1.1.0"

  Use structlog 24.x+ (from RESEARCH.md - supports contextvars for async-safe correlation tracking).
  Use ulid-py (from RESEARCH.md - most popular ULID library, time-ordered).
  </action>
  <verify>
  pip list | grep -E "structlog|ulid"
  </verify>
  <done>
  structlog and ulid-py installed, versions match requirements
  </done>
</task>

<task type="auto">
  <name>Create structured logging configuration</name>
  <files>apps/backend/app/logging_config.py</files>
  <action>
  Create apps/backend/app/logging_config.py with:

  1. Imports:
     import structlog
     import logging
     from logging.handlers import RotatingFileHandler
     from pathlib import Path

  2. configure_logging function:
     def configure_logging(log_level: str = "INFO"):
         # Create logs directory
         log_dir = Path("logs")
         log_dir.mkdir(exist_ok=True)

         # File handler with rotation
         file_handler = RotatingFileHandler(
             log_dir / "app.log",
             maxBytes=10*1024*1024,  # 10MB
             backupCount=5,
         )
         file_handler.setLevel(log_level)

         # Configure structlog
         structlog.configure(
             processors=[
                 structlog.contextvars.merge_contextvars,
                 structlog.stdlib.add_log_level,
                 structlog.processors.TimeStamper(fmt="iso"),
                 structlog.processors.StackInfoRenderer(),
                 structlog.processors.format_exc_info,
                 structlog.processors.JSONRenderer()
             ],
             wrapper_class=structlog.stdlib.BoundLogger,
             context_class=dict,
             logger_factory=structlog.stdlib.LoggerFactory(),
             cache_logger_on_first_use=True,
         )

         # Configure stdlib logging
         logging.basicConfig(
             level=log_level,
             handlers=[logging.StreamHandler(), file_handler],
             format='%(message)s'  # Let structlog handle formatting
         )

  3. get_logger function:
     def get_logger(name: str = None):
         return structlog.get_logger(name)

  Use RotatingFileHandler with 10MB max size and 5 backup files (from RESEARCH.md Q2).
  Use JSONRenderer for structured output (per PROD-13).
  </action>
  <verify>
  python -c "from app.logging_config import configure_logging, get_logger; configure_logging(); logger = get_logger('test'); logger.info('test log', extra={'key': 'value'})"
  </verify>
  <done>
  configure_logging and get_logger functions exist, logs output JSON format
  </done>
</task>

<task type="auto">
  <name>Create correlation ID middleware</name>
  <files>apps/backend/app/middleware.py</files>
  <action>
  Create apps/backend/app/middleware.py with:

  1. Imports:
     from fastapi import Request
     from contextvars import ContextVar
     import ulid
     import structlog

  2. Context variable declaration:
     correlation_id_var: ContextVar[str] = ContextVar('correlation_id', default=None)

  3. Middleware function:
     async def add_correlation_id(request: Request, call_next):
         # Generate or use existing correlation ID
         correlation_id = request.headers.get("X-Correlation-ID") or str(ulid.ULID())
         correlation_id_var.set(correlation_id)

         # Bind to structlog context for all log entries in this request
         structlog.contextvars.bind_contextvars(
             correlation_id=correlation_id,
             request_id=request.state.request_id if hasattr(request.state, 'request_id') else None,
         )

         # Process request
         response = await call_next(request)

         # Add correlation ID to response headers
         response.headers["X-Correlation-ID"] = correlation_id
         return response

  Use ulid-py for time-ordered, sortable ULIDs (from RESEARCH.md Q1).
  Use structlog.contextvars.bind_contextvars for async-safe context propagation (from RESEARCH.md).
  </action>
  <verify>
  python -c "from app.middleware import add_correlation_id, correlation_id_var; print('Middleware loaded successfully')"
  </verify>
  <done>
  add_correlation_id middleware exists, uses ULID generation, binds structlog context
  </done>
</task>

<task type="auto">
  <name>Integrate logging and middleware into FastAPI</name>
  <files>apps/backend/app/main.py</files>
  <action>
  1. Remove old logging configuration (lines 71-75):
     - Delete logging.basicConfig call
     - Delete logger = logging.getLogger(__name__) (will use structlog)

  2. Import and configure structured logging:
     from app.logging_config import configure_logging, get_logger
     logger = get_logger(__name__)

     # Configure logging at module level (before FastAPI app creation)
     configure_logging(log_level="INFO")

  3. Add middleware to FastAPI app:
     from app.middleware import add_correlation_id

     app.add_middleware(add_correlation_id)

     Place this AFTER CORSMiddleware, BEFORE exception handler.

  4. Update global exception handler to use structured logging:
     - Replace logging.error with logger.error (structured)
     - Add stack_trace field to error log:
       logger.error(
           "Unhandled exception",
           exc_info=exc,
           exception_type=type(exc).__name__,
           exception_message=str(exc),
           path=request.url.path,
           method=request.method,
       )

  5. Update all logger.info calls throughout main.py to use structured format:
     - Replace f-strings with keyword arguments: logger.info("Optimization request received", run_id=run_id, slate_id=request.slate_id)
     - Add context fields to all log calls

  Preserve all existing log messages, just change format to structured.
  </action>
  <verify>
  curl http://localhost:8000/health && docker-compose logs backend | tail -1 | jq .
  </verify>
  <done>
  Logs output JSON format with timestamp, level, message, correlation_id fields
  </done>
</task>

<task type="auto">
  <name>Implement graceful shutdown with job drain</name>
  <files>apps/backend/app/main.py</files>
  <action>
  1. Add shutdown_event to lifespan startup:
     import asyncio
     shutdown_event = asyncio.Event()
     app.state.shutdown_event = shutdown_event

  2. Create graceful_shutdown function:
     async def graceful_shutdown(job_manager, shutdown_event, timeout: int = 90):
         logger.info("Shutdown initiated", timeout_seconds=timeout)
         shutdown_event.set()  # Signal job submission to stop

         start_time = time.time()
         while time.time() - start_time < timeout:
             running = await job_manager.get_running_job_count()
             if running == 0:
                 logger.info("All jobs complete, shutting down")
                 return
             logger.info("Waiting for jobs to complete", running=running)
             await asyncio.sleep(1)

         remaining = await job_manager.get_running_job_count()
         logger.warning("Shutdown timeout with jobs still running", remaining=remaining)

  3. Call graceful_shutdown in lifespan shutdown section:
     await graceful_shutdown(
         job_manager=app.state.job_manager,
         shutdown_event=shutdown_event,
         timeout=90
     )

  Use 90-second timeout (from CONTEXT.md - Claude's discretion for balance).
  Log warning when timeout exceeded (per CONTEXT.md - stragglers get best effort).
  </action>
  <verify>
  docker-compose restart backend && docker-compose logs backend | grep "Shutdown"
  </verify>
  <done>
  Backend logs show shutdown sequence, running jobs counted before exit
  </done>
</task>

</tasks>

<verification>
1. All logs are JSON format with timestamp, level, message, context fields
2. Each request generates unique correlation ID (ULID format)
3. Correlation ID appears in all log entries for that request
4. Correlation ID returned in X-Correlation-ID response header
5. Error logs include stack_trace field
6. Graceful shutdown waits for running jobs or times out after 90 seconds
7. Shutdown timeout logs warning with remaining job count
</verification>

<success_criteria>
1. docker-compose logs backend | grep -v "^$" | tail -10 shows JSON log entries
2. curl -v http://localhost:8000/health shows X-Correlation-ID header in response
3. Correlation ID format is ULID (26 characters, e.g., 01ARZ3NDEKTSV4RRFFQ69G5FAV)
4. Triggering an error shows stack_trace in logs (not exposed to client)
5. Container restart with running jobs shows "Waiting for N jobs to complete" log
6. After 90 seconds, "Shutdown timeout" warning appears if jobs still running
</success_criteria>

<output>
After completion, create `.planning/phases/05-critical-infrastructure/05-03-SUMMARY.md`
</output>
