---
phase: 05-critical-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01"]
files_modified:
  - apps/backend/app/api/optimize.py
  - apps/backend/app/api/health.py
  - apps/backend/app/main.py
autonomous: false

must_haves:
  truths:
    - "Optimization jobs persist in Redis and survive container restart"
    - "GET /jobs/{id} returns job status with 200, 404 if not found"
    - "GET /jobs/{id}/result returns portfolio data for complete jobs, 202 for pending"
    - "/health returns 200 with timestamp, version, uptime"
    - "/ready returns 200 only when both Neo4j and Redis are available"
    - "API returns 503 when dependencies are down with diagnostic detail"
  artifacts:
    - path: "apps/backend/app/api/optimize.py"
      provides: "Redis-backed job endpoints"
      exports: ["router", "submit_optimization", "get_optimization_status", "get_optimization_result"]
      removes: ["optimization_jobs", "optimization_results global dicts"]
    - path: "apps/backend/app/api/health.py"
      provides: "Health check endpoints"
      exports: ["liveness", "readiness"]
      min_lines: 80
    - path: "apps/backend/app/main.py"
      provides: "Integrated health and job routers"
      contains: "@router.get(\"/health\"), @router.get(\"/ready\"), @app.include_router(health_router)"
  key_links:
    - from: "apps/backend/app/api/optimize.py"
      to: "app.state.job_manager"
      via: "Depends(get_job_manager) dependency injection"
      pattern: "Depends\\(get_job_manager\\)"
    - from: "apps/backend/app/api/optimize.py"
      to: "Redis job storage"
      via: "JobStateManager.create_job, get_job, update_job_status calls"
      pattern: "job_manager\\.(create_job|get_job|update_job_status)"
    - from: "apps/backend/app/api/health.py"
      to: "Redis and Neo4j dependency checks"
      via: "readiness endpoint health checks"
      pattern: "redis.*ping|neo4j.*verify_connectivity"
---

<objective>
Migrate optimization endpoints to use Redis-backed job state and implement health check endpoints for liveness and readiness probes.

Purpose: Replace in-memory job storage with durable Redis persistence, add Kubernetes-style health endpoints for orchestrator monitoring, and ensure graceful degradation when dependencies are unavailable.

Output: Migrated job endpoints, health check router, 503 error handling for dependency failures
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/05-critical-infrastructure/05-CONTEXT.md
@.planning/phases/05-critical-infrastructure/05-RESEARCH.md
@apps/backend/app/api/optimize.py
@apps/backend/app/main.py
@apps/backend/app/job_manager.py
</context>

<tasks>

<task type="auto">
  <name>Migrate /optimize endpoint to use JobStateManager</name>
  <files>apps/backend/app/api/optimize.py</files>
  <action>
  1. Remove global state (lines 45-46):
     - Delete optimization_jobs: Dict[str, OptimizationStatus] = {}
     - Delete optimization_results: Dict[str, OptimizeResponse] = {}

  2. Import JobStateManager and create dependency:
     - from app.job_manager import JobStateManager
     - from fastapi import Depends
     - def get_job_manager(request: Request) -> JobStateManager:
         return request.app.state.job_manager

  3. Update submit_optimization endpoint:
     - Change signature: async def submit_optimization(request: OptimizeRequest, background_tasks: BackgroundTasks, job_manager: JobStateManager = Depends(get_job_manager))
     - Replace optimization_jobs[run_id] = job_status with:
       * await job_manager.create_job(run_id, request.dict(), correlation_id=None)
     - Store correlation_id from request state if available

  4. Update get_optimization_status endpoint:
     - Add job_manager: JobStateManager = Depends(get_job_manager) parameter
     - Replace optimization_jobs[run_id] lookup with:
       * job = await job_manager.get_job(run_id)
       * Raise 404 if job is None
       * Parse JSON fields (input_params, result, error_message) from job dict

  5. Update get_optimization_result endpoint:
     - Add job_manager: JobStateManager = Depends(get_job_manager) parameter
     - Replace optimization_jobs and optimization_results lookups with job_manager.get_job()
     - Parse result JSON from job["result"] field

  6. Update run_optimization_background function:
     - Add job_manager: JobStateManager parameter
     - Replace optimization_jobs[run_id].status = "running" with:
       * await job_manager.update_job_status(run_id, "running")
     - Replace optimization_results[run_id] = response with:
       * await job_manager.update_job_status(run_id, "complete", result=response.dict())

  Preserve all existing endpoint contracts (status codes, response models).
  </action>
  <verify>
  pytest apps/backend/tests/test_api.py -v -k "test_optimization"
  </verify>
  <done>
  All global job state removed, endpoints use JobStateManager, tests pass
  </done>
</task>

<task type="auto">
  <name>Create health check endpoints</name>
  <files>apps/backend/app/api/health.py</files>
  <action>
  Create apps/backend/app/api/health.py with:

  1. Router setup:
     from fastapi import APIRouter, HTTPException, Depends, Request
     router = APIRouter()

  2. Dependency to get app state:
     async def get_app_state(request: Request):
         return request.app.state

  3. Liveness endpoint:
     @router.get("/health")
     async def liveness():
         import time, os
         START_TIME = os.getenv("APP_START_TIME", int(time.time()))
         return {
             "status": "ok",
             "timestamp": datetime.utcnow().isoformat(),
             "version": os.getenv("APP_VERSION", "0.3.0"),
             "uptime_seconds": int(time.time() - START_TIME)
         }

  4. Readiness endpoint:
     @router.get("/ready")
     async def readiness(app_state = Depends(get_app_state)):
         dependencies = {}
         all_ok = True

         # Check Redis
         try:
             await app_state.redis_client.ping(timeout=3)
             dependencies["redis"] = "ok"
         except Exception as e:
             dependencies["redis"] = f"down: {str(e)}"
             all_ok = False

         # Check Neo4j
         try:
             await app_state.neo4j_driver.verify_connectivity()
             dependencies["neo4j"] = "ok"
         except Exception as e:
             dependencies["neo4j"] = f"down: {str(e)}"
             all_ok = False

         if not all_ok:
             raise HTTPException(status_code=503, detail={"dependencies": dependencies})

         return {"status": "ready", "dependencies": dependencies}

  Use 3-second timeout per dependency check (from RESEARCH.md).
  Return 503 only when ANY dependency is down (per PROD-06).
  </action>
  <verify>
  curl http://localhost:8000/health && curl http://localhost:8000/ready
  </verify>
  <done>
  /health returns 200 with uptime, /ready returns 200 with dependency status
  </done>
</task>

<task type="auto">
  <name>Integrate health router and add APP_START_TIME</name>
  <files>apps/backend/app/main.py</files>
  <action>
  1. Import health router:
     from app.api.health import router as health_router

  2. Add APP_START_TIME in lifespan startup:
     - Import os, time
     - In startup section: os.environ["APP_START_TIME"] = str(int(time.time()))

  3. Include health router:
     - After optimize_router include, add:
       app.include_router(health_router, tags=["health"])

  4. Remove old /health endpoint (line 164-172 in current main.py):
     - Delete @app.get("/health") function (conflicts with health router)

  5. Update Neo4j driver storage for readiness checks:
     - In lifespan startup, after creating Neo4j driver, store in app.state.neo4j_driver
     - Verify it's the same driver used elsewhere (check OntologyDriver.get_driver())

  Ensure health router is included BEFORE other routers (health checks should work even if other routes fail).
  </action>
  <verify>
  curl http://localhost:8000/health | jq . && curl http://localhost:8000/ready | jq .
  </verify>
  <done>
  Health endpoints accessible, no route conflicts, Neo4j driver available in app.state
  </done>
</task>

<task type="auto">
  <name>Add 503 handling for dependency failures</name>
  <files>apps/backend/app/api/optimize.py</files>
  <action>
  Add error handling for Redis/Neo4j unavailability:

  1. In submit_optimization, wrap job_manager.create_job in try/except:
     try:
         await job_manager.create_job(run_id, request.dict(), correlation_id)
     except redis.ConnectionError as e:
         raise HTTPException(
             status_code=503,
             detail={"error": "Redis unavailable", "redis_error": str(e)}
         )

  2. In get_optimization_status and get_optimization_result, wrap job_manager calls:
     try:
         job = await job_manager.get_job(run_id)
     except redis.ConnectionError as e:
         raise HTTPException(
             status_code=503,
             detail={"error": "Redis unavailable", "redis_error": str(e)}
         )

  3. In run_optimization_background, wrap Neo4j driver call:
     try:
         ontology_driver = OntologyDriver.get_driver()
     except Exception as e:
         await job_manager.update_job_status(run_id, "failed", error=f"Neo4j unavailable: {e}")
         return

  Use redis.ConnectionError for Redis failures (specific exception from redis-py).
  Log all 503 errors with structured logging (existing logger.info calls).
  Never expose stack traces to client (only error messages).
  </action>
  <verify>
  docker-compose stop redis && curl -X POST http://localhost:8000/api/v1/optimize -H "Content-Type: application/json" -d '{}' | jq .status
  </verify>
  <done>
  API returns 503 when Redis is down, error message includes dependency status
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete Redis job persistence and health checks</what-built>
  <how-to-verify>
  1. Start all services: docker-compose up -d
  2. Verify Redis and Neo4j are healthy: docker-compose ps
  3. Check health endpoints:
     - curl http://localhost:8000/health | jq .
     - curl http://localhost:8000/ready | jq .
  4. Submit optimization job:
     - curl -X POST http://localhost:8000/api/v1/optimize -H "Content-Type: application/json" -d '{"slate_id": "test", "drivers": [], "scenario_config": {"n_scenarios": 100}}'
  5. Note the run_id from response
  6. Check job status: curl http://localhost:8000/api/v1/optimize/{run_id}/status
  7. Restart backend container: docker-compose restart backend
  8. Verify job still exists after restart: curl http://localhost:8000/api/v1/optimize/{run_id}/status
  9. Stop Redis: docker-compose stop redis
  10. Verify /ready returns 503 with redis down status
  11. Start Redis: docker-compose start redis
  12. Verify /ready returns 200 again
  </how-to-verify>
  <resume-signal>Type "approved" if job persists across restart and health checks work correctly, or describe issues</resume-signal>
</task>

</tasks>

<verification>
1. Jobs submitted via API are stored in Redis (check with redis-cli: HGETALL job:{run_id})
2. Jobs survive container restart (status still accessible after restart)
3. /health returns 200 with timestamp, version, uptime_seconds
4. /ready returns 200 only when both Neo4j and Redis are available
5. /ready returns 503 with diagnostic detail when dependencies are down
6. Optimization endpoints return 503 when Redis is unavailable
</verification>

<success_criteria>
1. Optimization job persists across backend container restart
2. GET /jobs/{id}/status returns job data before and after restart
3. /health returns 200 regardless of dependency status (liveness only checks process)
4. /ready returns 503 when Redis is stopped, 200 when Redis is started
5. 503 responses include diagnostic detail (which dependencies are down)
6. No stack traces exposed in API error responses
</success_criteria>

<output>
After completion, create `.planning/phases/05-critical-infrastructure/05-02-SUMMARY.md`
</output>
