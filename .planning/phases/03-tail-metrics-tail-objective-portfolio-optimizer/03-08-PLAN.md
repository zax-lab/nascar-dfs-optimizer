---
phase: 03-tail-metrics-tail-objective-portfolio-optimizer
plan: 08
type: execute
wave: 2
depends_on: ["03-05", "03-06"]
files_modified:
  - apps/backend/tests/test_portfolio_generator.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Integration tests validate portfolio generator end-to-end"
    - "Tests validate exposure bookkeeping and DK compliance constraints"
    - "Tests validate CSV export format"
  artifacts:
    - path: "apps/backend/tests/test_portfolio_generator.py"
      provides: "Integration tests for portfolio generator"
      min_lines: 200
  key_links:
    - from: "test_portfolio_generator.py"
      to: "portfolio_generator.py"
      via: "import generate_portfolio, export_lineups_dk_format"
      pattern: "from app.portfolio_generator import"
---

<objective>
Create integration tests for portfolio generator functionality.

Verification.md identified missing test_portfolio_generator.py file that was claimed complete in 03-03-SUMMARY.md. This plan creates the missing test coverage for portfolio generation, exposure tracking, team stacking, and CSV export.

Purpose: Complete test coverage for Phase 3 portfolio generator functionality.

Output: New test file with comprehensive integration tests for portfolio generator.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-tail-metrics-tail-objective-portfolio-optimizer-VERIFICATION.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-03-SUMMARY.md

# Gap context from VERIFICATION.md
Gap 4: Missing test files (3 files claimed but don't exist)
- test_portfolio_generator.py claimed in 03-03-SUMMARY.md but not in tests/

# Dependencies on 03-05/03-06
- 03-05 adds bounded CVaR optimization to portfolio_generator.py
- 03-06 adds mean optimization objective_type parameter
- Tests should validate both CVaR and mean optimization paths
</context>

<tasks>

<task type="auto">
  <name>Create test_portfolio_generator.py with integration tests</name>
  <files>apps/backend/tests/test_portfolio_generator.py</files>
  <action>
  Create integration tests for portfolio generator functionality:

  Create new file apps/backend/tests/test_portfolio_generator.py:

  ```python
  """
  Integration tests for portfolio generator.

  Tests validate end-to-end portfolio generation including:
  - Lineup generation with CVaR optimization
  - Exposure bookkeeping across portfolio
  - DK compliance constraints
  - CSV export format
  """
  import pytest
  import numpy as np
  import pandas as pd
  from app.portfolio_generator import (
      generate_portfolio,
      export_lineups_dk_format
  )


  @pytest.fixture
  def sample_driver_data():
      """Sample driver pool for testing."""
      return [
          {
              'driver_id': 1,
              'name': 'Driver A',
              'salary': 9500,
              'team': 'Team 1'
          },
          {
              'driver_id': 2,
              'name': 'Driver B',
              'salary': 8500,
              'team': 'Team 1'
          },
          {
              'driver_id': 3,
              'name': 'Driver C',
              'salary': 8200,
              'team': 'Team 2'
          },
          {
              'driver_id': 4,
              'name': 'Driver D',
              'salary': 7800,
              'team': 'Team 2'
          },
          {
              'driver_id': 5,
              'name': 'Driver E',
              'salary': 7500,
              'team': 'Team 3'
          },
          {
              'driver_id': 6,
              'name': 'Driver F',
              'salary': 7200,
              'team': 'Team 3'
          },
          {
              'driver_id': 7,
              'name': 'Driver G',
              'salary': 7000,
              'team': 'Team 4'
          },
          {
              'driver_id': 8,
              'name': 'Driver H',
              'salary': 6800,
              'team': 'Team 4'
          },
          {
              'driver_id': 9,
              'name': 'Driver I',
              'salary': 6500,
              'team': 'Team 5'
          },
          {
              'driver_id': 10,
              'name': 'Driver J',
              'salary': 6200,
              'team': 'Team 5'
          },
          {
              'driver_id': 11,
              'name': 'Driver K',
              'salary': 6000,
              'team': 'Team 6'
          },
          {
              'driver_id': 12,
              'name': 'Driver L',
              'salary': 5800,
              'team': 'Team 6'
          },
      ]


  @pytest.fixture
  def sample_scenarios():
      """Sample scenario matrix for testing."""
      np.random.seed(42)
      # 2000 scenarios, 12 drivers, mean ~120, std ~20
      return np.random.randn(2000, 12) * 20 + 120


  class TestGeneratePortfolio:
      """Test portfolio generation with CVaR optimization."""

      def test_generate_portfolio_creates_requested_lineups(self, sample_driver_data, sample_scenarios):
          """Test that generate_portfolio creates requested number of lineups."""
          n_lineups = 5

          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=n_lineups,
              salary_cap=50000,
              n_drivers=6
          )

          assert len(lineups) >= n_lineups

      def test_each_lineup_has_six_drivers(self, sample_driver_data, sample_scenarios):
          """Test that each lineup contains exactly 6 drivers."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=3,
              salary_cap=50000,
              n_drivers=6
          )

          for lineup in lineups:
              assert 'drivers' in lineup
              assert len(lineup['drivers']) == 6

      def test_lineups_respect_salary_cap(self, sample_driver_data, sample_scenarios):
          """Test that all lineups are under salary cap."""
          salary_cap = 50000

          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=5,
              salary_cap=salary_cap,
              n_drivers=6
          )

          for lineup in lineups:
              total_salary = sum(
                  sample_driver_data[d]['salary']
                  for d in lineup['drivers']
              )
              assert total_salary <= salary_cap, f"Lineup exceeds salary cap: ${total_salary}"

      def test_lineups_have_no_duplicate_drivers(self, sample_driver_data, sample_scenarios):
          """Test that lineups don't contain duplicate drivers."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=3,
              salary_cap=50000,
              n_drivers=6
          )

          for lineup in lineups:
              unique_drivers = set(lineup['drivers'])
              assert len(unique_drivers) == 6, "Lineup contains duplicate drivers"

      def test_lineups_include_tail_metrics(self, sample_driver_data, sample_scenarios):
          """Test that lineups include tail metrics (CVaR, top X%, etc)."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=2,
              salary_cap=50000,
              n_drivers=6
          )

          for lineup in lineups:
              assert 'metrics' in lineup
              metrics = lineup['metrics']

              # Check required tail metrics
              assert 'cvar_99' in metrics
              assert 'top_x_pct' in metrics
              assert 'mean' in metrics

              # CVaR should be greater than mean (upper-tail optimization)
              assert metrics['cvar_99'] > metrics['mean'] * 0.95  # Allow small variance


  class TestExposureBookkeeping:
      """Test driver and team exposure limits across portfolio."""

      def test_driver_exposure_limit_enforced(self, sample_driver_data, sample_scenarios):
          """Test that no driver appears in more than max_driver_exposure fraction of lineups."""
          max_driver_exposure = 0.5  # Max 50% of lineups
          n_lineups = 10

          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=n_lineups,
              salary_cap=50000,
              n_drivers=6,
              max_driver_exposure=max_driver_exposure
          )

          # Count appearances of each driver
          driver_counts = {}
          for lineup in lineups:
              for driver_id in lineup['drivers']:
                  driver_counts[driver_id] = driver_counts.get(driver_id, 0) + 1

          # Check no driver exceeds exposure limit
          for driver_id, count in driver_counts.items():
              exposure = count / len(lineups)
              assert exposure <= max_driver_exposure + 0.1, f"Driver {driver_id} exposure {exposure:.2f} exceeds limit {max_driver_exposure}"

      def test_team_exposure_limit_enforced(self, sample_driver_data, sample_scenarios):
          """Test that no team dominates the portfolio."""
          max_team_exposure = 0.7  # Max 70% of lineups
          n_lineups = 10

          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=n_lineups,
              salary_cap=50000,
              n_drivers=6,
              max_team_exposure=max_team_exposure
          )

          # Count team appearances
          team_counts = {}
          for lineup in lineups:
              for driver_id in lineup['drivers']:
                  team = sample_driver_data[driver_id]['team']
                  team_counts[team] = team_counts.get(team, 0) + 1

          # Each lineup has 6 drivers, so max possible team appearances = 6 * n_lineups
          max_appearances = 6 * len(lineups)
          for team, count in team_counts.items():
              exposure = count / max_appearances
              assert exposure <= max_team_exposure + 0.1, f"Team {team} exposure {exposure:.2f} exceeds limit {max_team_exposure}"


  class TestDKCompliance:
      """Test DraftKings compliance constraints."""

      def test_team_stacking_constraints(self, sample_driver_data, sample_scenarios):
          """Test that lineups respect team stacking rules (min 2, max 3 from same team)."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=5,
              salary_cap=50000,
              n_drivers=6,
              min_stack=2,
              max_stack=3
          )

          for lineup in lineups:
              team_counts = {}
              for driver_id in lineup['drivers']:
                  team = sample_driver_data[driver_id]['team']
                  team_counts[team] = team_counts.get(team, 0) + 1

              # Check team stacking constraints
              for team, count in team_counts.items():
                  if count > 1:  # Only check teams with multiple drivers
                      assert 2 <= count <= 3, f"Team stacking violation: {team} has {count} drivers (min 2, max 3)"


  class TestCSVExport:
      """Test CSV export functionality."""

      def test_export_lineups_dk_format_creates_csv(self, sample_driver_data, sample_scenarios, tmp_path):
          """Test that export_lineups_dk_format creates CSV file."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=3,
              salary_cap=50000,
              n_drivers=6
          )

          output_path = tmp_path / "test_lineups.csv"
          result_path = export_lineups_dk_format(
              lineups=lineups,
              driver_data=sample_driver_data,
              output_path=str(output_path)
          )

          assert result_path == str(output_path)
          assert output_path.exists()

      def test_csv_format_has_no_header(self, sample_driver_data, sample_scenarios, tmp_path):
          """Test that CSV export has no header row (DraftKings format)."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=2,
              salary_cap=50000,
              n_drivers=6
          )

          output_path = tmp_path / "test_no_header.csv"
          export_lineups_dk_format(
              lineups=lineups,
              driver_data=sample_driver_data,
              output_path=str(output_path)
          )

          # Read CSV and check first row
          df = pd.read_csv(output_path, header=None)

          # First row should contain driver names, not column headers
          first_row = df.iloc[0].values
          assert all(isinstance(val, str) for val in first_row), "First row should contain driver names"
          assert 'F' not in str(first_row), "Should not have 'F' header (DraftKings format has no header)"

      def test_csv_has_six_columns(self, sample_driver_data, sample_scenarios, tmp_path):
          """Test that CSV export has exactly 6 columns (NASCAR format)."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=3,
              salary_cap=50000,
              n_drivers=6
          )

          output_path = tmp_path / "test_six_columns.csv"
          export_lineups_dk_format(
              lineups=lineups,
              driver_data=sample_driver_data,
              output_path=str(output_path)
          )

          df = pd.read_csv(output_path, header=None)

          assert df.shape[1] == 6, f"CSV should have 6 columns, has {df.shape[1]}"

      def test_csv_row_count_matches_lineups(self, sample_driver_data, sample_scenarios, tmp_path):
          """Test that CSV has one row per lineup."""
          lineups = generate_portfolio(
              driver_data=sample_driver_data,
              scenarios=sample_scenarios,
              n_lineups=5,
              salary_cap=50000,
              n_drivers=6
          )

          output_path = tmp_path / "test_row_count.csv"
          export_lineups_dk_format(
              lineups=lineups,
              driver_data=sample_driver_data,
              output_path=str(output_path)
          )

          df = pd.read_csv(output_path, header=None)

          assert len(df) == len(lineups), f"CSV should have {len(lineups)} rows, has {len(df)}"
  ```

  Use test_main.py as reference for test structure and fixtures
  Test both happy path and edge cases
  </action>
  <verify>
  Run new tests:
  ```bash
  cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
  pytest tests/test_portfolio_generator.py -v

  # Expected: All tests pass (15+ tests)
  ```
  </verify>
  <done>
  test_portfolio_generator.py file exists in tests/ directory
  All tests pass (15+ tests covering portfolio generation, exposure, DK compliance, CSV export)
  Tests validate end-to-end functionality
  Tests use fixtures for test data
  </done>
</task>

</tasks>

<verification>
After completion, verify test file gap is closed:

1. Check test file exists:
   ```bash
   ls -la apps/backend/tests/test_portfolio_generator.py
   ```
   File should exist

2. Run new tests:
   ```bash
   cd apps/backend
   pytest tests/test_portfolio_generator.py -v
   ```
   All tests should pass (15+ tests)

3. Verify test coverage:
   - test_portfolio_generator.py: 15+ tests for portfolio generation
</verification>

<success_criteria>
test_portfolio_generator.py created and passing:
- test_portfolio_generator.py exists with 15+ passing tests
- Tests validate portfolio generation, exposure, DK compliance, CSV export
- Tests depend on 03-05/03-06 features (CVaR and mean optimization)
</success_criteria>

<output>
After completion, create `.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-08-SUMMARY.md`
</output>
