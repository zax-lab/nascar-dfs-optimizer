---
phase: 03-tail-metrics-tail-objective-portfolio-optimizer
plan: 03
type: execute
wave: 2
depends_on: [03-01, 03-02]
files_modified:
  - apps/backend/app/portfolio_generator.py
  - apps/backend/app/constraints/dk_rules.py
  - apps/backend/app/constraints/exposure.py
  - apps/backend/app/constraints/diversity.py
  - apps/backend/app/tests/test_portfolio_generator.py
autonomous: true

must_haves:
  truths:
    - "Portfolio generator produces 20-150 lineups with exposure controls (driver and team limits)"
    - "Each lineup optimized independently for CVaR (no warm starting, avoids local optima)"
    - "Scenario matrices cached and shared across portfolio (no re-sim per lineup)"
    - "DraftKings compliance constraints enforced (6 drivers, $50k salary cap, team stacking 2-3)"
    - "Correlation penalty minimizes pairwise lineup similarity for diversity"
    - "CSV export compatible with DraftKings upload format"
  artifacts:
    - path: "apps/backend/app/portfolio_generator.py"
      provides: "Iterative portfolio generation with exposure bookkeeping"
      exports: ["generate_portfolio", "generate_lineup_with_cvar", "compute_portfolio_correlation"]
      min_lines: 300
    - path: "apps/backend/app/constraints/dk_rules.py"
      provides: "DraftKings-specific constraints (salary, roster rules, team stacking)"
      exports: ["add_dk_compliance_constraints", "validate_dk_lineup"]
      min_lines: 100
    - path: "apps/backend/app/constraints/exposure.py"
      provides: "Driver and team exposure limits across portfolio"
      exports: ["add_exposure_constraints", "update_exposure_book"]
      min_lines: 80
    - path: "apps/backend/app/constraints/diversity.py"
      provides: "Correlation penalty and lineup diversity enforcement"
      exports: ["add_correlation_penalty", "compute_pairwise_similarity"]
      min_lines: 100
    - path: "apps/backend/app/tests/test_portfolio_generator.py"
      provides: "Integration tests for portfolio generation"
      min_lines: 200
  key_links:
    - from: "apps/backend/app/portfolio_generator.py"
      to: "apps/backend/app/tail_objectives.py"
      via: "build_multi_cvar_objective for CVaR optimization"
      pattern: "from app.tail_objectives import build_multi_cvar_objective"
    - from: "apps/backend/app/portfolio_generator.py"
      to: "packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py"
      via: "ScenarioComponents converted to cached scenario matrices"
      pattern: "scenarios_cache.*get_scenarios\\(.*race_id"
    - from: "apps/backend/app/constraints/dk_rules.py"
      to: "apps/backend/app/optimizer.py"
      via: "Existing DK salary and roster constraints"
      pattern: "salary_cap.*n_drivers.*min_stack.*max_stack"
---

<objective>
Build portfolio generator that produces multiple DFS lineups optimized for top-tail outcomes with exposure controls, diversity constraints, and DraftKings compliance. Each lineup is an independent CVaR optimization using cached scenario matrices.

Purpose: Generate 20-150 diverse lineups targeting tournament equity (top 1% outcomes) with exposure limits, correlation-based diversity, and full DraftKings compliance.

Output: Working portfolio_generator.py with iterative CVaR optimization, exposure bookkeeping, diversity constraints, DK compliance enforcement, CSV export, and comprehensive integration tests.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-CONTEXT.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-RESEARCH.md

# Phase 3 Plan 01-02 outputs
@apps/backend/app/tail_metrics.py
@apps/backend/app/tail_objectives.py

# Phase 2 outputs
@packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py

# Existing optimizer
@apps/backend/app/optimizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create constraints modules (DK rules, exposure, diversity)</name>
  <files>
    apps/backend/app/constraints/dk_rules.py
    apps/backend/app/constraints/exposure.py
    apps/backend/app/constraints/diversity.py
  </files>
  <action>
Create three constraint modules in apps/backend/app/constraints/:

### 1. dk_rules.py - DraftKings compliance constraints

```python
from pulp import LpProblem, lpSum, LpVariable
from typing import Dict, List, Any

def add_dk_compliance_constraints(
    prob: LpProblem,
    x: Dict[int, LpVariable],
    driver_data: List[Dict[str, Any]],
    salary_cap: int = 50000,
    n_drivers: int = 6,
    min_stack: int = 2,
    max_stack: int = 3
) -> None:
    """
    Add DraftKings NASCAR Classic constraints to optimization problem.

    Constraints:
    - Select exactly n_drivers (6)
    - Total salary <= salary_cap ($50,000)
    - Team stacking: min_stack (2) to max_stack (3) drivers per team

    Args:
        prob: PuLP problem to add constraints to
        x: Dict mapping driver_id -> binary selection variable
        driver_data: List of driver dicts with salary, team keys
        salary_cap: Maximum total salary
        n_drivers: Number of drivers to select
        min_stack: Minimum drivers from same team
        max_stack: Maximum drivers from same team
    """
    # Roster size constraint
    prob += lpSum(x.values()) == n_drivers, "Select_Exactly_N_Drivers"

    # Salary cap constraint
    prob += lpSum(
        d["salary"] * x[d["driver_id"]] for d in driver_data
    ) <= salary_cap, "Salary_Cap"

    # Team stacking constraints
    teams = {}
    for driver in driver_data:
        team = driver["team"]
        if team not in teams:
            teams[team] = []
        teams[team].append(driver)

    for team, team_drivers in teams.items():
        team_selection = lpSum(
            x[d["driver_id"]] for d in team_drivers
        )
        # Max constraint (min constraint handled by exposure bookkeeping)
        prob += team_selection <= max_stack, f"Max_{max_stack}_From_Team_{team}"

def validate_dk_lineup(
    lineup: List[int],
    driver_data: List[Dict[str, Any]],
    salary_cap: int = 50000,
    n_drivers: int = 6,
    min_stack: int = 2,
    max_stack: int = 3
) -> Dict[str, Any]:
    """
    Validate lineup against DraftKings rules.

    Returns dict with valid (bool) and errors (list of str).
    """
    errors = []

    # Check roster size
    if len(lineup) != n_drivers:
        errors.append(f"Roster size: {len(lineup)}, expected {n_drivers}")

    # Check salary cap
    total_salary = sum(
        next(d["salary"] for d in driver_data if d["driver_id"] == driver_id)
        for driver_id in lineup
    )
    if total_salary > salary_cap:
        errors.append(f"Salary cap violation: ${total_salary}, max ${salary_cap}")

    # Check team stacking
    team_counts = {}
    for driver_id in lineup:
        driver = next(d for d in driver_data if d["driver_id"] == driver_id)
        team = driver["team"]
        team_counts[team] = team_counts.get(team, 0) + 1

    for team, count in team_counts.items():
        if count < min_stack:
            errors.append(f"Team stacking: {team} has {count} drivers, min {min_stack}")
        if count > max_stack:
            errors.append(f"Team stacking: {team} has {count} drivers, max {max_stack}")

    # Check no duplicates
    if len(set(lineup)) != len(lineup):
        errors.append("Duplicate drivers in lineup")

    return {"valid": len(errors) == 0, "errors": errors}
```

### 2. exposure.py - Exposure bookkeeping

```python
from typing import Dict, List
from pulp import LpProblem, lpSum, LpVariable

def add_exposure_constraints(
    prob: LpProblem,
    x: Dict[int, LpVariable],
    exposure_book: Dict[int, int],
    n_lineups_generated: int,
    max_driver_exposure: float = 0.5,
    max_team_exposure: float = 0.7,
    driver_data: List[Dict] = None
) -> None:
    """
    Add exposure constraints to prevent overexposure across portfolio.

    Args:
        prob: PuLP problem
        x: Driver selection variables
        exposure_book: Dict mapping driver_id -> usage count
        n_lineups_generated: Number of lineups already generated
        max_driver_exposure: Max fraction of portfolios containing a driver
        max_team_exposure: Max fraction of portfolios from same team
        driver_data: List of driver dicts (for team exposure)
    """
    for driver_id, count in exposure_book.items():
        if n_lineups_generated > 0:
            current_exposure = count / n_lineups_generated
            if current_exposure >= max_driver_exposure:
                # Driver already at max exposure, force exclusion
                prob += x[driver_id] == 0, f"Exclude_Overexposed_Driver_{driver_id}"

    # Team exposure (if driver_data provided)
    if driver_data:
        teams = {}
        for driver in driver_data:
            team = driver["team"]
            if team not in teams:
                teams[team] = []
            teams[team].append(driver["driver_id"])

        for team, team_drivers in teams.items():
            team_usage = sum(exposure_book.get(did, 0) for did in team_drivers)
            if n_lineups_generated > 0:
                current_exposure = team_usage / n_lineups_generated
                if current_exposure >= max_team_exposure:
                    # Team at max exposure, exclude all drivers
                    for driver_id in team_drivers:
                        if driver_id in x:
                            prob += x[driver_id] == 0, f"Exclude_Overexposed_Team_{team}_{driver_id}"

def update_exposure_book(
    exposure_book: Dict[int, int],
    lineup: List[int]
) -> Dict[int, int]:
    """
    Update exposure book with new lineup.

    Args:
        exposure_book: Current exposure counts
        lineup: List of driver_ids in new lineup

    Returns:
        Updated exposure book
    """
    new_book = exposure_book.copy()
    for driver_id in lineup:
        new_book[driver_id] = new_book.get(driver_id, 0) + 1
    return new_book
```

### 3. diversity.py - Correlation penalty

```python
import numpy as np
from typing import Dict, List
from pulp import LpProblem, lpSum, LpVariable

def add_correlation_penalty(
    prob: LpProblem,
    x: Dict[int, LpVariable],
    previous_lineups: List[Dict],
    correlation_weight: float = 0.1
) -> pulp.LpAffineExpression:
    """
    Add correlation penalty to encourage lineup diversity.

    Penalizes selecting drivers that frequently appear in previous lineups.

    Args:
        prob: PuLP problem
        x: Driver selection variables
        previous_lineups: List of previous lineup dicts with "drivers" key
        correlation_weight: Penalty strength

    Returns:
        Correlation penalty expression (to subtract from objective)
    """
    if not previous_lineups:
        return 0  # No penalty for first lineup

    penalty = 0
    for prev_lineup in previous_lineups:
        overlap = lpSum(
            x[driver_id] for driver_id in prev_lineup["drivers"] if driver_id in x
        )
        penalty += overlap

    return correlation_weight * penalty

def compute_pairwise_similarity(
    lineup1: List[int],
    lineup2: List[int]
) -> float:
    """
    Compute Jaccard similarity between two lineups.

    Jaccard = |intersection| / |union|
    """
    set1 = set(lineup1)
    set2 = set(lineup2)
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0

def compute_portfolio_correlation(
    lineups: List[Dict[str, Any]]
) -> Dict[str, float]:
    """
    Compute pairwise correlation metrics for portfolio.

    Returns dict with avg_similarity, max_similarity, most_similar_pair.
    """
    n = len(lineups)
    if n < 2:
        return {"avg_similarity": 0.0, "max_similarity": 0.0, "most_similar_pair": None}

    similarities = []
    most_similar = (0.0, None, None)  # (similarity, lineup_i, lineup_j)

    for i in range(n):
        for j in range(i + 1, n):
            sim = compute_pairwise_similarity(
                lineups[i]["drivers"],
                lineups[j]["drivers"]
            )
            similarities.append(sim)
            if sim > most_similar[0]:
                most_similar = (sim, i, j)

    return {
        "avg_similarity": np.mean(similarities),
        "max_similarity": np.max(similarities) if similarities else 0.0,
        "most_similar_pair": most_similar[1:] if most_similar[1] is not None else None
    }
```

DO NOT hardcode salary cap or roster size (make parameters configurable).
DO NOT skip team stacking validation (min 2, max 3 per team is critical for NASCAR DFS).
DO NOT use hard no-good constraints (use soft correlation penalty instead).
DO validate lineups after optimization (catch solver errors or constraint bugs).

Reference: 03-RESEARCH.md "Pattern 4: Portfolio Diversity via Correlation Penalty"
  </action>
  <verify>
Run: python3 -c "
from app.constraints.dk_rules import add_dk_compliance_constraints, validate_dk_lineup
from app.constraints.exposure import add_exposure_constraints, update_exposure_book
from app.constraints.diversity import add_correlation_penalty, compute_pairwise_similarity
from pulp import LpProblem, LpVariable, LpMaximize

# Test DK rules
prob = LpProblem('Test_DK', LpMaximize)
x = {i: LpVariable(f'd_{i}', cat='Binary') for i in range(10)}
drivers = [{'driver_id': i, 'salary': 8000, 'team': f'team_{i%3}'} for i in range(10)]
add_dk_compliance_constraints(prob, x, drivers)
print(f'DK constraints added: {len(prob.constraints)} constraints')

# Test exposure
book = {0: 5, 1: 3}
new_book = update_exposure_book(book, [0, 2, 4])
print(f'Exposure book updated: {new_book}')

# Test diversity
sim = compute_pairwise_similarity([0, 1, 2, 3, 4, 5], [0, 1, 2, 6, 7, 8])
print(f'Jaccard similarity: {sim:.3f}')
"
Expected: DK constraints added (1 roster + 1 salary + team constraints), exposure book updated, similarity computed
  </verify>
  <done>
add_dk_compliance_constraints() enforces roster size, salary cap, team stacking
validate_dk_lineup() catches violations and returns error list
update_exposure_book() tracks driver usage across portfolio
add_correlation_penalty() creates penalty expression for diversity
compute_pairwise_similarity() calculates Jaccard similarity
  </done>
</task>

<task type="auto">
  <name>Task 2: Create portfolio generator with CVaR optimization</name>
  <files>apps/backend/app/portfolio_generator.py</files>
  <action>
Create portfolio_generator.py with iterative CVaR optimization:

```python
import logging
from typing import Dict, List, Any, Optional
import numpy as np
import pandas as pd
from pulp import LpProblem, LpMaximize, LpVariable, PULP_CBC_CMD, LpStatus

from app.tail_objectives import build_multi_cvar_objective
from app.tail_metrics import compute_tail_metrics, adaptive_scenario_count
from app.constraints.dk_rules import add_dk_compliance_constraints, validate_dk_lineup
from app.constraints.exposure import add_exposure_constraints, update_exposure_book
from app.constraints.diversity import add_correlation_penalty, compute_portfolio_correlation

logger = logging.getLogger(__name__)

class ScenarioCache:
    """Cache scenario matrices to avoid re-simulation."""

    def __init__(self):
        self._cache = {}

    def get_scenarios(
        self,
        race_id: str,
        n_scenarios: int,
        scenario_fn: callable
    ) -> np.ndarray:
        """
        Get scenarios from cache or generate using scenario_fn.

        Args:
            race_id: Race identifier
            n_scenarios: Number of scenarios to generate
            scenario_fn: Function that generates scenarios (takes n_scenarios, returns ndarray)

        Returns:
            ndarray (n_scenarios, n_drivers) with DFS points per scenario
        """
        cache_key = f"{race_id}_{n_scenarios}"

        if cache_key in self._cache:
            logger.info(f"Cache hit for {cache_key}")
            return self._cache[cache_key]

        logger.info(f"Cache miss for {cache_key}, generating {n_scenarios} scenarios")
        scenarios = scenario_fn(n_scenarios)
        self._cache[cache_key] = scenarios
        return scenarios

def generate_lineup_with_cvar(
    scenarios: np.ndarray,
    driver_data: List[Dict[str, Any]],
    exposure_book: Dict[int, int],
    n_lineups_generated: int,
    previous_lineups: List[Dict],
    cvar_alphas: List[float] = [0.99, 0.95],
    cvar_weights: List[float] = [0.7, 0.3],
    correlation_weight: float = 0.1,
    max_driver_exposure: float = 0.5,
    max_team_exposure: float = 0.7,
    salary_cap: int = 50000,
    n_drivers: int = 6,
    min_stack: int = 2,
    max_stack: int = 3,
    solver_time_limit: int = 30
) -> Optional[Dict[str, Any]]:
    """
    Generate single lineup optimized for CVaR.

    Args:
        scenarios: ndarray (n_scenarios, n_drivers) with DFS points
        driver_data: List of driver dicts with salary, team
        exposure_book: Current exposure counts
        n_lineups_generated: Number of lineups already generated
        previous_lineups: Previous lineups for diversity penalty
        cvar_alphas: CVaR quantiles for Multi-CVaR
        cvar_weights: Weights for Multi-CVaR
        correlation_weight: Penalty strength for diversity
        max_driver_exposure: Max driver exposure fraction
        max_team_exposure: Max team exposure fraction
        salary_cap: DK salary cap
        n_drivers: Roster size
        min_stack: Min team stacking
        max_stack: Max team stacking
        solver_time_limit: Max solve time in seconds

    Returns:
        Lineup dict with drivers, cvar_99, cvar_95, top_1pct, exposure, or None if infeasible
    """
    # Create optimization problem
    prob = LpProblem("CVaR_Lineup", LpMaximize)

    # Binary selection variables
    n_drivers_total = len(driver_data)
    x = {d["driver_id"]: LpVariable(f"select_{d['driver_id']}", cat="Binary") for d in driver_data}

    # Build Multi-CVaR objective
    cvar_objective = build_multi_cvar_objective(prob, scenarios, x, cvar_alphas, cvar_weights)

    # Add correlation penalty for diversity
    correlation_penalty = add_correlation_penalty(prob, x, previous_lineups, correlation_weight)

    # Combined objective: maximize CVaR - penalty
    prob += cvar_objective - correlation_penalty, "Multi_CVaR_With_Diversity"

    # Add DK compliance constraints
    add_dk_compliance_constraints(prob, x, driver_data, salary_cap, n_drivers, min_stack, max_stack)

    # Add exposure constraints
    add_exposure_constraints(prob, x, exposure_book, n_lineups_generated, max_driver_exposure, max_team_exposure, driver_data)

    # Solve
    solver = PULP_CBC_CMD(msg=0, timeLimit=solver_time_limit)
    prob.solve(solver)

    # Check solution status
    if LpStatus[prob.status] != "Optimal":
        logger.warning(f"Solver status: {LpStatus[prob.status]}")
        return None

    # Extract selected drivers
    selected_drivers = [
        d["driver_id"] for d in driver_data
        if x[d["driver_id"]].value() == 1
    ]

    # Validate lineup
    validation = validate_dk_lineup(selected_drivers, driver_data, salary_cap, n_drivers, min_stack, max_stack)
    if not validation["valid"]:
        logger.error(f"Lineup validation failed: {validation['errors']}")
        return None

    # Compute tail metrics for this lineup
    lineup_scenarios = scenarios[:, [driver_data.index(d) for d in driver_data if d["driver_id"] in selected_drivers]]
    lineup_points = lineup_scenarios.sum(axis=1)
    metrics = compute_tail_metrics(lineup_points, alpha=cvar_alphas[0])

    # Calculate exposure for this lineup
    exposure = {
        driver_id: (exposure_book.get(driver_id, 0) + 1) / (n_lineups_generated + 1)
        for driver_id in selected_drivers
    }

    return {
        "drivers": selected_drivers,
        "cvar_99": metrics.CVaR,
        "cvar_95": compute_tail_metrics(lineup_points, alpha=0.95).CVaR,
        "top_1pct": metrics.top_X_pct,
        "conditional_upside": metrics.conditional_upside,
        "exposure": exposure,
        "total_salary": sum(d["salary"] for d in driver_data if d["driver_id"] in selected_drivers)
    }

def generate_portfolio(
    race_id: str,
    driver_data: List[Dict[str, Any]],
    scenario_fn: callable,
    n_lineups: int = 20,
    n_scenarios: int = 10000,
    cvar_alphas: List[float] = [0.99, 0.95],
    cvar_weights: List[float] = [0.7, 0.3],
    correlation_weight: float = 0.1,
    max_driver_exposure: float = 0.5,
    max_team_exposure: float = 0.7,
    salary_cap: int = 50000,
    n_drivers: int = 6,
    min_stack: int = 2,
    max_stack: int = 3,
    solver_time_limit: int = 30,
    random_seed: int = 42
) -> List[Dict[str, Any]]:
    """
    Generate portfolio of lineups optimized for CVaR.

    Args:
        race_id: Race identifier
        driver_data: List of driver dicts
        scenario_fn: Function to generate scenarios (n_scenarios) -> ndarray
        n_lineups: Number of lineups to generate
        n_scenarios: Number of scenarios for CVaR optimization
        cvar_alphas: CVaR quantiles
        cvar_weights: CVaR weights
        correlation_weight: Diversity penalty weight
        max_driver_exposure: Max driver exposure
        max_team_exposure: Max team exposure
        salary_cap: DK salary cap
        n_drivers: Roster size
        min_stack: Min team stacking
        max_stack: Max team stacking
        solver_time_limit: Max solve time
        random_seed: Random seed for scenario generation

    Returns:
        List of lineup dicts
    """
    logger.info(f"Generating portfolio: {n_lineups} lineups, {n_scenarios} scenarios")

    # Check adaptive scenario count
    min_scenarios = adaptive_scenario_count(cvar_alphas[0])
    if n_scenarios < min_scenarios:
        logger.warning(f"Requested {n_scenarios} < recommended {min_scenarios} for CVaR({cvar_alphas[0]})")

    # Cache scenarios
    cache = ScenarioCache()
    scenarios = cache.get_scenarios(race_id, n_scenarios, scenario_fn)
    logger.info(f"Using {len(scenarios)} scenarios for optimization")

    # Generate lineups iteratively
    lineups = []
    exposure_book = {}

    for lineup_idx in range(n_lineups):
        logger.info(f"Generating lineup {lineup_idx + 1}/{n_lineups}")

        lineup = generate_lineup_with_cvar(
            scenarios=scenarios,
            driver_data=driver_data,
            exposure_book=exposure_book,
            n_lineups_generated=len(lineups),
            previous_lineups=lineups,
            cvar_alphas=cvar_alphas,
            cvar_weights=cvar_weights,
            correlation_weight=correlation_weight,
            max_driver_exposure=max_driver_exposure,
            max_team_exposure=max_team_exposure,
            salary_cap=salary_cap,
            n_drivers=n_drivers,
            min_stack=min_stack,
            max_stack=max_stack,
            solver_time_limit=solver_time_limit
        )

        if lineup is None:
            logger.warning(f"Failed to generate lineup {lineup_idx + 1}")
            break

        lineups.append(lineup)
        exposure_book = update_exposure_book(exposure_book, lineup["drivers"])

        logger.info(
            f"Lineup {lineup_idx + 1}: CVaR(99%)={lineup['cvar_99']:.2f}, "
            f"Top 1%={lineup['top_1pct']:.2f}, Salary=${lineup['total_salary']}"
        )

    # Compute portfolio correlation
    correlation = compute_portfolio_correlation(lineups)
    logger.info(
        f"Portfolio correlation: avg={correlation['avg_similarity']:.3f}, "
        f"max={correlation['max_similarity']:.3f}"
    )

    return lineups
```

DO NOT use warm starting (each lineup is independent fresh solve for diversity).
DO NOT re-generate scenarios per lineup (use ScenarioCache for 100x speedup).
DO NOT skip exposure bookkeeping (critical for portfolio risk management).
DO validate each lineup after optimization (catch solver errors).

Reference: 03-RESEARCH.md "Pattern 2: Scenario Matrix Caching and Reuse"
  </action>
  <verify>
Run: python3 -c "
from app.portfolio_generator import generate_portfolio, ScenarioCache
import numpy as np

# Mock scenario function
def mock_scenarios(n):
    np.random.seed(42)
    return np.random.randn(n, 10) * 10 + 50  # 10 drivers, DFS points ~50 +/- 10

# Mock driver data
drivers = [
    {'driver_id': i, 'salary': 8000, 'team': f'team_{i%3}'}
    for i in range(10)
]

# Generate small portfolio
lineups = generate_portfolio(
    race_id='test_race',
    driver_data=drivers,
    scenario_fn=mock_scenarios,
    n_lineups=3,
    n_scenarios=1000,
    correlation_weight=0.1
)

print(f'Generated {len(lineups)} lineups')
for i, lineup in enumerate(lineups):
    print(f'Lineup {i+1}: CVaR(99%)={lineup[\"cvar_99\"]:.2f}, drivers={lineup[\"drivers\"]}')
"
Expected: 3 lineups generated, each with 6 drivers, CVaR values logged
  </verify>
  <done>
ScenarioCache caches and reuses scenarios across lineups
generate_lineup_with_cvar() creates single CVaR-optimized lineup
generate_portfolio() iteratively generates multiple lineups
Exposure book tracks driver usage across portfolio
Correlation penalty encourages lineup diversity
  </done>
</task>

<task type="auto">
  <name>Task 3: Add CSV export and integration tests</name>
  <files>
    apps/backend/app/portfolio_generator.py
    apps/backend/app/tests/test_portfolio_generator.py
  </files>
  <action>
Add CSV export function to portfolio_generator.py and create integration tests:

### CSV export function

```python
def export_lineups_dk_format(
    lineups: List[Dict[str, Any]],
    driver_data: List[Dict[str, Any]],
    filename: str = "nascar_lineups.csv"
) -> str:
    """
    Export lineups in DraftKings CSV upload format.

    Format: One row per lineup, 6 columns (F, F.1, F.2, F.3, F.4, F.5)
    Values: Driver names
    No header row (DK auto-detects format)

    Args:
        lineups: List of lineup dicts with "drivers" key
        driver_data: List of driver dicts with driver_id, name
        filename: Output filename

    Returns:
        Path to exported CSV file
    """
    # Build driver_id -> name mapping
    driver_names = {d["driver_id"]: d["name"] for d in driver_data}

    # Create rows (one per lineup)
    rows = []
    for lineup in lineups:
        # Ensure exactly 6 drivers
        if len(lineup["drivers"]) != 6:
            logger.warning(f"Lineup has {len(lineup['drivers'])} drivers, expected 6")
            continue

        row = {
            "F": driver_names.get(lineup["drivers"][0], "Unknown"),
            "F.1": driver_names.get(lineup["drivers"][1], "Unknown"),
            "F.2": driver_names.get(lineup["drivers"][2], "Unknown"),
            "F.3": driver_names.get(lineup["drivers"][3], "Unknown"),
            "F.4": driver_names.get(lineup["drivers"][4], "Unknown"),
            "F.5": driver_names.get(lineup["drivers"][5], "Unknown"),
        }
        rows.append(row)

    # Export to CSV (no header)
    df = pd.DataFrame(rows)
    filepath = f"/tmp/{filename}"
    df.to_csv(filepath, header=False, index=False)

    logger.info(f"Exported {len(rows)} lineups to {filepath}")
    return filepath
```

### Integration tests (test_portfolio_generator.py)

```python
import pytest
import numpy as np
from app.portfolio_generator import generate_portfolio, ScenarioCache, export_lineups_dk_format
from app.constraints.dk_rules import validate_dk_lineup

@pytest.fixture
def mock_driver_data():
    """Create mock driver data for testing."""
    return [
        {
            "driver_id": i,
            "name": f"Driver_{i}",
            "salary": 7500 + i * 100,
            "team": f"team_{i % 3}"
        }
        for i in range(12)  # 12 drivers (4 teams of 3)
    ]

@pytest.fixture
def mock_scenario_fn():
    """Create mock scenario generation function."""
    def fn(n_scenarios):
        np.random.seed(42)
        return np.random.randn(n_scenarios, 12) * 10 + 50
    return fn

def test_scenario_cache_reuse(mock_scenario_fn):
    """Test that scenarios are cached and reused."""
    cache = ScenarioCache()

    # First call should generate
    scenarios1 = cache.get_scenarios("race_1", 1000, mock_scenario_fn)
    assert scenarios1.shape == (1000, 12)

    # Second call should use cache
    scenarios2 = cache.get_scenarios("race_1", 1000, mock_scenario_fn)
    assert scenarios2 is scenarios1  # Same object reference

    # Different race should generate new
    scenarios3 = cache.get_scenarios("race_2", 1000, mock_scenario_fn)
    assert scenarios3 is not scenarios1

def test_generate_portfolio_size(mock_driver_data, mock_scenario_fn):
    """Test portfolio generation produces requested lineups."""
    lineups = generate_portfolio(
        race_id="test_race",
        driver_data=mock_driver_data,
        scenario_fn=mock_scenario_fn,
        n_lineups=5,
        n_scenarios=1000
    )

    assert len(lineups) == 5

    for lineup in lineups:
        assert len(lineup["drivers"]) == 6
        assert "cvar_99" in lineup
        assert "top_1pct" in lineup
        assert "exposure" in lineup

def test_dk_compliance_all_lineups(mock_driver_data, mock_scenario_fn):
    """Test all generated lineups are DK-compliant."""
    lineups = generate_portfolio(
        race_id="test_race",
        driver_data=mock_driver_data,
        scenario_fn=mock_scenario_fn,
        n_lineups=10,
        n_scenarios=1000
    )

    for lineup in lineups:
        validation = validate_dk_lineup(
            lineup["drivers"],
            mock_driver_data,
            salary_cap=50000,
            n_drivers=6,
            min_stack=2,
            max_stack=3
        )
        assert validation["valid"], f"Lineup validation failed: {validation['errors']}"

def test_exposure_limits(mock_driver_data, mock_scenario_fn):
    """Test exposure limits are respected."""
    lineups = generate_portfolio(
        race_id="test_race",
        driver_data=mock_driver_data,
        scenario_fn=mock_scenario_fn,
        n_lineups=10,
        n_scenarios=1000,
        max_driver_exposure=0.5  # Max 50% exposure
    )

    # Count driver usage
    driver_usage = {}
    for lineup in lineups:
        for driver_id in lineup["drivers"]:
            driver_usage[driver_id] = driver_usage.get(driver_id, 0) + 1

    # Check no driver exceeds 50% exposure (5 out of 10 lineups)
    for driver_id, count in driver_usage.items():
        assert count <= 5, f"Driver {driver_id} has {count}/10 exposure (max 50%)"

def test_portfolio_diversity(mock_driver_data, mock_scenario_fn):
    """Test portfolio lineups are diverse (not all identical)."""
    lineups = generate_portfolio(
        race_id="test_race",
        driver_data=mock_driver_data,
        scenario_fn=mock_scenario_fn,
        n_lineups=10,
        n_scenarios=1000,
        correlation_weight=0.2
    )

    # Compute pairwise similarities
    from app.constraints.diversity import compute_portfolio_correlation
    correlation = compute_portfolio_correlation(lineups)

    # Average similarity should be < 0.7 (diverse)
    assert correlation["avg_similarity"] < 0.7, \
        f"Average similarity {correlation['avg_similarity']:.3f} too high (lineups not diverse)"

def test_export_csv_format(mock_driver_data, mock_scenario_fn):
    """Test CSV export produces DK-compatible format."""
    lineups = generate_portfolio(
        race_id="test_race",
        driver_data=mock_driver_data,
        scenario_fn=mock_scenario_fn,
        n_lineups=3,
        n_scenarios=1000
    )

    filepath = export_lineups_dk_format(lineups, mock_driver_data, "test_lineups.csv")

    # Read CSV and check format
    import pandas as pd
    df = pd.read_csv(filepath, header=None)

    assert df.shape == (3, 6)  # 3 lineups, 6 driver columns
    assert all(df.dtypes == object)  # All strings (driver names)
```

DO NOT include header row in CSV export (DK auto-detects format).
DO NOT allow lineups with != 6 drivers in export (skip and log warning).
DO validate all lineups for DK compliance in tests (catch constraint bugs).
DO test exposure limits are actually enforced (not just documented).

Reference: 03-RESEARCH.md "CSV Export for DraftKings Upload"
  </action>
  <verify>
Run: cd apps/backend && python3 -m pytest tests/test_portfolio_generator.py -v
Expected: All 6 integration tests pass
Run: python3 -c "
from app.portfolio_generator import export_lineups_dk_format
import pandas as pd

# Mock lineups
lineups = [{'drivers': [0, 1, 2, 3, 4, 5]}]
drivers = [{'driver_id': i, 'name': f'Driver_{i}'} for i in range(6)]

# Export
filepath = export_lineups_dk_format(lineups, drivers, 'test.csv')
df = pd.read_csv(filepath, header=None)

print(f'CSV shape: {df.shape}')
print(f'CSV content:\\n{df}')
"
Expected: CSV shape (1, 6), no header, driver names in columns
  </verify>
  <done>
export_lineups_dk_format() creates DK-compatible CSV (6 columns, no header)
All 6 integration tests pass (cache reuse, portfolio size, DK compliance, exposure limits, diversity, CSV export)
Exposure limits enforced (no driver >50% exposure)
Portfolio correlation < 0.7 (diverse lineups)
CSV format validated for DK upload
  </done>
</task>

</tasks>

<verification>
## Phase Verification Criteria

### Core Functionality
1. Portfolio generator produces 20-150 lineups with exposure controls
2. Each lineup optimized independently for CVaR (no warm starting)
3. Scenario matrices cached and shared across portfolio
4. DraftKings compliance constraints enforced (6 drivers, $50k salary, team stacking 2-3)
5. Correlation penalty minimizes pairwise lineup similarity
6. CSV export compatible with DraftKings upload format

### Integration Points
1. generate_portfolio() uses build_multi_cvar_objective() from Plan 02
2. compute_tail_metrics() from Plan 01 validates lineup performance
3. ScenarioCache integrates with Phase 2 scenario generation
4. DK constraints extend existing optimizer.py constraints

### Avoidance Checks
1. No warm starting lineups (each is independent fresh solve)
2. No re-simulating scenarios per lineup (use ScenarioCache)
3. No hard no-good constraints (use soft correlation penalty)
4. No skipping exposure validation (must track and enforce limits)
5. No CSV header row (DK upload format requires no header)
</verification>

<success_criteria>
1. generate_portfolio() produces requested number of lineups (20-150)
2. All lineups pass validate_dk_lineup() checks
3. Exposure limits enforced (max_driver_exposure, max_team_exposure)
4. Portfolio correlation < 0.7 (avg pairwise similarity)
5. ScenarioCache caches scenarios (verified by object reference check)
6. CSV export produces 6-column format with no header
7. All 6 integration tests pass
8. Solver converges to optimal solutions for all lineups
</success_criteria>

<output>
After completion, create `.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-03-SUMMARY.md`
</output>
