---
phase: 03-tail-metrics-tail-objective-portfolio-optimizer
plan: 06
type: execute
wave: 2
depends_on: ["03-05"]
files_modified:
  - apps/backend/app/api/optimize_portfolio.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Tail validation compares CVaR-optimized vs actual mean-optimized baseline (not fake multiplier)"
    - "Validation confirms optimizer produces higher CVaR than mean optimization"
    - "Tail improvement computed from real portfolio comparison"
  artifacts:
    - path: "apps/backend/app/api/optimize_portfolio.py"
      provides: "API endpoint with real tail validation"
      contains: "generate_mean_baseline_portfolio function call"
  key_links:
    - from: "optimize_portfolio.py"
      to: "portfolio_generator.py"
      via: "generate_portfolio with objective='mean'"
      pattern: "generate_portfolio\(..., objective=['\"]mean['\"].*\)"
---

<objective>
Implement real mean-optimized baseline for tail validation to replace fake comparison.

Current tail validation uses fake baseline (mean_cvar = cvar_cvar * 0.9). This plan implements actual mean optimization to provide legitimate comparison, enabling verification that CVaR optimizer truly targets top-tail outcomes.

Purpose: Enable empirical validation that CVaR optimization outperforms mean optimization on tail metrics.

Output: Tail validation function that generates real mean-optimized baseline for comparison.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-tail-metrics-tail-objective-portfolio-optimizer-VERIFICATION.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-04-SUMMARY.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-05-PLAN.md

# Gap context from VERIFICATION.md
Gap 2: Tail validation uses fake baseline
- optimize_portfolio.py lines 349-352: mean_cvar = cvar_cvar * 0.9
- TODO: "Actually optimize for mean and compare"
- Cannot confirm CVaR > mean without real comparison

# Research guidance (03-RESEARCH.md lines 472-522)
Pitfall 2: Mean-Optimized Tail (Optimizer Secretly Targets Mean)
- Validate objective function: explicitly print CVaR vs mean for optimized lineups
- Compare against baseline: optimize pure mean, check if CVaR optimizer produces different lineups
- Use extreme quantile (α=0.99) to force tail focus
</context>

<tasks>

<task type="auto">
  <name>Add mean optimization objective to portfolio generator</name>
  <files>apps/backend/app/portfolio_generator.py</files>
  <action>
  Extend portfolio_generator.py to support mean optimization as objective option:

  IMPORTANT: After 03-05 replaces expected value fallback with CVaR, this plan adds mean optimization as a separate path (not a fallback).

  1. Modify _optimize_lineup function signature (line ~150) to accept objective_type parameter:
     ```python
     def _optimize_lineup(
         driver_data: List[Dict],
         scenarios: np.ndarray,
         previous_lineups: List[Dict],
         exposure_book: Dict[str, int],
         n_lineups_generated: int,
         salary_cap: int,
         n_drivers: int,
         min_stack: int,
         max_stack: int,
         max_driver_exposure: float,
         max_team_exposure: float,
         correlation_weight: float,
         solver_time_limit: int,
         cvar_alphas: List[float],
         cvar_weights: List[float],
         objective_type: str = "cvar"  # "cvar" or "mean"
     ) -> Optional[Dict]:
     ```

  2. In objective building section (after line 195), add conditional logic:
     ```python
     # Build objective based on objective_type
     if objective_type == "mean":
         # Mean optimization: maximize expected points across scenarios
         expected_points = {}
         for i, driver in enumerate(driver_data):
             driver_id = driver["driver_id"]
             mean_points = float(scenarios[:, i].mean())
             expected_points[driver_id] = mean_points

         cvar_objective = lpSum(
             expected_points[driver_id] * x[driver_id]
             for driver_id in x.keys()
         )
     else:  # objective_type == "cvar" (default after 03-05)
         # Bounded CVaR optimization for upper-tail maximization
         cvar_objective = build_upper_tail_cvar_objective(
             prob, scenarios, x, cvar_alphas[0], var_prefix="cvar_99"
         )

         # Optional: Add CVaR(95%) for stability
         if len(cvar_alphas) > 1:
             cvar_95_objective = build_upper_tail_cvar_objective(
                 prob, scenarios, x, cvar_alphas[1], var_prefix="cvar_95"
             )
             cvar_objective = cvar_weights[0] * cvar_objective + cvar_weights[1] * cvar_95_objective
     ```

  3. Update generate_portfolio function signature (line ~95) to accept objective_type:
     ```python
     def generate_portfolio(
         driver_data: List[Dict],
         scenarios: np.ndarray,
         n_lineups: int = 20,
         salary_cap: int = 50000,
         n_drivers: int = 6,
         min_stack: int = 2,
         max_stack: int = 3,
         max_driver_exposure: float = 0.5,
         max_team_exposure: float = 0.7,
         correlation_weight: float = 0.1,
         solver_time_limit: int = 30,
         cvar_alphas: List[float] = [0.99, 0.95],
         cvar_weights: List[float] = [0.7, 0.3],
         objective_type: str = "cvar"  # NEW: "cvar" or "mean"
     ) -> List[Dict]:
     ```

  4. Pass objective_type to _optimize_lineup call (line ~160)

  5. Update docstring to document objective_type parameter

  Do NOT modify CVaR optimization logic (preserve 03-05 work)
  </action>
  <verify>
  Test mean optimization produces valid lineups:
  ```bash
  cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
  python -c "
  from app.portfolio_generator import generate_portfolio
  import numpy as np

  np.random.seed(42)
  driver_data = [
      {'driver_id': i, 'name': f'Driver {i}', 'salary': 7500 + i*100, 'team': f'Team {i//3}'}
      for i in range(15)
  ]
  scenarios = np.random.randn(2000, 15) * 15 + 120

  # Test mean optimization
  mean_lineups = generate_portfolio(
      driver_data=driver_data,
      scenarios=scenarios,
      n_lineups=3,
      objective_type='mean'
  )

  print(f'✓ Generated {len(mean_lineups)} lineups with mean optimization')
  assert len(mean_lineups) >= 1, 'Should generate at least one lineup'
  print('✓ Mean optimization works correctly')
  "
  ```
  </verify>
  <done>
  portfolio_generator.py supports objective_type parameter ("cvar" or "mean")
  Mean optimization uses expected value objective
  CVaR optimization preserved (no changes to bounded CVaR formulation)
  Test confirms mean optimization generates valid lineups
  </done>
</task>

<task type="auto">
  <name>Implement real tail validation with mean baseline comparison</name>
  <files>apps/backend/app/api/optimize_portfolio.py</files>
  <action>
  Replace fake tail validation with real mean-optimized baseline:

  1. Add helper function after _validate_tail_stability (line ~340):
     ```python
     def _generate_mean_baseline_portfolio(
         driver_data: List[Dict],
         scenarios: np.ndarray
     ) -> Dict[str, Any]:
         """
         Generate mean-optimized baseline portfolio for tail validation.

         Creates a single lineup optimized for mean points to compare
         against CVaR-optimized lineups.

         Args:
             driver_data: Driver pool with salary, team info
             scenarios: Scenario matrix (n_scenarios, n_drivers)

         Returns:
             Dict with mean baseline metrics (CVaR, mean, top_X%)
         """
         from app.portfolio_generator import generate_portfolio

         # Generate mean-optimized lineup
         mean_lineups = generate_portfolio(
             driver_data=driver_data,
             scenarios=scenarios,
             n_lineups=1,
             salary_cap=50000,
             n_drivers=6,
             objective_type="mean",  # Mean optimization
             correlation_weight=0.0,  # No diversity penalty for baseline
             solver_time_limit=30
         )

         if not mean_lineups:
             logger.warning("Failed to generate mean baseline portfolio")
             return {
                 "mean_cvar": 0.0,
                 "mean_mean": 0.0,
                 "mean_top_x_pct": 0.0
             }

         # Compute tail metrics for mean baseline
         mean_lineup = mean_lineups[0]
         mean_lineup_points = scenarios[:, mean_lineup["drivers"]].sum(axis=1)

         from app.tail_metrics import compute_cvar, compute_tail_metrics
         mean_metrics = compute_tail_metrics(mean_lineup_points, alpha=0.99)

         return {
             "mean_cvar": mean_metrics.CVaR,
             "mean_mean": mean_lineup_points.mean(),
             "mean_top_x_pct": mean_metrics.top_X_pct
         }
     ```

  2. Replace _validate_tail_objective function (lines 343-363) with:
     ```python
     def _validate_tail_objective(
         lineups: List[Dict],
         scenarios: np.ndarray,
         driver_data: List[Dict]
     ) -> Dict[str, Any]:
         """
         Validate that optimizer targets tails, not mean.

         Compares CVaR-optimized lineups against actual mean-optimized baseline.

         Args:
             lineups: Generated CVaR-optimized lineups
             scenarios: Scenario matrix
             driver_data: Driver pool

         Returns:
             Dict with tail validation metrics
         """
         if not lineups:
             return {
                 "cvar_alphas": [0.99, 0.95],
                 "tail_improvement": 0.0,
                 "mean_sacrifice": 0.0,
                 "actually_optimizing_tail": False
             }

         # Compute CVaR-optimized lineup metrics
         first_lineup = lineups[0]
         lineup_points = scenarios[:, first_lineup["drivers"]].sum(axis=1)

         from app.tail_metrics import compute_tail_metrics
         cvar_metrics = compute_tail_metrics(lineup_points, alpha=0.99)
         cvar_cvar = cvar_metrics.CVaR
         cvar_mean = lineup_points.mean()

         # Generate REAL mean-optimized baseline
         logger.info("Generating mean-optimized baseline for tail validation...")
         mean_baseline = _generate_mean_baseline_portfolio(driver_data, scenarios)
         mean_cvar = mean_baseline["mean_cvar"]
         mean_mean = mean_baseline["mean_mean"]

         # Compute improvement metrics
         tail_improvement = (cvar_cvar - mean_cvar) / mean_cvar if mean_cvar > 0 else 0.0
         mean_sacrifice = (cvar_mean - mean_mean) / mean_mean if mean_mean > 0 else 0.0

         logger.info(
             f"Tail validation: CVaR={cvar_cvar:.2f}, MeanCVaR={mean_cvar:.2f}, "
             f"Improvement={tail_improvement:.1%}"
         )

         return {
             "cvar_alphas": [0.99, 0.95],
             "tail_improvement": tail_improvement,
             "mean_sacrifice": mean_sacrifice,
             "actually_optimizing_tail": tail_improvement > 0.05
         }
     ```

  3. Delete TODO comment at line 350 (replaced with real implementation)

  CRITICAL: Remove fake baseline calculation (mean_cvar = cvar_cvar * 0.9)
  </action>
  <verify>
  Test tail validation with real baseline:
  ```bash
  cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
  python -c "
  from app.api.optimize_portfolio import _validate_tail_objective, _generate_mean_baseline_portfolio
  import numpy as np

  np.random.seed(42)
  driver_data = [
      {'driver_id': i, 'name': f'Driver {i}', 'salary': 7500 + i*100, 'team': f'Team {i//3}'}
      for i in range(15)
  ]
  scenarios = np.random.randn(2000, 15) * 15 + 120

  # Test mean baseline generation
  baseline = _generate_mean_baseline_portfolio(driver_data, scenarios)
  print(f'✓ Mean baseline CVaR: {baseline[\"mean_cvar\"]:.2f}')

  # Test full validation with mock lineups
  mock_lineups = [{'drivers': [0, 1, 2, 3, 4, 5]}]
  validation = _validate_tail_objective(mock_lineups, scenarios, driver_data)
  print(f'✓ Tail improvement: {validation[\"tail_improvement\"]:.1%}')
  print(f'✓ Actually optimizing tail: {validation[\"actually_optimizing_tail\"]}')
  "

  # Verify API integration path
  grep -n "_validate_tail_objective" apps/backend/app/api/optimize_portfolio.py | grep "optimize_endpoint" || echo "Note: API integration verified in separate test"
  ```
  </verify>
  <done>
  _generate_mean_baseline_portfolio function exists
  _validate_tail_objective calls real mean baseline (not fake multiplier)
  TODO comment deleted
  Test confirms tail validation computes real improvement
  Validation returns actually_optimizing_tail based on real comparison
  </done>
</task>

<task type="auto">
  <name>Update API tests to verify real tail validation</name>
  <files>apps/backend/tests/test_main.py</files>
  <action>
  Update existing API test to verify tail validation uses real baseline:

  1. Find test_optimize_endpoint_includes_tail_validation in test_main.py
  2. Update assertions to check for real baseline comparison:
     ```python
     def test_optimize_endpoint_includes_tail_validation():
         \"\"\"Test that /optimize endpoint includes tail validation with real baseline.\"\"\"
         response = client.post("/optimize", json={
             "race_id": "test_race_tail_validation",
             "n_scenarios": 500,
             "n_lineups": 3
         })

         assert response.status_code == 200
         data = response.json()

         # Check tail validation exists
         assert "tail_validation" in data
         tail_val = data["tail_validation"]

         # Check real baseline metrics (not fake)
         assert "tail_improvement" in tail_val
         assert "mean_sacrifice" in tail_val
         assert "actually_optimizing_tail" in tail_val

         # Tail improvement should be computed from real comparison
         # (not hardcoded to 10% like fake baseline)
         tail_improvement = tail_val["tail_improvement"]
         assert isinstance(tail_improvement, float)
         # Real improvement can be positive or negative, not fixed at 0.10
         assert tail_improvement != 0.10, "Tail improvement should be computed from real baseline, not hardcoded"

         # Log validation results for debugging
         print(f"Tail improvement: {tail_improvement:.2%}")
         print(f"Actually optimizing tail: {tail_val['actually_optimizing_tail']}")
     ```

  3. Run updated test:
     ```bash
     pytest tests/test_main.py::test_optimize_endpoint_includes_tail_validation -v
     ```

  Test confirms tail validation uses real mean-optimized baseline
  </action>
  <verify>
  ```bash
  cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
  pytest tests/test_main.py::test_optimize_endpoint_includes_tail_validation -v

  # Expected: test passes, tail_improvement is not hardcoded to 0.10
  ```
  </verify>
  <done>
  Updated test confirms tail_improvement is not hardcoded
  Test validates actually_optimizing_tail computed from real comparison
  All API integration tests pass
  </done>
</task>

</tasks>

<verification>
After completion, verify tail validation gap is closed:

1. Read optimize_portfolio.py lines 340-380:
   - _generate_mean_baseline_portfolio function exists
   - _validate_tail_objective calls real baseline generation
   - No fake multiplier (cvar_cvar * 0.9) present
   - TODO comments deleted

2. Run API integration test:
   ```bash
   cd apps/backend && pytest tests/test_main.py::test_optimize_endpoint_includes_tail_validation -v
   ```
   Test passes, tail_improvement not hardcoded

3. Verify validation logic:
   - Mean baseline generated via generate_portfolio(objective_type="mean")
   - Tail improvement computed from real CVaR comparison
   - actually_optimizing_tail based on >5% improvement threshold
</verification>

<success_criteria>
Tail validation uses real mean-optimized baseline:
- _generate_mean_baseline_portfolio function exists and generates mean-optimized lineup
- _validate_tail_objective computes tail_improvement from real CVaR comparison
- No fake baseline (cvar_cvar * 0.9) in code
- API test confirms tail_improvement is not hardcoded
  </success_criteria>

<output>
After completion, create `.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-06-SUMMARY.md`
</output>
