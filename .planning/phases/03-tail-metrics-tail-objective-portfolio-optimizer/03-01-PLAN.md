---
phase: 03-tail-metrics-tail-objective-portfolio-optimizer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/backend/app/tail_metrics.py
  - apps/backend/app/tests/test_tail_metrics.py
autonomous: true

must_haves:
  truths:
    - "Tail metrics computed from scenario outcomes (Top X%, Top 0.1%, CVaR, conditional upside)"
    - "Metrics use vectorized NumPy operations for efficiency (np.partition not np.sort)"
    - "CVaR computed correctly using Rockafellar-Uryasev formulation (zeta + tail_slack mean)"
    - "Adaptive scenario count thresholds prevent instability from insufficient samples"
    - "Unit tests validate tail metric calculations against known values"
  artifacts:
    - path: "apps/backend/app/tail_metrics.py"
      provides: "Tail metric computations (CVaR, Top X%, conditional upside)"
      exports: ["compute_tail_metrics", "compute_cvar", "adaptive_scenario_count"]
      min_lines: 150
    - path: "apps/backend/app/tests/test_tail_metrics.py"
      provides: "Unit tests for tail metric computations"
      min_lines: 100
  key_links:
    - from: "apps/backend/app/tail_metrics.py"
      to: "packages/axiomatic-sim/src/axiomatic_sim/cbn.py"
      via: "ScenarioComponents from Phase 2 CBN sampling"
      pattern: "from axiomatic_sim.scenario_generator import ScenarioComponents"
    - from: "apps/backend/app/tail_metrics.py"
      to: "numpy"
      via: "np.partition for O(n) tail selection"
      pattern: "np\\.partition\\(.*-k\\)"
---

<objective>
Build tail metrics computation module that calculates Conditional Value at Risk (CVaR), Top X% outcomes, and conditional upside from scenario distributions. This provides the foundational objective functions for tail optimization in Phase 3.

Purpose: Enable optimization for top-tail outcomes (tournament equity) rather than mean points by providing efficient, stable tail metric calculations.

Output: Working tail_metrics.py module with vectorized NumPy implementation, Rockafellar-Uryasev CVaR formulation, adaptive scenario count handling, and comprehensive unit tests.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-CONTEXT.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-RESEARCH.md

# Phase 2 outputs
@packages/axiomatic-sim/src/axiomatic_sim/scenario_generator.py
@apps/backend/app/calibration/diagnostics.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tail_metrics.py module with core CVaR computation</name>
  <files>apps/backend/app/tail_metrics.py</files>
  <action>
Create new tail_metrics.py module with:

1. **compute_tail_metrics()** function:
   - Input: scenario_points (ndarray shape (n_scenarios,) with portfolio points per scenario)
   - Output: Dict with VaR, CVaR, Top_X_pct, conditional_upside
   - Use np.partition() for O(n) tail selection (NOT np.sort which is O(n log n))
   - Compute VaR (worst outcome in tail), CVaR (mean of tail), Top X% (best in tail)
   - Calculate conditional_upside = CVaR - overall_mean

2. **compute_cvar()** function implementing Rockafellar-Uryasev formulation:
   - Input: scenario_points (ndarray), alpha (tail quantile, e.g., 0.99)
   - Compute k = max(1, int((1-alpha) * len(scenario_points)))
   - Use np.partition to get top k scenarios
   - Return: CVaR = mean(top_k_points)

3. **Top X% metrics** for multiple quantiles:
   - Compute Top 1%, Top 5%, Top 10% outcomes
   - Return dict with {f"Top_{int((1-alpha)*100)}pct": value}

4. **Logging** for debugging:
   - Log scenario count, tail size, computed metrics
   - Warn if tail size < 10 (unstable estimates)

Use dataclass for tail metrics return:
```python
@dataclass
class TailMetrics:
    VaR: float  # Value at Risk (worst outcome in tail)
    CVaR: float  # Conditional Value at Risk (mean of tail)
    top_X_pct: float  # Best outcome in tail
    conditional_upside: float  # CVaR - overall_mean
    alpha: float  # Tail quantile used
```

DO NOT use np.sort() - use np.partition() for performance (O(n) vs O(n log n)).
DO NOT implement custom tail estimation - use standard Rockafellar-Uryasev.
DO include docstrings with NumPy-style parameter documentation.
  </action>
  <verify>
Run: python3 -c "
from app.tail_metrics import compute_tail_metrics, compute_cvar
import numpy as np
np.random.seed(42)
scenarios = np.random.randn(1000)
metrics = compute_tail_metrics(scenarios, alpha=0.99)
print(f'CVaR(99%): {metrics.CVaR:.3f}')
print(f'Top 1%: {metrics.top_X_pct:.3f}')
print(f'Conditional upside: {metrics.conditional_upside:.3f}')
"
Expected: Output shows computed metrics, no errors
  </verify>
  <done>
compute_tail_metrics() returns TailMetrics dataclass with all fields populated
compute_cvar() correctly computes CVaR using np.partition
Functions handle edge cases (empty scenarios, alpha=1.0)
  </done>
</task>

<task type="auto">
  <name>Task 2: Add adaptive scenario count handling</name>
  <files>apps/backend/app/tail_metrics.py</files>
  <action>
Add adaptive_scenario_count() function to prevent unstable tail estimates:

1. **Function signature:**
```python
def adaptive_scenario_count(target_alpha: float, min_tail_samples: int = 100) -> int:
    """
    Calculate minimum scenario count for stable tail estimation.

    Rule of thumb: Need at least min_tail_samples in the tail region.
    For alpha=0.99 (top 1%), need at least 100 / 0.01 = 10,000 scenarios.

    Args:
        target_alpha: Target tail quantile (e.g., 0.99 for top 1%)
        min_tail_samples: Minimum samples required in tail region

    Returns:
        Minimum number of scenarios for stable CVaR estimation
    """
```

2. **Tiered thresholds:**
   - alpha >= 0.99: return max(n_scenarios, 10000)
   - alpha >= 0.95: return max(n_scenarios, 2000)
   - alpha >= 0.90: return max(n_scenarios, 1000)
   - else: return max(n_scenarios, 500)

3. **Helper function: validate_tail_stability()**
   - Input: scenarios (ndarray), optimize_fn (callable), n_bootstrap (int=10)
   - Perform bootstrap resampling of scenarios
   - Compute CVaR for each bootstrap sample
   - Return: Dict with cvar_cv (coefficient of variation), lineup_consistency, stable (bool)
   - Log warning if CV > 0.2 or consistency < 0.7

DO NOT hardcode scenario counts - use formula based on alpha and min_tail_samples.
DO NOT skip stability validation - bootstrap checks are critical for detecting unstable tails.
DO log warnings when tail estimates are likely unstable.

Reference: 03-RESEARCH.md "Pitfall 1: Unstable Tail Estimates from Insufficient Scenarios"
  </action>
  <verify>
Run: python3 -c "
from app.tail_metrics import adaptive_scenario_count, validate_tail_stability
import numpy as np

# Test adaptive scenario counts
print(f'CVaR(99%) needs: {adaptive_scenario_count(0.99)} scenarios')
print(f'CVaR(95%) needs: {adaptive_scenario_count(0.95)} scenarios')
print(f'CVaR(90%) needs: {adaptive_scenario_count(0.90)} scenarios')

# Test stability validation
np.random.seed(42)
scenarios = np.random.randn(10000)
def dummy_optimize(s): return list(range(6))
stability = validate_tail_stability(scenarios, dummy_optimize, n_bootstrap=5)
print(f'Stable: {stability[\"stable\"]}, CV: {stability[\"cvar_cv\"]:.3f}')
"
Expected: Outputs show tiered scenario counts (10000, 2000, 1000) and stability metrics
  </verify>
  <done>
adaptive_scenario_count() returns tiered thresholds based on alpha
validate_tail_stability() performs bootstrap validation and returns stability metrics
Warnings logged for unstable tail estimates (CV > 0.2 or consistency < 0.7)
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for tail metrics</name>
  <files>apps/backend/app/tests/test_tail_metrics.py</files>
  <action>
Create test_tail_metrics.py with comprehensive unit tests:

1. **test_compute_cvar_known_values():**
   - Create fixed scenario array: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
   - CVaR(0.90) should be mean of top 1 scenario = 10
   - CVaR(0.80) should be mean of top 2 scenarios = (9+10)/2 = 9.5
   - Assert computed values match expected

2. **test_compute_tail_metrics_dataclass():**
   - Generate random scenarios (seed=42)
   - Compute metrics for alpha=0.99
   - Assert TailMetrics dataclass has all required fields
   - Assert types are correct (VaR, CVaR, top_X_pct are float; alpha is float)

3. **test_adaptive_scenario_count_tiers():**
   - Test alpha=0.99 returns >= 10000
   - Test alpha=0.95 returns >= 2000
   - Test alpha=0.90 returns >= 1000
   - Test min_tail_samples parameter affects output

4. **test_edge_cases():**
   - Empty scenarios array raises ValueError
   - Single scenario returns that scenario as CVaR
   - alpha=1.0 returns max(scenarios) as VaR/CVaR
   - alpha=0.0 returns min(scenarios) as VaR/CVaR

5. **test_partition_performance():**
   - Generate large scenario array (10000 scenarios)
   - Time compute_cvar with np.partition
   - Assert execution time < 100ms (vectorized operations should be fast)

6. **test_conditional_upside_calculation():**
   - Scenarios with mean=0, tail mean=5 should return conditional_upside=5
   - Scenarios with mean=10, tail mean=8 should return conditional_upside=-2
   - Assert conditional_upside = CVaR - overall_mean

DO NOT use random values without fixed seeds (makes tests flaky).
DO NOT skip edge case testing (tail metrics need robustness).
DO use pytest fixtures for common test data (scenario arrays).
DO assert performance characteristics (vectorized operations must be fast).
  </action>
  <verify>
Run: cd apps/backend && python3 -m pytest tests/test_tail_metrics.py -v
Expected: All 6 tests pass, no failures, no skipped tests
Run: python3 -m pytest tests/test_tail_metrics.py --cov=app.tail_metrics --cov-report=term-missing
Expected: Coverage > 90% for tail_metrics.py
  </verify>
  <done>
All 6 unit tests pass with known-value validation
Edge cases handled correctly (empty, single scenario, extreme alpha)
Performance test validates np.partition is fast (<100ms for 10k scenarios)
Coverage > 90% for tail_metrics.py
  </done>
</task>

</tasks>

<verification>
## Phase Verification Criteria

### Core Functionality
1. Tail metrics computed from scenario outcomes (Top X%, Top 0.1%, CVaR, conditional upside)
2. Metrics use vectorized NumPy operations (np.partition not np.sort)
3. CVaR computed correctly using Rockafellar-Uryasev formulation
4. Adaptive scenario count thresholds prevent instability
5. Unit tests validate calculations against known values

### Integration Points
1. ScenarioComponents from Phase 2 CBN sampling can be converted to point arrays
2. Metrics work with both NumPy and JAX arrays (JAX compatibility for Phase 2 GPU acceleration)
3. Logging provides visibility into tail estimation stability

### Avoidance Checks
1. No np.sort() usage (must use np.partition for O(n) performance)
2. No custom tail estimation formulas (must use Rockafellar-Uryasev)
3. No hardcoded scenario counts (must use adaptive formula)
4. No skipping stability validation (must bootstrap check)
</verification>

<success_criteria>
1. compute_tail_metrics() returns TailMetrics dataclass with VaR, CVaR, Top X%, conditional_upside
2. compute_cvar() correctly implements Rockafellar-Uryasev formulation with np.partition
3. adaptive_scenario_count() returns tiered thresholds (10000 for 99%, 2000 for 95%, 1000 for 90%)
4. validate_tail_stability() performs bootstrap validation and detects unstable estimates
5. All 6 unit tests pass with >90% code coverage
6. Performance test validates <100ms execution for 10,000 scenarios
7. Logging shows tail size, computed metrics, and stability warnings
</success_criteria>

<output>
After completion, create `.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-01-SUMMARY.md`
</output>
