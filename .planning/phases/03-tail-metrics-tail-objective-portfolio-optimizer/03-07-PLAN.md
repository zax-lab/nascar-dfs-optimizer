---
phase: 03-tail-metrics-tail-objective-portfolio-optimizer
plan: 07
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/backend/tests/test_tail_metrics.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Unit tests validate tail metric calculations (CVaR, Top X%, conditional upside)"
    - "Tests cover edge cases (empty arrays, NaN, single scenario, insufficient scenarios)"
    - "Tests validate correctness against known values"
  artifacts:
    - path: "apps/backend/tests/test_tail_metrics.py"
      provides: "Unit tests for tail metrics (CVaR, Top X%, conditional upside)"
      min_lines: 150
  key_links:
    - from: "test_tail_metrics.py"
      to: "tail_metrics.py"
      via: "import compute_cvar, compute_tail_metrics, adaptive_scenario_count"
      pattern: "from app.tail_metrics import"
---

<objective>
Create unit tests for tail metrics calculations.

Verification.md identified missing test_tail_metrics.py file that was claimed complete in 03-01-SUMMARY.md. This plan creates the missing test coverage for tail metric calculations (CVaR, Top X%, conditional upside, adaptive scenario counts).

Purpose: Complete test coverage for Phase 3 tail metrics functionality.

Output: New test file with comprehensive unit tests for tail metrics.
</objective>

<execution_context>
@/Users/zax/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-tail-metrics-tail-objective-portfolio-optimizer-VERIFICATION.md
@.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-01-SUMMARY.md

# Gap context from VERIFICATION.md
Gap 4: Missing test files (3 files claimed but don't exist)
- test_tail_metrics.py claimed in 03-01-SUMMARY.md but not in tests/

# Existing test patterns
- test_tail_objectives.py exists (21 tests passing) - use as structural reference
</context>

<tasks>

<task type="auto">
  <name>Create test_tail_metrics.py with unit tests for tail metric calculations</name>
  <files>apps/backend/tests/test_tail_metrics.py</files>
  <action>
  Create comprehensive unit tests for tail metrics module:

  Create new file apps/backend/tests/test_tail_metrics.py:

  ```python
  """
  Unit tests for tail metrics computation (CVaR, Top X%, conditional upside).

  Tests validate correctness of tail metric calculations against known values
  and edge cases (empty arrays, single scenario, insufficient scenarios).
  """
  import pytest
  import numpy as np
  from app.tail_metrics import (
      compute_cvar,
      compute_tail_metrics,
      adaptive_scenario_count,
      TailMetrics
  )


  class TestComputeCVaR:
      """Test CVaR calculation using Rockafellar-Uryasev formulation."""

      def test_cvar_with_known_values(self):
          """Test CVaR calculation with deterministic scenario outcomes."""
          # Simple case: 100 scenarios, values 0-99
          scenarios = np.arange(100, dtype=float)

          # CVaR at alpha=0.99 (top 1% = best scenario)
          cvar = compute_cvar(scenarios, alpha=0.99)

          # Top 1% of 100 scenarios = 1 scenario (value 99)
          # CVaR should be mean of top 1% = 99.0
          assert cvar == pytest.approx(99.0, rel=0.01)

      def test_cvar_with_multiple_tail_scenarios(self):
          """Test CVaR with multiple scenarios in tail region."""
          # 1000 scenarios, values 0-999
          scenarios = np.arange(1000, dtype=float)

          # CVaR at alpha=0.95 (top 5% = 50 scenarios)
          cvar = compute_cvar(scenarios, alpha=0.95)

          # Top 5% of 1000 = 50 scenarios (values 950-999)
          # Mean of 950-999 = (950 + 999) / 2 = 974.5
          expected_cvar = np.mean(np.arange(950, 1000, dtype=float))
          assert cvar == pytest.approx(expected_cvar, rel=0.01)

      def test_cvar_with_negative_values(self):
          """Test CVaR handles negative scenario outcomes (losses)."""
          scenarios = np.array([-50, -40, -30, -20, -10, 0, 10, 20, 30, 40])

          # CVaR at alpha=0.90 (top 10% = 1 scenario)
          cvar = compute_cvar(scenarios, alpha=0.90)

          # Top 10% = best scenario = 40
          assert cvar == pytest.approx(40.0, rel=0.01)

      def test_cvar_with_all_same_values(self):
          """Test CVaR when all scenarios have same outcome."""
          scenarios = np.full(100, 100.0)

          cvar = compute_cvar(scenarios, alpha=0.99)

          # CVaR should equal the constant value
          assert cvar == pytest.approx(100.0, rel=0.01)

      def test_cvar_with_insufficient_scenarios(self):
          """Test CVaR falls back to mean when too few scenarios."""
          # Only 10 scenarios for alpha=0.99 (needs 100+)
          scenarios = np.arange(10, dtype=float)

          # Should handle gracefully (either raise warning or use available)
          cvar = compute_cvar(scenarios, alpha=0.99)

          # With only 10 scenarios, tail is 1 scenario = value 9
          assert cvar == pytest.approx(9.0, rel=0.01)


  class TestComputeTailMetrics:
      """Test comprehensive tail metrics computation."""

      def test_tail_metrics_returns_complete_dataclass(self):
          """Test that tail metrics returns all required fields."""
          scenarios = np.random.randn(1000) * 10 + 100

          metrics = compute_tail_metrics(scenarios, alpha=0.99)

          # Check all required fields present
          assert hasattr(metrics, 'CVaR')
          assert hasattr(metrics, 'VaR')
          assert hasattr(metrics, 'top_X_pct')
          assert hasattr(metrics, 'conditional_upside')
          assert hasattr(metrics, 'mean')

          # Check types
          assert isinstance(metrics.CVaR, (float, np.floating))
          assert isinstance(metrics.VaR, (float, np.floating))
          assert isinstance(metrics.top_X_pct, (float, np.floating))
          assert isinstance(metrics.conditional_upside, (float, np.floating))
          assert isinstance(metrics.mean, (float, np.floating))

      def test_tail_metrics_cvar_greater_than_var(self):
          """Test that CVaR >= VaR for upper-tail optimization."""
          scenarios = np.random.randn(1000) * 15 + 120

          metrics = compute_tail_metrics(scenarios, alpha=0.99)

          # For upper tail, CVaR (mean of tail) should be >= VaR (worst in tail)
          assert metrics.CVaR >= metrics.VaR

      def test_tail_metrics_top_x_pct_is_maximum(self):
          """Test that top_X_pct equals maximum scenario outcome."""
          scenarios = np.array([100, 110, 120, 130, 140, 150])

          metrics = compute_tail_metrics(scenarios, alpha=0.90)

          # Top X% should be max value in scenarios
          assert metrics.top_X_pct == pytest.approx(150.0, rel=0.01)

      def test_tail_metrics_conditional_upside_calculation(self):
          """Test conditional upside = CVaR - mean."""
          scenarios = np.random.randn(500) * 12 + 115

          metrics = compute_tail_metrics(scenarios, alpha=0.95)

          # Conditional upside should be CVaR - mean
          expected_upside = metrics.CVaR - metrics.mean
          assert metrics.conditional_upside == pytest.approx(expected_upside, rel=0.01)

      def test_tail_metrics_with_realistic_dfs_points(self):
          """Test tail metrics with realistic NASCAR DFS point distribution."""
          # Simulate DFS points: mean ~120, std ~20, skew right
          np.random.seed(42)
          scenarios = np.random.gamma(shape=6, scale=20, size=1000) + 80

          metrics = compute_tail_metrics(scenarios, alpha=0.99)

          # Sanity checks for realistic DFS outcomes
          assert 80 < metrics.mean < 200  # Mean in reasonable range
          assert metrics.CVaR > metrics.mean  # CVaR > mean for upper tail
          assert metrics.top_X_pct >= metrics.CVaR  # Top X% >= CVaR


  class TestAdaptiveScenarioCount:
      """Test adaptive scenario count thresholds."""

      def test_adaptive_count_for_cvar_99(self):
          """Test that CVaR(99%) requires at least 10,000 scenarios."""
          n_scenarios = adaptive_scenario_count(target_alpha=0.99)

          assert n_scenarios >= 10000

      def test_adaptive_count_for_cvar_95(self):
          """Test that CVaR(95%) requires at least 2,000 scenarios."""
          n_scenarios = adaptive_scenario_count(target_alpha=0.95)

          assert n_scenarios >= 2000

      def test_adaptive_count_for_cvar_90(self):
          """Test that CVaR(90%) requires at least 1,000 scenarios."""
          n_scenarios = adaptive_scenario_count(target_alpha=0.90)

          assert n_scenarios >= 1000

      def test_adaptive_count_increases_with_alpha(self):
          """Test that higher alpha (more extreme tail) requires more scenarios."""
          n_90 = adaptive_scenario_count(target_alpha=0.90)
          n_95 = adaptive_scenario_count(target_alpha=0.95)
          n_99 = adaptive_scenario_count(target_alpha=0.99)

          assert n_99 > n_95 > n_90


  class TestEdgeCases:
      """Test edge cases and error handling."""

      def test_empty_scenarios_raises_error(self):
          """Test that empty scenario array raises appropriate error."""
          scenarios = np.array([])

          with pytest.raises((ValueError, IndexError)):
              compute_cvar(scenarios, alpha=0.99)

      def test_single_scenario_returns_that_value(self):
          """Test CVaR with single scenario returns that scenario's value."""
          scenarios = np.array([150.0])

          cvar = compute_cvar(scenarios, alpha=0.99)

          assert cvar == pytest.approx(150.0, rel=0.01)

      def test_nan_scenarios_handled_gracefully(self):
          """Test that NaN values are handled or raise clear error."""
          scenarios = np.array([100, np.nan, 120, 130])

          # Should either filter NaN or raise error
          try:
              cvar = compute_cvar(scenarios, alpha=0.99)
              # If succeeds, result should be finite
              assert np.isfinite(cvar)
          except (ValueError, RuntimeError):
              # Also acceptable: raise clear error
              pass
  ```

  Use existing test_tail_objectives.py (21 tests passing) as structural reference
  Follow pytest conventions (test_ prefix, assert statements, fixtures)
  Cover edge cases: empty arrays, NaN, single scenario, insufficient scenarios
  </action>
  <verify>
  Run new tests:
  ```bash
  cd /Users/zax/Desktop/nascar-model\ copy\ 2/apps/backend
  pytest tests/test_tail_metrics.py -v

  # Expected: All tests pass (15+ tests)
  ```
  </verify>
  <done>
  test_tail_metrics.py file exists in tests/ directory
  All tests pass (15+ tests covering CVaR, tail metrics, adaptive counts, edge cases)
  Tests validate correctness against known values
  Tests cover edge cases (empty, NaN, single scenario)
  </done>
</task>

</tasks>

<verification>
After completion, verify test file gap is closed:

1. Check test file exists:
   ```bash
   ls -la apps/backend/tests/test_tail_metrics.py
   ```
   File should exist

2. Run new tests:
   ```bash
   cd apps/backend
   pytest tests/test_tail_metrics.py -v
   ```
   All tests should pass (15+ tests)

3. Verify test coverage:
   - test_tail_metrics.py: 15+ tests for tail metrics calculations
</verification>

<success_criteria>
test_tail_metrics.py created and passing:
- test_tail_metrics.py exists with 15+ passing tests
- Tests validate tail metric calculations (CVaR, Top X%, conditional upside)
- Tests cover edge cases and known value validation
</success_criteria>

<output>
After completion, create `.planning/phases/03-tail-metrics-tail-objective-portfolio-optimizer/03-07-SUMMARY.md`
</output>
